{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQvIcgSdtWkc",
        "colab_type": "code",
        "outputId": "7f23e061-33c1-4858-fd0f-dbc4c055120f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "!pip install pytorch-transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-transformers in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.8.19)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.9.224)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.1.83)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.0.34)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.16.5)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.1.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.224 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.12.224)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.13.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.224->boto3->pytorch-transformers) (2.5.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.224->boto3->pytorch-transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlZXyoGcthuO",
        "colab_type": "code",
        "outputId": "f4663991-8c2e-4ab2-fd20-44010366eaae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import zipfile\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from pytorch_transformers import (WEIGHTS_NAME, AdamW, WarmupLinearSchedule,\n",
        "                                  BertConfig, BertForMaskedLM, BertTokenizer)\n",
        "\n",
        "import random\n",
        "manualSeed = 999\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    'bert': (BertConfig, BertForMaskedLM, BertTokenizer)\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUnwR7l85Q4-",
        "colab_type": "code",
        "outputId": "403343b3-870b-442d-a268-bd170c527045",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "from urllib import request\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "from typing import *\n",
        "\n",
        "save_dir = '/content/corpus/'\n",
        "file_list: List[str] = ['condensed_2009.json.zip', 'condensed_2010.json.zip', 'condensed_2011.json.zip', 'condensed_2012.json.zip', 'condensed_2013.json.zip', 'condensed_2014.json.zip', 'condensed_2015.json.zip', 'condensed_2016.json.zip', 'condensed_2017.json.zip', 'condensed_2018.json.zip']\n",
        "url_root = 'https://github.com/bpb27/trump_tweet_data_archive/raw/master/'\n",
        "\n",
        "# Download Trump tweets\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "for file_name in file_list:\n",
        "  print(f'Downloading {file_name}..')\n",
        "  file_path = save_dir + file_name\n",
        "  request.urlretrieve(url_root + file_name, file_path)\n",
        "  with ZipFile(file_path, 'r') as zip:\n",
        "    zip.extractall(save_dir)\n",
        "  os.remove(file_path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading condensed_2009.json.zip..\n",
            "Downloading condensed_2010.json.zip..\n",
            "Downloading condensed_2011.json.zip..\n",
            "Downloading condensed_2012.json.zip..\n",
            "Downloading condensed_2013.json.zip..\n",
            "Downloading condensed_2014.json.zip..\n",
            "Downloading condensed_2015.json.zip..\n",
            "Downloading condensed_2016.json.zip..\n",
            "Downloading condensed_2017.json.zip..\n",
            "Downloading condensed_2018.json.zip..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peVKTab-5YF1",
        "colab_type": "code",
        "outputId": "184e9ac3-82e6-4451-8c20-1c59e991cfdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "import json\n",
        "\n",
        "tweets = []\n",
        "file_list = list(map(lambda s: s.replace('.zip', ''), file_list))\n",
        "for f in file_list:\n",
        "  with open(save_dir + f, 'r', encoding='utf-8') as fp:\n",
        "    raw_tweets = json.load(fp)\n",
        "    for raw_tweet in raw_tweets:\n",
        "      text = raw_tweet[\"text\"]\n",
        "      tweets.append(text)\n",
        "\n",
        "print(str(len(tweets)) + \" tweets\")\n",
        "for tweet in tweets[:5]:\n",
        "  print(tweet)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36307 tweets\n",
            "From Donald Trump: Wishing everyone a wonderful holiday & a happy, healthy, prosperous New Year. Let’s think like champions in 2010!\n",
            "Trump International Tower in Chicago ranked 6th tallest building in world by Council on Tall Buildings & Urban Habitat http://bit.ly/sqvQq\n",
            "Wishing you and yours a very Happy and Bountiful Thanksgiving!\n",
            "Donald Trump Partners with TV1 on New Reality Series Entitled, Omarosa's Ultimate Merger: http://tinyurl.com/yk5m3lc\n",
            "--Work has begun, ahead of schedule, to build the greatest golf course in history: Trump International – Scotland.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T39xkDHEtovk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "e71032a8-91c0-42fc-bf7b-c9b168c3da33"
      },
      "source": [
        "# Tokenize tweets\n",
        "MAX_TWEET_LENGTH = 30\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "examples = []\n",
        "for tweet in tweets:\n",
        "  tokenized = tokenizer.tokenize(tweet)\n",
        "  if len(tokenized) < MAX_TWEET_LENGTH:\n",
        "    while len(tokenized) < MAX_TWEET_LENGTH:\n",
        "      tokenized.append(tokenizer.pad_token)\n",
        "    tokenized_ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
        "    examples.append(tokenized_ids)\n",
        "\n",
        "\n",
        "print(str(len(examples)) + \" examples\")\n",
        "for example in examples[:5]:\n",
        "  print(example)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "13722 examples\n",
            "[2013, 6221, 8398, 1024, 10261, 3071, 1037, 6919, 6209, 1004, 1037, 3407, 1010, 7965, 1010, 18241, 2047, 2095, 1012, 2292, 1521, 1055, 2228, 2066, 3966, 1999, 2230, 999, 0, 0]\n",
            "[10261, 2017, 1998, 6737, 1037, 2200, 3407, 1998, 8945, 16671, 18424, 15060, 999, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1011, 1011, 2147, 2038, 5625, 1010, 3805, 1997, 6134, 1010, 2000, 3857, 1996, 4602, 5439, 2607, 1999, 2381, 1024, 8398, 2248, 1516, 3885, 1012, 0, 0, 0, 0, 0, 0]\n",
            "[3191, 6221, 8398, 1005, 1055, 2327, 2702, 10247, 2005, 3112, 1024, 8299, 1024, 1013, 1013, 4714, 3126, 2140, 1012, 4012, 1013, 11338, 2078, 2620, 8516, 0, 0, 0, 0, 0]\n",
            "[7332, 2912, 2003, 2085, 2006, 10474, 1011, 2017, 2064, 3582, 2014, 1030, 7332, 24498, 6824, 2361, 1011, 2031, 1037, 27547, 5353, 999, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtPtsHY5uUXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mask parts of a tweet, adapted from https://github.com/huggingface/pytorch-transformers/blob/master/examples/run_lm_finetuning.py\n",
        "def mask_tokens(inputs, tokenizer):\n",
        "    \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
        "    labels = inputs.clone()\n",
        "    for i in range(len(inputs)):\n",
        "      irow = inputs[i]\n",
        "      lrow = labels[i]\n",
        "      \n",
        "      # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
        "      pad_start = 0\n",
        "      for j in range(len(irow)):\n",
        "        if irow[j] == tokenizer.convert_tokens_to_ids(tokenizer.pad_token):\n",
        "          pad_start = j\n",
        "          break\n",
        "      masked_indices = torch.bernoulli(torch.full((pad_start,), 0.25)).to(torch.bool)\n",
        "      for j in range(len(lrow)):\n",
        "        if j >= pad_start or not masked_indices[j]:\n",
        "          lrow[j] = -1\n",
        "      #print(lrow)\n",
        "      #print(masked_indices)\n",
        "\n",
        "      # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "      indices_replaced = torch.bernoulli(torch.full((pad_start,), 0.8)).to(torch.bool)\n",
        "      for j in range(pad_start):\n",
        "        if indices_replaced[j] and masked_indices[j]:\n",
        "          irow[j] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
        "      #print(irow)\n",
        "\n",
        "      # 10% of the time, we replace masked input tokens with random word\n",
        "      indices_random = torch.bernoulli(torch.full((pad_start,), 0.5)).to(torch.bool)\n",
        "      for j in range(pad_start):\n",
        "        if indices_random[j] and not indices_replaced[j] and masked_indices[j]:\n",
        "          irow[j] = np.random.randint(len(tokenizer))\n",
        "      #print(irow)\n",
        "\n",
        "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
        "    return inputs, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqIi5FOW9YXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(inputs, outputs, labels):\n",
        "  predictions = []\n",
        "  for j in range(len(inputs)):\n",
        "    prediction = []\n",
        "    for k in range(len(inputs[j])):\n",
        "      if labels[j][k] == -1:\n",
        "        prediction.append(inputs[j][k].item())\n",
        "      else:\n",
        "        predicted_index = torch.argmax(outputs[j][k]).item()\n",
        "        prediction.append(predicted_index)\n",
        "    predictions.append(prediction)\n",
        "  return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtninVUv7FDV",
        "colab_type": "code",
        "outputId": "9a4d49c4-3b80-4243-eb44-1d58826fd471",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load model etc.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "model = model.to(device)\n",
        "\n",
        "# Optimization parameters\n",
        "lr = 1e-4\n",
        "max_grad_norm = 1.0\n",
        "num_total_steps = 25\n",
        "num_warmup_steps = 10\n",
        "\n",
        "optimizerG = AdamW(model.parameters(), lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
        "schedulerG = WarmupLinearSchedule(optimizerG, warmup_steps=num_warmup_steps, t_total=num_total_steps)  # PyTorch scheduler\n",
        "\n",
        "batch_size = 100\n",
        "print_interval = 5\n",
        "print_size = 1\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "for i in range(num_total_steps):\n",
        "  permuted_examples = list(np.random.permutation(examples))\n",
        "  #for batch_start in range(0, len(permuted_examples) - batch_size, batch_size):\n",
        "  #  batch_end = batch_start + batch_size\n",
        "  batch = permuted_examples[:batch_size]\n",
        "  inputs = torch.as_tensor(batch, dtype=torch.int64)\n",
        "  inputs, labels = mask_tokens(inputs, tokenizer)\n",
        "  inputs = inputs.to(device)\n",
        "  labels = labels.to(device)\n",
        "  model.train()\n",
        "  outputs = model(inputs, masked_lm_labels=labels)\n",
        "  loss = outputs[0]\n",
        "  loss.backward()\n",
        "  torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "  optimizerG.step()\n",
        "  schedulerG.step()\n",
        "  optimizerG.zero_grad()\n",
        "  print(str(i + 1) + \"/\" + str(num_total_steps) + \" steps\")\n",
        "  print(\"loss: \" + str(loss.item()))\n",
        "  if (i + 1) % print_interval == 0:\n",
        "    print(\"sample predictions:\")\n",
        "    predictions = predict(inputs, outputs[1], labels)\n",
        "    for j in range(min(batch_size, print_size)):\n",
        "      print(\"original : \" + tokenizer.decode(batch[j], skip_special_tokens=True))\n",
        "      print(\"predicted: \" + tokenizer.decode(predictions[j], skip_special_tokens=True))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Training Loop...\n",
            "1/25 steps\n",
            "loss: 5.819645881652832\n",
            "2/25 steps\n",
            "loss: 6.207118034362793\n",
            "3/25 steps\n",
            "loss: 6.1653594970703125\n",
            "4/25 steps\n",
            "loss: 5.720026969909668\n",
            "5/25 steps\n",
            "loss: 5.150602340698242\n",
            "sample predictions:\n",
            "original : i will be interviewed on @ oreillyfactor tonight at 11pme @ foxnews. enjoy!\n",
            "predicted: i will be back on @ oreillyfaz tonight. the way on @ foxnews. enjoy.\n",
            "6/25 steps\n",
            "loss: 4.924920558929443\n",
            "7/25 steps\n",
            "loss: 4.935698986053467\n",
            "8/25 steps\n",
            "loss: 6.618624210357666\n",
            "9/25 steps\n",
            "loss: 4.8811750411987305\n",
            "10/25 steps\n",
            "loss: 4.949885368347168\n",
            "sample predictions:\n",
            "original : i will be interviewed from cleveland, ohio, on @ seanhannity - tonight at 10 : 00 p. m. enjoy!\n",
            "predicted: i will be interviewed from cleveland. ohio. on \" seanhan \" - tonight : 10 : 00 p. m. enjoy.\n",
            "11/25 steps\n",
            "loss: 4.865874767303467\n",
            "12/25 steps\n",
            "loss: 4.731205463409424\n",
            "13/25 steps\n",
            "loss: 4.672391414642334\n",
            "14/25 steps\n",
            "loss: 4.229905128479004\n",
            "15/25 steps\n",
            "loss: 4.276388645172119\n",
            "sample predictions:\n",
            "original : i am watching crooked hillary speak. same old stuff, our country needs change!\n",
            "predicted: i am a crooked hillary speak. same old stuff, our country needs change!\n",
            "16/25 steps\n",
            "loss: 4.0266265869140625\n",
            "17/25 steps\n",
            "loss: 4.339563369750977\n",
            "18/25 steps\n",
            "loss: 4.125237941741943\n",
            "19/25 steps\n",
            "loss: 4.052849292755127\n",
            "20/25 steps\n",
            "loss: 3.9974663257598877\n",
            "sample predictions:\n",
            "original : @ davidbarringer2 that's right.\n",
            "predicted: @ davidbarringer is it's right.\n",
            "21/25 steps\n",
            "loss: 3.7384352684020996\n",
            "22/25 steps\n",
            "loss: 3.832040786743164\n",
            "23/25 steps\n",
            "loss: 3.874335527420044\n",
            "24/25 steps\n",
            "loss: 4.242733955383301\n",
            "25/25 steps\n",
            "loss: 3.963332414627075\n",
            "sample predictions:\n",
            "original : with all that congress has to work on, do they really have to make the weakening of the independent ethics watchdog, as unfair as it\n",
            "predicted: with all that congress has to work on, do they really have to stop the weakening of the national ethics in be, as long as it\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMPjaTinQcmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir = \"models\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "model.save_pretrained(model_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24J64_s18fh6",
        "colab_type": "code",
        "outputId": "cd16e273-2598-4bff-93bf-16e153f1e9a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "source": [
        "#generating_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "#generating_model = generating_model.to(device)\n",
        "generating_model = model\n",
        "\n",
        "# Bert as Text Generator: https://arxiv.org/pdf/1902.04094.pdf\n",
        "def generate_tweet(model, length):\n",
        "  #tweet = np.random.randint(len(tokenizer), size=(length))\n",
        "  tweet = examples[np.random.randint(len(examples))]\n",
        "  length = len(tweet)\n",
        "  order = np.random.permutation(range(length))\n",
        "  for i in order:\n",
        "    tweet[i] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
        "    inputs = torch.as_tensor([tweet], dtype=torch.int64)\n",
        "    labels = inputs.clone()\n",
        "    labels = torch.full((1, length), -1, dtype=torch.int64)\n",
        "    labels[0][i] = inputs[0][i]\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      outputs = model(inputs)\n",
        "    predictions = predict(inputs, outputs[0], labels)\n",
        "    tweet[i] = predictions[0][i]\n",
        "    print(tokenizer.decode(tweet))\n",
        "  return tweet\n",
        "\n",
        "# Generate a tweet\n",
        "tweet = generate_tweet(generating_model, 10)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "@ tanjas great world tanja - - never give up! [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "@ tanjas great world tanja - - never give up! [PAD] [PAD] @ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "@ tanjas great world tanja - - never give up! [PAD] [PAD] @ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] @ [PAD] [PAD] [PAD] [PAD]\n",
            "@ tanjas great world tanja - - never give up! [PAD] [PAD] @ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] @ [PAD] [PAD] [PAD] [PAD]\n",
            "@ tanjas great world tanja - - never give up! [PAD] [PAD] @ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] @ [PAD] [PAD] [PAD] [PAD]\n",
            "@ tanjas great world tanja - - never give up. [PAD] [PAD] @ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] @ [PAD] [PAD] [PAD] [PAD]\n",
            "@ tanjas great world tanja - - never give up. [PAD] [PAD] @ [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] @ [PAD] [PAD] [PAD] @\n",
            "@ tanjas great world tanja - - never give up. [PAD] [PAD] @ [PAD] [PAD] [PAD]. [PAD] [PAD] [PAD] [PAD] [PAD] @ [PAD] [PAD] [PAD] @\n",
            "@ tanjas great world tanja - - never give up. [PAD] [PAD] @ [PAD]. [PAD]. [PAD] [PAD] [PAD] [PAD] [PAD] @ [PAD] [PAD] [PAD] @\n",
            "@ tanjas great world tanja - - never give up. [PAD] [PAD] @ [PAD]. thanks. [PAD] [PAD] [PAD] [PAD] [PAD] @ [PAD] [PAD] [PAD] @\n",
            "@ tanjas great world tanja - - never give up. [PAD] [PAD] @ [PAD]. thanks. [PAD] [PAD] [PAD] [PAD] [PAD] @ trump [PAD] [PAD] @\n",
            "@ tanjas great world tanja - - never give up. [PAD] [PAD] @ [PAD]. thanks. [PAD] [PAD] [PAD] [PAD] [PAD] @ trump. [PAD] @\n",
            "@ tanjas great world tanja - - never give up. [PAD] [PAD] @ [PAD]. thanks. [PAD] [PAD] [PAD] [PAD] [PAD] @ trump. \" @\n",
            "@ tanjas great world tanja - - never give up. [PAD] [PAD] @ [PAD]. thanks. [PAD] [PAD] [PAD] [PAD] [PAD] @ trump. \" @\n",
            "@ tanjas great world tanja - - never give up. [PAD] [PAD] @ trump. thanks. [PAD] [PAD] [PAD] [PAD] [PAD] @ trump. \" @\n",
            "@ tanjas great world tanja - - never give up. [PAD] [PAD] @ trump. thanks. [PAD] [PAD] [PAD] [PAD] [PAD] @ trump. \" @\n",
            "@ tanja great world tanja - - never give up. [PAD] [PAD] @ trump. thanks. [PAD] [PAD] [PAD] [PAD] [PAD] @ trump. \" @\n",
            "@ tanja great world tanja - - never give up. [PAD] [PAD] @ trump. thanks. [PAD] [PAD] [PAD] [PAD] @ @ trump. \" @\n",
            "@ tanja great world tanja - - never give up. [PAD] [PAD] @ trump. thanks. [PAD] [PAD] [PAD] [PAD] @ @ trump. \" @\n",
            "@ tanja great. tanja - - never give up. [PAD] [PAD] @ trump. thanks. [PAD] [PAD] [PAD] [PAD] @ @ trump. \" @\n",
            "@ tanja great. tanja - - never give up. [PAD] [PAD] @ trump. thanks. [PAD] [PAD] [PAD] [PAD] @ @ trump. \" @\n",
            "@ tanja great. tanja - - never give up. [PAD] [PAD] @ trump. thanks. [PAD] \" [PAD] [PAD] @ @ trump. \" @\n",
            "@ tanja great. tanja - - never give up. [PAD] [PAD] @ trump. thanks. \" \" [PAD] [PAD] @ @ trump. \" @\n",
            "@ tanja great. tanja - - never give up. \" [PAD] @ trump. thanks. \" \" [PAD] [PAD] @ @ trump. \" @\n",
            "\" tanja great. tanja - - never give up. \" [PAD] @ trump. thanks. \" \" [PAD] [PAD] @ @ trump. \" @\n",
            "\" tanja great. tanja - - never give up. \" \" @ trump. thanks. \" \" [PAD] [PAD] @ @ trump. \" @\n",
            "\" tanja great. tanja - - never give up. \" \" @ trump. thanks. \" \" - [PAD] @ @ trump. \" @\n",
            "\" tanja great. tanja - - never give up. \" \" @ trump. thanks. \" \" - - @ @ trump. \" @\n",
            "\" tanja great. tanja great - never give up. \" \" @ trump. thanks. \" \" - - @ @ trump. \" @\n",
            "\" tanja great. tanja great - i give up. \" \" @ trump. thanks. \" \" - - @ @ trump. \" @\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}