{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQvIcgSdtWkc",
        "colab_type": "code",
        "outputId": "7f23e061-33c1-4858-fd0f-dbc4c055120f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        }
      },
      "source": [
        "!pip install pytorch-transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-transformers in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.8.19)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.9.224)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.1.83)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (0.0.34)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.16.5)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.1.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.224 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.12.224)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.13.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.224->boto3->pytorch-transformers) (2.5.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.224->boto3->pytorch-transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlZXyoGcthuO",
        "colab_type": "code",
        "outputId": "f4663991-8c2e-4ab2-fd20-44010366eaae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import zipfile\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from pytorch_transformers import (WEIGHTS_NAME, AdamW, WarmupLinearSchedule,\n",
        "                                  BertConfig, BertForMaskedLM, BertTokenizer)\n",
        "\n",
        "import random\n",
        "manualSeed = 999\n",
        "print(\"Random Seed: \", manualSeed)\n",
        "random.seed(manualSeed)\n",
        "torch.manual_seed(manualSeed)\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    'bert': (BertConfig, BertForMaskedLM, BertTokenizer)\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUnwR7l85Q4-",
        "colab_type": "code",
        "outputId": "403343b3-870b-442d-a268-bd170c527045",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "from urllib import request\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "from typing import *\n",
        "\n",
        "save_dir = '/content/corpus/'\n",
        "file_list: List[str] = ['condensed_2009.json.zip', 'condensed_2010.json.zip', 'condensed_2011.json.zip', 'condensed_2012.json.zip', 'condensed_2013.json.zip', 'condensed_2014.json.zip', 'condensed_2015.json.zip', 'condensed_2016.json.zip', 'condensed_2017.json.zip', 'condensed_2018.json.zip']\n",
        "url_root = 'https://github.com/bpb27/trump_tweet_data_archive/raw/master/'\n",
        "\n",
        "# Download Trump tweets\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "for file_name in file_list:\n",
        "  print(f'Downloading {file_name}..')\n",
        "  file_path = save_dir + file_name\n",
        "  request.urlretrieve(url_root + file_name, file_path)\n",
        "  with ZipFile(file_path, 'r') as zip:\n",
        "    zip.extractall(save_dir)\n",
        "  os.remove(file_path)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading condensed_2009.json.zip..\n",
            "Downloading condensed_2010.json.zip..\n",
            "Downloading condensed_2011.json.zip..\n",
            "Downloading condensed_2012.json.zip..\n",
            "Downloading condensed_2013.json.zip..\n",
            "Downloading condensed_2014.json.zip..\n",
            "Downloading condensed_2015.json.zip..\n",
            "Downloading condensed_2016.json.zip..\n",
            "Downloading condensed_2017.json.zip..\n",
            "Downloading condensed_2018.json.zip..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peVKTab-5YF1",
        "colab_type": "code",
        "outputId": "184e9ac3-82e6-4451-8c20-1c59e991cfdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "import json\n",
        "\n",
        "tweets = []\n",
        "file_list = list(map(lambda s: s.replace('.zip', ''), file_list))\n",
        "for f in file_list:\n",
        "  with open(save_dir + f, 'r', encoding='utf-8') as fp:\n",
        "    raw_tweets = json.load(fp)\n",
        "    for raw_tweet in raw_tweets:\n",
        "      text = raw_tweet[\"text\"]\n",
        "      tweets.append(text)\n",
        "\n",
        "print(str(len(tweets)) + \" tweets\")\n",
        "for tweet in tweets[:5]:\n",
        "  print(tweet)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36307 tweets\n",
            "From Donald Trump: Wishing everyone a wonderful holiday & a happy, healthy, prosperous New Year. Let’s think like champions in 2010!\n",
            "Trump International Tower in Chicago ranked 6th tallest building in world by Council on Tall Buildings & Urban Habitat http://bit.ly/sqvQq\n",
            "Wishing you and yours a very Happy and Bountiful Thanksgiving!\n",
            "Donald Trump Partners with TV1 on New Reality Series Entitled, Omarosa's Ultimate Merger: http://tinyurl.com/yk5m3lc\n",
            "--Work has begun, ahead of schedule, to build the greatest golf course in history: Trump International – Scotland.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T39xkDHEtovk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "7a9ce486-5d54-4b75-982f-eb3d6cd7b624"
      },
      "source": [
        "# Tokenize tweets\n",
        "MAX_TWEET_LENGTH = 50\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "examples = []\n",
        "for tweet in tweets:\n",
        "  tokenized = tokenizer.tokenize(tweet)\n",
        "  if len(tokenized) < MAX_TWEET_LENGTH:\n",
        "    while len(tokenized) < MAX_TWEET_LENGTH:\n",
        "      tokenized.append(tokenizer.pad_token)\n",
        "    tokenized_ids = tokenizer.convert_tokens_to_ids(tokenized)\n",
        "    examples.append(tokenized_ids)\n",
        "\n",
        "\n",
        "print(str(len(examples)) + \" examples\")\n",
        "for example in examples[:5]:\n",
        "  print(example)"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "33622 examples\n",
            "[2013, 6221, 8398, 1024, 10261, 3071, 1037, 6919, 6209, 1004, 1037, 3407, 1010, 7965, 1010, 18241, 2047, 2095, 1012, 2292, 1521, 1055, 2228, 2066, 3966, 1999, 2230, 999, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[8398, 2248, 3578, 1999, 3190, 4396, 5351, 13747, 2311, 1999, 2088, 2011, 2473, 2006, 4206, 3121, 1004, 3923, 6552, 8299, 1024, 1013, 1013, 2978, 1012, 1048, 2100, 1013, 5490, 2615, 4160, 4160, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[10261, 2017, 1998, 6737, 1037, 2200, 3407, 1998, 8945, 16671, 18424, 15060, 999, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[6221, 8398, 5826, 2007, 2694, 2487, 2006, 2047, 4507, 2186, 4709, 1010, 13192, 8820, 1005, 1055, 7209, 7660, 1024, 8299, 1024, 1013, 1013, 4714, 3126, 2140, 1012, 4012, 1013, 1061, 2243, 2629, 2213, 2509, 15472, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[1011, 1011, 2147, 2038, 5625, 1010, 3805, 1997, 6134, 1010, 2000, 3857, 1996, 4602, 5439, 2607, 1999, 2381, 1024, 8398, 2248, 1516, 3885, 1012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtPtsHY5uUXw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mask parts of a tweet, adapted from https://github.com/huggingface/pytorch-transformers/blob/master/examples/run_lm_finetuning.py\n",
        "def mask_tokens(inputs, tokenizer):\n",
        "    \"\"\" Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original. \"\"\"\n",
        "    labels = inputs.clone()\n",
        "    for i in range(len(inputs)):\n",
        "      irow = inputs[i]\n",
        "      lrow = labels[i]\n",
        "      \n",
        "      # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
        "      pad_start = 0\n",
        "      for j in range(len(irow)):\n",
        "        if irow[j] == tokenizer.convert_tokens_to_ids(tokenizer.pad_token):\n",
        "          pad_start = j\n",
        "          break\n",
        "      masked_indices = torch.bernoulli(torch.full((pad_start,), 0.25)).to(torch.bool)\n",
        "      for j in range(len(lrow)):\n",
        "        if j >= pad_start or not masked_indices[j]:\n",
        "          lrow[j] = -1\n",
        "      #print(lrow)\n",
        "      #print(masked_indices)\n",
        "\n",
        "      # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "      indices_replaced = torch.bernoulli(torch.full((pad_start,), 0.8)).to(torch.bool)\n",
        "      for j in range(pad_start):\n",
        "        if indices_replaced[j] and masked_indices[j]:\n",
        "          irow[j] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
        "      #print(irow)\n",
        "\n",
        "      # 10% of the time, we replace masked input tokens with random word\n",
        "      indices_random = torch.bernoulli(torch.full((pad_start,), 0.5)).to(torch.bool)\n",
        "      for j in range(pad_start):\n",
        "        if indices_random[j] and not indices_replaced[j] and masked_indices[j]:\n",
        "          irow[j] = np.random.randint(len(tokenizer))\n",
        "      #print(irow)\n",
        "\n",
        "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
        "    return inputs, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqIi5FOW9YXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(inputs, outputs, labels):\n",
        "  predictions = []\n",
        "  for j in range(len(inputs)):\n",
        "    prediction = []\n",
        "    for k in range(len(inputs[j])):\n",
        "      if labels[j][k] == -1:\n",
        "        prediction.append(inputs[j][k].item())\n",
        "      else:\n",
        "        predicted_index = torch.argmax(outputs[j][k]).item()\n",
        "        prediction.append(predicted_index)\n",
        "    predictions.append(prediction)\n",
        "  return predictions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtninVUv7FDV",
        "colab_type": "code",
        "outputId": "144fa67f-c36a-4b53-fbbe-3302b255b8d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Load model etc.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "#model = BertForMaskedLM.from_pretrained('models')\n",
        "model = model.to(device)\n",
        "\n",
        "# Optimization parameters\n",
        "lr = 1e-4\n",
        "max_grad_norm = 1.0\n",
        "num_total_steps = 100\n",
        "num_warmup_steps = 10\n",
        "\n",
        "optimizerG = AdamW(model.parameters(), lr=lr, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n",
        "schedulerG = WarmupLinearSchedule(optimizerG, warmup_steps=num_warmup_steps, t_total=num_total_steps)  # PyTorch scheduler\n",
        "\n",
        "batch_size = 100\n",
        "print_interval = 5\n",
        "print_size = 1\n",
        "\n",
        "print(\"Starting Training Loop...\")\n",
        "for i in range(num_total_steps):\n",
        "  permuted_examples = list(np.random.permutation(examples))\n",
        "  #for batch_start in range(0, len(permuted_examples) - batch_size, batch_size):\n",
        "  #  batch_end = batch_start + batch_size\n",
        "  batch = permuted_examples[:batch_size]\n",
        "  inputs = torch.as_tensor(batch, dtype=torch.int64)\n",
        "  inputs, labels = mask_tokens(inputs, tokenizer)\n",
        "  inputs = inputs.to(device)\n",
        "  labels = labels.to(device)\n",
        "  model.train()\n",
        "  outputs = model(inputs, masked_lm_labels=labels)\n",
        "  loss = outputs[0]\n",
        "  loss.backward()\n",
        "  torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "  optimizerG.step()\n",
        "  schedulerG.step()\n",
        "  optimizerG.zero_grad()\n",
        "  print(str(i + 1) + \"/\" + str(num_total_steps) + \" steps\")\n",
        "  print(\"loss: \" + str(loss.item()))\n",
        "  if (i + 1) % print_interval == 0:\n",
        "    print(\"sample predictions:\")\n",
        "    predictions = predict(inputs, outputs[1], labels)\n",
        "    for j in range(min(batch_size, print_size)):\n",
        "      print(\"original : \" + tokenizer.decode(batch[j], skip_special_tokens=True))\n",
        "      print(\"predicted: \" + tokenizer.decode(predictions[j], skip_special_tokens=True))"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Training Loop...\n",
            "1/100 steps\n",
            "loss: 5.747446060180664\n",
            "2/100 steps\n",
            "loss: 5.8749518394470215\n",
            "3/100 steps\n",
            "loss: 5.5652055740356445\n",
            "4/100 steps\n",
            "loss: 5.305379867553711\n",
            "5/100 steps\n",
            "loss: 4.9442901611328125\n",
            "sample predictions:\n",
            "original : the benghazi terrorist is getting speedier care than our vets at the va. obama has his priorities.\n",
            "predicted: the benwalazi terrorist is getting speedier care than our vets can...a has his rights.\n",
            "6/100 steps\n",
            "loss: 4.397333145141602\n",
            "7/100 steps\n",
            "loss: 5.479149341583252\n",
            "8/100 steps\n",
            "loss: 4.6567864418029785\n",
            "9/100 steps\n",
            "loss: 4.40280294418335\n",
            "10/100 steps\n",
            "loss: 4.603474140167236\n",
            "sample predictions:\n",
            "original : let the arab league take care of syria. why are these rich arab countries not paying us for the tremendous cost of such an attack?\n",
            "predicted: let the arab countries take care of syria. why are sure the american countries countries paying us for the tremendous cost of such an attack?\n",
            "11/100 steps\n",
            "loss: 4.095590114593506\n",
            "12/100 steps\n",
            "loss: 4.340753555297852\n",
            "13/100 steps\n",
            "loss: 4.336602210998535\n",
            "14/100 steps\n",
            "loss: 4.019033908843994\n",
            "15/100 steps\n",
            "loss: 3.8241522312164307\n",
            "sample predictions:\n",
            "original : \" @ modivmusic i've been reading @ therealkiyosaki books and about warrenbuffett lately. i need some trump books to read! \" the art of the deal \"\n",
            "predicted: \" @ modiv : i've been reading @ qalkiyosaki books and about warren moffett lately. i need some new books to read! \" the art of the deal \"\n",
            "16/100 steps\n",
            "loss: 3.781795024871826\n",
            "17/100 steps\n",
            "loss: 4.049126625061035\n",
            "18/100 steps\n",
            "loss: 4.023678779602051\n",
            "19/100 steps\n",
            "loss: 3.7491884231567383\n",
            "20/100 steps\n",
            "loss: 3.648731231689453\n",
            "sample predictions:\n",
            "original : \" @ laurencristmann : thank you for coming to pennsylvania today. i will be there with @ c _ lynne _ ryan your 3rd cd delegate at 2 pm \"\n",
            "predicted: \" @ laurencrists : thank you for going to pennsylvania today. i will be there with @ c _ lynne _ ryan the 3rd state delegate at 2 pm.\n",
            "21/100 steps\n",
            "loss: 3.6913137435913086\n",
            "22/100 steps\n",
            "loss: 3.75441312789917\n",
            "23/100 steps\n",
            "loss: 3.303905487060547\n",
            "24/100 steps\n",
            "loss: 3.68058180809021\n",
            "25/100 steps\n",
            "loss: 3.5432333946228027\n",
            "sample predictions:\n",
            "original : \" @ nstaicer : @ realdonaldtrump visited @ trumpwinery over the weekend. very impressive and we will be going back soon! \"\n",
            "predicted: \" @ nsmo : @ realdonaldtrump visited @ trumpwinery over the weekend. very impressive and we will be going home soon! \"\n",
            "26/100 steps\n",
            "loss: 3.823930501937866\n",
            "27/100 steps\n",
            "loss: 3.5154707431793213\n",
            "28/100 steps\n",
            "loss: 3.540419816970825\n",
            "29/100 steps\n",
            "loss: 3.786618709564209\n",
            "30/100 steps\n",
            "loss: 3.540661573410034\n",
            "sample predictions:\n",
            "original : “ @ damacofficial announces @ tigerwoods to create golf course for trump world golf club dubai ” http : / / t. co / cnbboxfbdb via @ businesswire\n",
            "predicted: “ @ trumpbaffier announces @ tigerwood to the golf course for trump world golf club dubai ” http : / / t. co / cnbboxfbdb via @ businesswire\n",
            "31/100 steps\n",
            "loss: 3.6162631511688232\n",
            "32/100 steps\n",
            "loss: 3.6286118030548096\n",
            "33/100 steps\n",
            "loss: 3.930589199066162\n",
            "34/100 steps\n",
            "loss: 3.3573036193847656\n",
            "35/100 steps\n",
            "loss: 3.352520227432251\n",
            "sample predictions:\n",
            "original : \" @ pmadden86 : @ realdonaldtrump trump hotel vegas glistening in sunshine http : / / t. co / o7n1ol784d \"\n",
            "predicted: \" @ pmadden86 : @ realdonaldtrump a hotel vegas glistening in sunshine http : / / t. co / o7n1ol7e4d \"\n",
            "36/100 steps\n",
            "loss: 3.5267138481140137\n",
            "37/100 steps\n",
            "loss: 3.5488991737365723\n",
            "38/100 steps\n",
            "loss: 3.263849973678589\n",
            "39/100 steps\n",
            "loss: 3.4891810417175293\n",
            "40/100 steps\n",
            "loss: 3.302241563796997\n",
            "sample predictions:\n",
            "original : \" @ belllabooo13 : @ foxandfriends mr trump glad to hear you say we are going to make this country great again! your honest you tell it like it \"\n",
            "predicted: \" @ belllabooo13 : @ foxandfriends donald trump glad to see you say you are going to make this country great again! your honest words tell you like it \"\n",
            "41/100 steps\n",
            "loss: 3.3271820545196533\n",
            "42/100 steps\n",
            "loss: 3.464874267578125\n",
            "43/100 steps\n",
            "loss: 3.512457847595215\n",
            "44/100 steps\n",
            "loss: 3.301922559738159\n",
            "45/100 steps\n",
            "loss: 3.348135471343994\n",
            "sample predictions:\n",
            "original : \" @ cnnpolitics : trump : san francisco killing shows perils of illegal immigration http : / / t. co / aibnaohyup ( via @ teddyschleifer ) \"\n",
            "predicted: \" @ trumppolitics # trump : san francisco killing shows perils of illegal immigration http : / / t. co / aibnao7up ( via @ teddy _mfer ) \"\n",
            "46/100 steps\n",
            "loss: 3.240781307220459\n",
            "47/100 steps\n",
            "loss: 3.209822177886963\n",
            "48/100 steps\n",
            "loss: 3.3694193363189697\n",
            "49/100 steps\n",
            "loss: 3.2222084999084473\n",
            "50/100 steps\n",
            "loss: 3.252279281616211\n",
            "sample predictions:\n",
            "original : @ tonycatering @ trumptowerny thanks!\n",
            "predicted: @ tonycatering @ trumptower : thanks!\n",
            "51/100 steps\n",
            "loss: 3.5028209686279297\n",
            "52/100 steps\n",
            "loss: 3.302050828933716\n",
            "53/100 steps\n",
            "loss: 3.2058606147766113\n",
            "54/100 steps\n",
            "loss: 3.1555864810943604\n",
            "55/100 steps\n",
            "loss: 3.0569145679473877\n",
            "sample predictions:\n",
            "original : \" @ 123jayne : that's our president! that's why we need you to run in 2016! i know you would never waste our money! president trump \"\n",
            "predicted: \" @ 123jayne : he's a president! that's why we need you should run in 2016! i know you make a waste of money!!! \"\n",
            "56/100 steps\n",
            "loss: 3.204726457595825\n",
            "57/100 steps\n",
            "loss: 3.3744852542877197\n",
            "58/100 steps\n",
            "loss: 3.1495790481567383\n",
            "59/100 steps\n",
            "loss: 3.3015329837799072\n",
            "60/100 steps\n",
            "loss: 3.1355338096618652\n",
            "sample predictions:\n",
            "original : ... they should realize that these relationships are a good thing, not a bad thing. the u. s. is being respected again. watch trade!\n",
            "predicted: ... they all realize that business relationships are a good thing, but a bad thing. the u. s. is being respected.. watch trade!\n",
            "61/100 steps\n",
            "loss: 3.0198469161987305\n",
            "62/100 steps\n",
            "loss: 3.158639669418335\n",
            "63/100 steps\n",
            "loss: 3.4570937156677246\n",
            "64/100 steps\n",
            "loss: 3.0014700889587402\n",
            "65/100 steps\n",
            "loss: 3.054450273513794\n",
            "sample predictions:\n",
            "original : \" @ franzthompson : @ louisfanucchiiv go vote and support @ realdonaldtrump at http : / / t. co / 9lnakc0kgf \" that sounds cool!!!\n",
            "predicted: \" @ franzthompp : @ louisbrucchiiv gop and support @ realdonaldtrump at http : / / t. co / 9lnakj0lgf \" that sounds cool! thanks!\n",
            "66/100 steps\n",
            "loss: 3.5869672298431396\n",
            "67/100 steps\n",
            "loss: 3.0229177474975586\n",
            "68/100 steps\n",
            "loss: 3.0210373401641846\n",
            "69/100 steps\n",
            "loss: 3.042327642440796\n",
            "70/100 steps\n",
            "loss: 3.103127956390381\n",
            "sample predictions:\n",
            "original : \" @ matteo5anchez : @ realdonaldtrump shut up u stuck up cock! hope u lose ur cash and get run over \" too smart to lose the cash - run over, maybe\n",
            "predicted: \" @ matteo5anchez : @ realdonaldpp shut up u look up cock! hope u lose ur cash and get run. \" too smart to lose the cash - run over, maybe\n",
            "71/100 steps\n",
            "loss: 3.1048989295959473\n",
            "72/100 steps\n",
            "loss: 3.237720489501953\n",
            "73/100 steps\n",
            "loss: 3.0042452812194824\n",
            "74/100 steps\n",
            "loss: 2.9157207012176514\n",
            "75/100 steps\n",
            "loss: 3.1901116371154785\n",
            "sample predictions:\n",
            "original : that trip would be to the trump international hotel las vegas... http : / / www. trump. com / hotel _ collection / trump _ las _ vegas / trump _ las _ vegas. asp\n",
            "predicted: that trip would be to the trump international hotel las vegas... http : / / www. trump. com / t _ co / trump _ las _ vegas / trump _ las _ vegas. asp\n",
            "76/100 steps\n",
            "loss: 2.9300107955932617\n",
            "77/100 steps\n",
            "loss: 3.0508406162261963\n",
            "78/100 steps\n",
            "loss: 3.0964441299438477\n",
            "79/100 steps\n",
            "loss: 3.26765513420105\n",
            "80/100 steps\n",
            "loss: 3.009035110473633\n",
            "sample predictions:\n",
            "original : @ harry _ vickers @ dannyzuker @ apprenticenbc thanks!\n",
            "predicted: @ @ _ vickers @ dannyzuker @ apprenticenbc thanks!\n",
            "81/100 steps\n",
            "loss: 3.0238051414489746\n",
            "82/100 steps\n",
            "loss: 3.209820508956909\n",
            "83/100 steps\n",
            "loss: 2.985100746154785\n",
            "84/100 steps\n",
            "loss: 2.946723222732544\n",
            "85/100 steps\n",
            "loss: 3.0048182010650635\n",
            "sample predictions:\n",
            "original : via @ newsmax _ media by @ owentew : “ trump on 2016 run : i would self - fund, appoint wall street experts ” http : / / t. co / 4ya9kjruxe\n",
            "predicted: via @ newsmax _ co by @ jshow : “ democrats on 2016 run : i would self - be for by wall street experts \" http : / / t. co / oya9kjruqe\n",
            "86/100 steps\n",
            "loss: 3.33909273147583\n",
            "87/100 steps\n",
            "loss: 2.8918046951293945\n",
            "88/100 steps\n",
            "loss: 2.8400511741638184\n",
            "89/100 steps\n",
            "loss: 3.0090901851654053\n",
            "90/100 steps\n",
            "loss: 2.874995231628418\n",
            "sample predictions:\n",
            "original : thank you for the wonderful welcome @ wef! # davos2018 https : / / t. co / iqkoqjxr5b\n",
            "predicted: thank you for the wonderful welcome @ wel! # dapp2018 https : / / t. co / iqkopjxr5b\n",
            "91/100 steps\n",
            "loss: 2.964462995529175\n",
            "92/100 steps\n",
            "loss: 3.1595921516418457\n",
            "93/100 steps\n",
            "loss: 2.9658403396606445\n",
            "94/100 steps\n",
            "loss: 3.0150511264801025\n",
            "95/100 steps\n",
            "loss: 2.9818618297576904\n",
            "sample predictions:\n",
            "original : \" @ mia _ liner : @ gsrpygmies @ realdonaldtrump trump supporters are a very loyal breed! we see the truth, only trump can fix this mess we are in\n",
            "predicted: \" @ mia _ liner : @ gsrpygmies @ realdonaldtrump trump supporters are a very loyal breed! \" see the truth, donald trump can fix this mess we are in\n",
            "96/100 steps\n",
            "loss: 3.3248350620269775\n",
            "97/100 steps\n",
            "loss: 2.8803610801696777\n",
            "98/100 steps\n",
            "loss: 3.282687187194824\n",
            "99/100 steps\n",
            "loss: 2.8554913997650146\n",
            "100/100 steps\n",
            "loss: 2.8455910682678223\n",
            "sample predictions:\n",
            "original : isn't it amazing that @ cnn paid a fortune for an iowa poll, which shows me in first place over cruz by 13 %, 33 % to 20 % - then doesn't use it\n",
            "predicted: isn't it amazing that @ cnn spent a fortune for the iowa poll, which shows me in first place. cruz at 13 %, 9 %, 20 % - then doesn't use.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMPjaTinQcmS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dir = \"models\"\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "model.save_pretrained(model_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24J64_s18fh6",
        "colab_type": "code",
        "outputId": "37403685-9d91-4aba-e951-9d70500cae71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "#generating_model = BertForMaskedLM.from_pretrained('models')\n",
        "generating_model = model\n",
        "generating_model = generating_model.to(device)\n",
        "\n",
        "# Bert as Text Generator: https://arxiv.org/pdf/1902.04094.pdf\n",
        "def generate_tweet(model, length):\n",
        "  tweet = np.random.randint(len(tokenizer), size=(length))\n",
        "  \n",
        "  # uncomment the next line to initialise with real tweet instead\n",
        "  #tweet = examples[np.random.randint(len(examples))]\n",
        "  \n",
        "  # uncomment some of the next lines to use a fixed sentence\n",
        "  #sentence = \"Learn Git and GitHub without any code! Using the Hello World guide, you’ll start a branch, write comments, and open a pull request.\"\n",
        "  sentence = \"Looking forward to the machine learning course at Goethe University Frankfurt! #ifi\"\n",
        "  tweet = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence))\n",
        "  length = len(tweet)\n",
        "  order = np.random.permutation(range(length))\n",
        "  for i in order:\n",
        "    tweet[i] = tokenizer.convert_tokens_to_ids(tokenizer.mask_token)\n",
        "    inputs = torch.as_tensor([tweet], dtype=torch.int64)\n",
        "    labels = inputs.clone()\n",
        "    labels = torch.full((1, length), -1, dtype=torch.int64)\n",
        "    labels[0][i] = inputs[0][i]\n",
        "    inputs = inputs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      outputs = model(inputs)\n",
        "    predictions = predict(inputs, outputs[0], labels)\n",
        "    tweet[i] = predictions[0][i]\n",
        "    print(tokenizer.decode(tweet))\n",
        "  return tweet\n",
        "\n",
        "# Generate a tweet\n",
        "tweet = generate_tweet(generating_model, 20)"
      ],
      "execution_count": 349,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "looking forward to the machine learning course at goethe university frankfurt! # ifi\n",
            "looking forward to the machine learning course at goethe university frankfurt! # miti\n",
            "looking forward to the machine learning course at goethe university frankfurt! # miti\n",
            "looking forward to the machine learning course at goethe university frankfurt! # miti\n",
            "looking forward to the machine learning course at goethe university frankfurt! # miti\n",
            "looking forward to the machine golf course at goethe university frankfurt! # miti\n",
            "looking forward to the machine golf course at goethe university frankfurt! # miti\n",
            "looking forward to the machine golf course at goethe university frankfurt! # mitt\n",
            "looking forward to the machine golf course at goethe university today! # mitt\n",
            "looking forward to the great golf course at goethe university today! # mitt\n",
            "i forward to the great golf course at goethe university today! # mitt\n",
            "i welcome to the great golf course at goethe university today! # mitt\n",
            "i welcome to the great golf course at miami university today! # mitt\n",
            "i welcome to the great golf course at miami university today! # mitt\n",
            "i welcome to the great golf course at miami university today! # mitt\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}