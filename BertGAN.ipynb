{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "BertGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-stoeckel/TrumpTweetGAN/blob/dev_manu/BertGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDYuhETz_1W-",
        "colab_type": "code",
        "outputId": "f0ea7ed6-892e-4eee-8ca9-8350f0e72466",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        }
      },
      "source": [
        "pip install torch transformers tensorboardX"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (1.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.34)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.83)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.8.19)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.9.224)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.13.2)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.224 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.12.224)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorboardX) (41.2.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.224->boto3->transformers) (2.5.3)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.224->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_hP_ivA4e1Q",
        "colab_type": "text"
      },
      "source": [
        "### Aquire corpus from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxLGGUe-tHFI",
        "colab_type": "code",
        "outputId": "154236da-dcb2-4bc2-d396-fc49a5645beb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "!git clone https://github.com/m-stoeckel/TrumpTweetGAN.git\n",
        "!git clone https://github.com/huggingface/transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'TrumpTweetGAN' already exists and is not an empty directory.\n",
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQib-BABtYg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!rm -rf /content/gpt2-trumped /content/output /content/runs \n",
        "!mkdir /content/gpt2-trumped"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaJA-m8yo6-U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee663418-8c7d-4e35-e870-45fcd024fd37"
      },
      "source": [
        "!python ./transformers/examples/run_lm_finetuning.py \\\n",
        "    --do_train \\\n",
        "    --model_type gpt2 --model_name_or_path gpt2 \\\n",
        "    --per_gpu_train_batch_size 2 \\\n",
        "    --train_data_file /content/TrumpTweetGAN/dataset/trump_tweets_2009-2019.txt \\\n",
        "    --eval_data_file /content/TrumpTweetGAN/dataset/testdata/trump_tweets_2009-2019_test.txt \\\n",
        "    --output_dir /content/gpt2-trumped"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "09/27/2019 21:57:22 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "09/27/2019 21:57:23 - INFO - transformers.configuration_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80\n",
            "09/27/2019 21:57:23 - INFO - transformers.configuration_utils -   Model config {\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_labels\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pruned_heads\": {},\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"torchscript\": false,\n",
            "  \"use_bfloat16\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "09/27/2019 21:57:24 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "09/27/2019 21:57:24 - INFO - transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "09/27/2019 21:57:25 - INFO - transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
            "09/27/2019 21:57:34 - INFO - __main__ -   Training/evaluation parameters Namespace(adam_epsilon=1e-08, block_size=1024, cache_dir='', config_name='', device=device(type='cuda'), do_eval=False, do_lower_case=False, do_train=True, eval_all_checkpoints=False, eval_data_file='/content/TrumpTweetGAN/dataset/testdata/trump_tweets_2009-2019_test.txt', evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, learning_rate=5e-05, local_rank=-1, logging_steps=50, max_grad_norm=1.0, max_steps=-1, mlm=False, mlm_probability=0.15, model_name_or_path='gpt2', model_type='gpt2', n_gpu=1, no_cuda=False, num_train_epochs=1.0, output_dir='/content/gpt2-trumped', overwrite_cache=False, overwrite_output_dir=False, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=2, save_steps=50, seed=42, server_ip='', server_port='', tokenizer_name='', train_data_file='/content/TrumpTweetGAN/dataset/trump_tweets_2009-2019.txt', warmup_steps=0, weight_decay=0.0)\n",
            "09/27/2019 21:57:34 - INFO - __main__ -   Loading features from cached file /content/TrumpTweetGAN/dataset/cached_lm_1024_trump_tweets_2009-2019.txt\n",
            "09/27/2019 21:57:34 - INFO - __main__ -   ***** Running training *****\n",
            "09/27/2019 21:57:34 - INFO - __main__ -     Num examples = 952\n",
            "09/27/2019 21:57:34 - INFO - __main__ -     Num Epochs = 1\n",
            "09/27/2019 21:57:34 - INFO - __main__ -     Instantaneous batch size per GPU = 2\n",
            "09/27/2019 21:57:34 - INFO - __main__ -     Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "09/27/2019 21:57:34 - INFO - __main__ -     Gradient Accumulation steps = 1\n",
            "09/27/2019 21:57:34 - INFO - __main__ -     Total optimization steps = 476\n",
            "Epoch:   0% 0/1 [00:00<?, ?it/s]\n",
            "Iteration:   0% 0/476 [00:00<?, ?it/s]\u001b[A\n",
            "Iteration:   0% 1/476 [00:00<06:20,  1.25it/s]\u001b[A\n",
            "Iteration:   0% 2/476 [00:01<06:10,  1.28it/s]\u001b[A\n",
            "Iteration:   1% 3/476 [00:02<06:04,  1.30it/s]\u001b[A\n",
            "Iteration:   1% 4/476 [00:03<06:00,  1.31it/s]\u001b[A\n",
            "Iteration:   1% 5/476 [00:03<05:57,  1.32it/s]\u001b[A\n",
            "Iteration:   1% 6/476 [00:04<05:56,  1.32it/s]\u001b[A\n",
            "Iteration:   1% 7/476 [00:05<05:53,  1.33it/s]\u001b[A\n",
            "Iteration:   2% 8/476 [00:06<05:52,  1.33it/s]\u001b[A\n",
            "Iteration:   2% 9/476 [00:06<05:51,  1.33it/s]\u001b[A\n",
            "Iteration:   2% 10/476 [00:07<05:50,  1.33it/s]\u001b[A\n",
            "Iteration:   2% 11/476 [00:08<05:48,  1.33it/s]\u001b[A\n",
            "Iteration:   3% 12/476 [00:09<05:47,  1.33it/s]\u001b[A\n",
            "Iteration:   3% 13/476 [00:09<05:46,  1.33it/s]\u001b[A\n",
            "Iteration:   3% 14/476 [00:10<05:46,  1.33it/s]\u001b[A\n",
            "Iteration:   3% 15/476 [00:11<05:45,  1.33it/s]\u001b[A\n",
            "Iteration:   3% 16/476 [00:12<05:45,  1.33it/s]\u001b[A\n",
            "Iteration:   4% 17/476 [00:12<05:45,  1.33it/s]\u001b[A\n",
            "Iteration:   4% 18/476 [00:13<05:45,  1.33it/s]\u001b[A\n",
            "Iteration:   4% 19/476 [00:14<05:44,  1.32it/s]\u001b[A\n",
            "Iteration:   4% 20/476 [00:15<05:46,  1.32it/s]\u001b[A\n",
            "Iteration:   4% 21/476 [00:15<05:46,  1.31it/s]\u001b[A\n",
            "Iteration:   5% 22/476 [00:16<05:45,  1.31it/s]\u001b[A\n",
            "Iteration:   5% 23/476 [00:17<05:45,  1.31it/s]\u001b[A\n",
            "Iteration:   5% 24/476 [00:18<05:46,  1.31it/s]\u001b[A\n",
            "Iteration:   5% 25/476 [00:18<05:45,  1.30it/s]\u001b[A\n",
            "Iteration:   5% 26/476 [00:19<05:45,  1.30it/s]\u001b[A\n",
            "Iteration:   6% 27/476 [00:20<05:45,  1.30it/s]\u001b[A\n",
            "Iteration:   6% 28/476 [00:21<05:44,  1.30it/s]\u001b[A\n",
            "Iteration:   6% 29/476 [00:21<05:44,  1.30it/s]\u001b[A\n",
            "Iteration:   6% 30/476 [00:22<05:43,  1.30it/s]\u001b[A\n",
            "Iteration:   7% 31/476 [00:23<05:42,  1.30it/s]\u001b[A\n",
            "Iteration:   7% 32/476 [00:24<05:43,  1.29it/s]\u001b[A\n",
            "Iteration:   7% 33/476 [00:25<05:43,  1.29it/s]\u001b[A\n",
            "Iteration:   7% 34/476 [00:25<05:44,  1.28it/s]\u001b[A\n",
            "Iteration:   7% 35/476 [00:26<05:45,  1.28it/s]\u001b[A\n",
            "Iteration:   8% 36/476 [00:27<05:46,  1.27it/s]\u001b[A\n",
            "Iteration:   8% 37/476 [00:28<05:46,  1.27it/s]\u001b[A\n",
            "Iteration:   8% 38/476 [00:29<05:45,  1.27it/s]\u001b[A\n",
            "Iteration:   8% 39/476 [00:29<05:45,  1.27it/s]\u001b[A\n",
            "Iteration:   8% 40/476 [00:30<05:44,  1.26it/s]\u001b[A\n",
            "Iteration:   9% 41/476 [00:31<05:43,  1.26it/s]\u001b[A\n",
            "Iteration:   9% 42/476 [00:32<05:44,  1.26it/s]\u001b[A\n",
            "Iteration:   9% 43/476 [00:33<05:44,  1.26it/s]\u001b[A\n",
            "Iteration:   9% 44/476 [00:33<05:43,  1.26it/s]\u001b[A\n",
            "Iteration:   9% 45/476 [00:34<05:43,  1.25it/s]\u001b[A\n",
            "Iteration:  10% 46/476 [00:35<05:42,  1.26it/s]\u001b[A\n",
            "Iteration:  10% 47/476 [00:36<05:40,  1.26it/s]\u001b[A\n",
            "Iteration:  10% 48/476 [00:36<05:40,  1.26it/s]\u001b[A\n",
            "Iteration:  10% 49/476 [00:37<05:40,  1.25it/s]\u001b[A09/27/2019 21:58:13 - INFO - transformers.configuration_utils -   Configuration saved in /content/gpt2-trumped/checkpoint-50/config.json\n",
            "09/27/2019 21:58:14 - INFO - transformers.modeling_utils -   Model weights saved in /content/gpt2-trumped/checkpoint-50/pytorch_model.bin\n",
            "09/27/2019 21:58:14 - INFO - __main__ -   Saving model checkpoint to /content/gpt2-trumped/checkpoint-50\n",
            "\n",
            "Iteration:  11% 50/476 [00:40<09:06,  1.28s/it]\u001b[A\n",
            "Iteration:  11% 51/476 [00:40<07:58,  1.12s/it]\u001b[A\n",
            "Iteration:  11% 52/476 [00:41<07:18,  1.03s/it]\u001b[A\n",
            "Iteration:  11% 53/476 [00:42<06:49,  1.03it/s]\u001b[A\n",
            "Iteration:  11% 54/476 [00:43<06:29,  1.08it/s]\u001b[A\n",
            "Iteration:  12% 55/476 [00:44<06:14,  1.12it/s]\u001b[A\n",
            "Iteration:  12% 56/476 [00:45<06:03,  1.15it/s]\u001b[A\n",
            "Iteration:  12% 57/476 [00:45<05:56,  1.18it/s]\u001b[A\n",
            "Iteration:  12% 58/476 [00:46<05:51,  1.19it/s]\u001b[A\n",
            "Iteration:  12% 59/476 [00:47<05:49,  1.19it/s]\u001b[A\n",
            "Iteration:  13% 60/476 [00:48<05:47,  1.20it/s]\u001b[A\n",
            "Iteration:  13% 61/476 [00:49<05:46,  1.20it/s]\u001b[A\n",
            "Iteration:  13% 62/476 [00:50<05:44,  1.20it/s]\u001b[A\n",
            "Iteration:  13% 63/476 [00:50<05:44,  1.20it/s]\u001b[A\n",
            "Iteration:  13% 64/476 [00:51<05:43,  1.20it/s]\u001b[A\n",
            "Iteration:  14% 65/476 [00:52<05:42,  1.20it/s]\u001b[A\n",
            "Iteration:  14% 66/476 [00:53<05:41,  1.20it/s]\u001b[A\n",
            "Iteration:  14% 67/476 [00:54<05:41,  1.20it/s]\u001b[A\n",
            "Iteration:  14% 68/476 [00:55<05:41,  1.19it/s]\u001b[A\n",
            "Iteration:  14% 69/476 [00:55<05:42,  1.19it/s]\u001b[A\n",
            "Iteration:  15% 70/476 [00:56<05:42,  1.18it/s]\u001b[A\n",
            "Iteration:  15% 71/476 [00:57<05:42,  1.18it/s]\u001b[A\n",
            "Iteration:  15% 72/476 [00:58<05:42,  1.18it/s]\u001b[A\n",
            "Iteration:  15% 73/476 [00:59<05:42,  1.18it/s]\u001b[A\n",
            "Iteration:  16% 74/476 [01:00<05:41,  1.18it/s]\u001b[A\n",
            "Iteration:  16% 75/476 [01:00<05:41,  1.17it/s]\u001b[A\n",
            "Iteration:  16% 76/476 [01:01<05:41,  1.17it/s]\u001b[A\n",
            "Iteration:  16% 77/476 [01:02<05:40,  1.17it/s]\u001b[A\n",
            "Iteration:  16% 78/476 [01:03<05:39,  1.17it/s]\u001b[A\n",
            "Iteration:  17% 79/476 [01:04<05:37,  1.18it/s]\u001b[A\n",
            "Iteration:  17% 80/476 [01:05<05:37,  1.17it/s]\u001b[A\n",
            "Iteration:  17% 81/476 [01:06<05:35,  1.18it/s]\u001b[A\n",
            "Iteration:  17% 82/476 [01:06<05:34,  1.18it/s]\u001b[A\n",
            "Iteration:  17% 83/476 [01:07<05:33,  1.18it/s]\u001b[A\n",
            "Iteration:  18% 84/476 [01:08<05:32,  1.18it/s]\u001b[A\n",
            "Iteration:  18% 85/476 [01:09<05:31,  1.18it/s]\u001b[A\n",
            "Iteration:  18% 86/476 [01:10<05:30,  1.18it/s]\u001b[A\n",
            "Iteration:  18% 87/476 [01:11<05:29,  1.18it/s]\u001b[A\n",
            "Iteration:  18% 88/476 [01:12<05:28,  1.18it/s]\u001b[A\n",
            "Iteration:  19% 89/476 [01:12<05:26,  1.19it/s]\u001b[A\n",
            "Iteration:  19% 90/476 [01:13<05:23,  1.19it/s]\u001b[A\n",
            "Iteration:  19% 91/476 [01:14<05:21,  1.20it/s]\u001b[A\n",
            "Iteration:  19% 92/476 [01:15<05:19,  1.20it/s]\u001b[A\n",
            "Iteration:  20% 93/476 [01:16<05:18,  1.20it/s]\u001b[A\n",
            "Iteration:  20% 94/476 [01:16<05:17,  1.20it/s]\u001b[A\n",
            "Iteration:  20% 95/476 [01:17<05:16,  1.20it/s]\u001b[A\n",
            "Iteration:  20% 96/476 [01:18<05:15,  1.21it/s]\u001b[A\n",
            "Iteration:  20% 97/476 [01:19<05:14,  1.21it/s]\u001b[A\n",
            "Iteration:  21% 98/476 [01:20<05:11,  1.21it/s]\u001b[A\n",
            "Iteration:  21% 99/476 [01:21<05:10,  1.21it/s]\u001b[A09/27/2019 21:58:56 - INFO - transformers.configuration_utils -   Configuration saved in /content/gpt2-trumped/checkpoint-100/config.json\n",
            "09/27/2019 21:58:57 - INFO - transformers.modeling_utils -   Model weights saved in /content/gpt2-trumped/checkpoint-100/pytorch_model.bin\n",
            "09/27/2019 21:58:57 - INFO - __main__ -   Saving model checkpoint to /content/gpt2-trumped/checkpoint-100\n",
            "\n",
            "Iteration:  21% 100/476 [01:23<08:05,  1.29s/it]\u001b[A\n",
            "Iteration:  21% 101/476 [01:24<07:04,  1.13s/it]\u001b[A\n",
            "Iteration:  21% 102/476 [01:25<06:29,  1.04s/it]\u001b[A\n",
            "Iteration:  22% 103/476 [01:25<06:03,  1.03it/s]\u001b[A\n",
            "Iteration:  22% 104/476 [01:26<05:43,  1.08it/s]\u001b[A\n",
            "Iteration:  22% 105/476 [01:27<05:29,  1.13it/s]\u001b[A\n",
            "Iteration:  22% 106/476 [01:28<05:19,  1.16it/s]\u001b[A\n",
            "Iteration:  22% 107/476 [01:29<05:12,  1.18it/s]\u001b[A\n",
            "Iteration:  23% 108/476 [01:29<05:08,  1.19it/s]\u001b[A\n",
            "Iteration:  23% 109/476 [01:30<05:04,  1.21it/s]\u001b[A\n",
            "Iteration:  23% 110/476 [01:31<05:00,  1.22it/s]\u001b[A\n",
            "Iteration:  23% 111/476 [01:32<04:58,  1.22it/s]\u001b[A\n",
            "Iteration:  24% 112/476 [01:33<04:56,  1.23it/s]\u001b[A\n",
            "Iteration:  24% 113/476 [01:33<04:55,  1.23it/s]\u001b[A\n",
            "Iteration:  24% 114/476 [01:34<04:54,  1.23it/s]\u001b[A\n",
            "Iteration:  24% 115/476 [01:35<04:52,  1.23it/s]\u001b[A\n",
            "Iteration:  24% 116/476 [01:36<04:51,  1.23it/s]\u001b[A\n",
            "Iteration:  25% 117/476 [01:37<04:51,  1.23it/s]\u001b[A\n",
            "Iteration:  25% 118/476 [01:38<04:50,  1.23it/s]\u001b[A\n",
            "Iteration:  25% 119/476 [01:38<04:49,  1.23it/s]\u001b[A\n",
            "Iteration:  25% 120/476 [01:39<04:48,  1.23it/s]\u001b[A\n",
            "Iteration:  25% 121/476 [01:40<04:47,  1.24it/s]\u001b[A\n",
            "Iteration:  26% 122/476 [01:41<04:46,  1.24it/s]\u001b[A\n",
            "Iteration:  26% 123/476 [01:42<04:44,  1.24it/s]\u001b[A\n",
            "Iteration:  26% 124/476 [01:42<04:43,  1.24it/s]\u001b[A\n",
            "Iteration:  26% 125/476 [01:43<04:42,  1.24it/s]\u001b[A\n",
            "Iteration:  26% 126/476 [01:44<04:42,  1.24it/s]\u001b[A\n",
            "Iteration:  27% 127/476 [01:45<04:42,  1.24it/s]\u001b[A\n",
            "Iteration:  27% 128/476 [01:46<04:41,  1.24it/s]\u001b[A\n",
            "Iteration:  27% 129/476 [01:46<04:40,  1.24it/s]\u001b[A\n",
            "Iteration:  27% 130/476 [01:47<04:40,  1.23it/s]\u001b[A\n",
            "Iteration:  28% 131/476 [01:48<04:39,  1.23it/s]\u001b[A\n",
            "Iteration:  28% 132/476 [01:49<04:39,  1.23it/s]\u001b[A\n",
            "Iteration:  28% 133/476 [01:50<04:37,  1.24it/s]\u001b[A\n",
            "Iteration:  28% 134/476 [01:50<04:37,  1.23it/s]\u001b[A\n",
            "Iteration:  28% 135/476 [01:51<04:36,  1.23it/s]\u001b[A\n",
            "Iteration:  29% 136/476 [01:52<04:35,  1.23it/s]\u001b[A\n",
            "Iteration:  29% 137/476 [01:53<04:34,  1.23it/s]\u001b[A\n",
            "Iteration:  29% 138/476 [01:54<04:33,  1.23it/s]\u001b[A\n",
            "Iteration:  29% 139/476 [01:55<04:33,  1.23it/s]\u001b[A\n",
            "Iteration:  29% 140/476 [01:55<04:32,  1.23it/s]\u001b[A\n",
            "Iteration:  30% 141/476 [01:56<04:31,  1.23it/s]\u001b[A\n",
            "Iteration:  30% 142/476 [01:57<04:30,  1.23it/s]\u001b[A\n",
            "Iteration:  30% 143/476 [01:58<04:29,  1.24it/s]\u001b[A\n",
            "Iteration:  30% 144/476 [01:59<04:29,  1.23it/s]\u001b[A\n",
            "Iteration:  30% 145/476 [01:59<04:28,  1.23it/s]\u001b[A\n",
            "Iteration:  31% 146/476 [02:00<04:29,  1.23it/s]\u001b[A\n",
            "Iteration:  31% 147/476 [02:01<04:28,  1.23it/s]\u001b[A\n",
            "Iteration:  31% 148/476 [02:02<04:28,  1.22it/s]\u001b[A\n",
            "Iteration:  31% 149/476 [02:03<04:27,  1.22it/s]\u001b[A09/27/2019 21:59:38 - INFO - transformers.configuration_utils -   Configuration saved in /content/gpt2-trumped/checkpoint-150/config.json\n",
            "09/27/2019 21:59:40 - INFO - transformers.modeling_utils -   Model weights saved in /content/gpt2-trumped/checkpoint-150/pytorch_model.bin\n",
            "09/27/2019 21:59:40 - INFO - __main__ -   Saving model checkpoint to /content/gpt2-trumped/checkpoint-150\n",
            "\n",
            "Iteration:  32% 150/476 [02:05<07:28,  1.38s/it]\u001b[A\n",
            "Iteration:  32% 151/476 [02:06<06:27,  1.19s/it]\u001b[A\n",
            "Iteration:  32% 152/476 [02:07<05:51,  1.09s/it]\u001b[A\n",
            "Iteration:  32% 153/476 [02:08<05:24,  1.01s/it]\u001b[A\n",
            "Iteration:  32% 154/476 [02:09<05:06,  1.05it/s]\u001b[A\n",
            "Iteration:  33% 155/476 [02:09<04:53,  1.09it/s]\u001b[A\n",
            "Iteration:  33% 156/476 [02:10<04:43,  1.13it/s]\u001b[A\n",
            "Iteration:  33% 157/476 [02:11<04:36,  1.16it/s]\u001b[A\n",
            "Iteration:  33% 158/476 [02:12<04:31,  1.17it/s]\u001b[A\n",
            "Iteration:  33% 159/476 [02:13<04:29,  1.18it/s]\u001b[A\n",
            "Iteration:  34% 160/476 [02:14<04:26,  1.19it/s]\u001b[A\n",
            "Iteration:  34% 161/476 [02:14<04:24,  1.19it/s]\u001b[A\n",
            "Iteration:  34% 162/476 [02:15<04:22,  1.20it/s]\u001b[A\n",
            "Iteration:  34% 163/476 [02:16<04:21,  1.20it/s]\u001b[A\n",
            "Iteration:  34% 164/476 [02:17<04:19,  1.20it/s]\u001b[A\n",
            "Iteration:  35% 165/476 [02:18<04:19,  1.20it/s]\u001b[A\n",
            "Iteration:  35% 166/476 [02:19<04:18,  1.20it/s]\u001b[A\n",
            "Iteration:  35% 167/476 [02:19<04:16,  1.20it/s]\u001b[A\n",
            "Iteration:  35% 168/476 [02:20<04:16,  1.20it/s]\u001b[A\n",
            "Iteration:  36% 169/476 [02:21<04:15,  1.20it/s]\u001b[A\n",
            "Iteration:  36% 170/476 [02:22<04:14,  1.20it/s]\u001b[A\n",
            "Iteration:  36% 171/476 [02:23<04:13,  1.20it/s]\u001b[A\n",
            "Iteration:  36% 172/476 [02:24<04:12,  1.20it/s]\u001b[A\n",
            "Iteration:  36% 173/476 [02:24<04:11,  1.20it/s]\u001b[A\n",
            "Iteration:  37% 174/476 [02:25<04:10,  1.21it/s]\u001b[A\n",
            "Iteration:  37% 175/476 [02:26<04:09,  1.21it/s]\u001b[A\n",
            "Iteration:  37% 176/476 [02:27<04:09,  1.20it/s]\u001b[A\n",
            "Iteration:  37% 177/476 [02:28<04:08,  1.20it/s]\u001b[A\n",
            "Iteration:  37% 178/476 [02:29<04:07,  1.21it/s]\u001b[A\n",
            "Iteration:  38% 179/476 [02:29<04:06,  1.21it/s]\u001b[A\n",
            "Iteration:  38% 180/476 [02:30<04:05,  1.21it/s]\u001b[A\n",
            "Iteration:  38% 181/476 [02:31<04:04,  1.20it/s]\u001b[A\n",
            "Iteration:  38% 182/476 [02:32<04:04,  1.20it/s]\u001b[A\n",
            "Iteration:  38% 183/476 [02:33<04:03,  1.21it/s]\u001b[A\n",
            "Iteration:  39% 184/476 [02:33<04:02,  1.20it/s]\u001b[A\n",
            "Iteration:  39% 185/476 [02:34<04:01,  1.21it/s]\u001b[A\n",
            "Iteration:  39% 186/476 [02:35<04:00,  1.20it/s]\u001b[A\n",
            "Iteration:  39% 187/476 [02:36<03:59,  1.20it/s]\u001b[A\n",
            "Iteration:  39% 188/476 [02:37<03:58,  1.21it/s]\u001b[A\n",
            "Iteration:  40% 189/476 [02:38<03:58,  1.21it/s]\u001b[A\n",
            "Iteration:  40% 190/476 [02:38<03:57,  1.21it/s]\u001b[A\n",
            "Iteration:  40% 191/476 [02:39<03:56,  1.21it/s]\u001b[A\n",
            "Iteration:  40% 192/476 [02:40<03:55,  1.21it/s]\u001b[A\n",
            "Iteration:  41% 193/476 [02:41<03:54,  1.21it/s]\u001b[A\n",
            "Iteration:  41% 194/476 [02:42<03:53,  1.21it/s]\u001b[A\n",
            "Iteration:  41% 195/476 [02:43<03:52,  1.21it/s]\u001b[A\n",
            "Iteration:  41% 196/476 [02:43<03:50,  1.21it/s]\u001b[A\n",
            "Iteration:  41% 197/476 [02:44<03:49,  1.21it/s]\u001b[A\n",
            "Iteration:  42% 198/476 [02:45<03:48,  1.22it/s]\u001b[A"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O93rOcqctPQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python ./transformers/examples/run_generation.py \\\n",
        "    --model_type=gpt2 \\\n",
        "    --length=20 \\\n",
        "    --model_name_or_path=gpt2-trumped \\"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}