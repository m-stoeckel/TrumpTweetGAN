{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "BertGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-stoeckel/TrumpTweetGAN/blob/dev_manu/BertGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDYuhETz_1W-",
        "colab_type": "code",
        "outputId": "6404c868-684b-4964-a4bc-96c891ccc30e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "pip install torch transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.1.0)\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/99/ca0e4c35ccde7d290de3c9c236d5629d1879b04927e5ace9bd6d9183e236/transformers-2.0.0-py3-none-any.whl (290kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.16.5)\n",
            "Collecting sentencepiece (from transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 43.2MB/s \n",
            "\u001b[?25hCollecting regex (from transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/a6/99eeb5904ab763db87af4bd71d9b1dfdd9792681240657a4c0a599c10a81/regex-2019.08.19.tar.gz (654kB)\n",
            "\u001b[K     |████████████████████████████████| 655kB 46.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.9.224)\n",
            "Collecting sacremoses (from transformers)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/04/b92425ca552116afdb7698fa3f00ca1c975cfd86a847cf132fd813c5d901/sacremoses-0.0.34.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 37.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.224 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.12.224)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.13.2)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.6.16)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.224->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.224->boto3->transformers) (2.5.3)\n",
            "Building wheels for collected packages: regex, sacremoses\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2019.8.19-cp36-cp36m-linux_x86_64.whl size=609230 sha256=a6b8742d61a889409b2ae252471f662db6d2a39fa4328ef78f36f65195109b10\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/04/07/b5010fb816721eb3d6dd64ed5cc8111ca23f97fdab8619b5be\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.34-cp36-none-any.whl size=883992 sha256=f114087afcd3c3cbded93e8a910f2d3bc88c7c3fb28f39fa7157e660a2f644a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/07/b9/5b/8bd674c23e962fbff34420a9fa7a2c374d591ecadd5bc37684\n",
            "Successfully built regex sacremoses\n",
            "Installing collected packages: sentencepiece, regex, sacremoses, transformers\n",
            "Successfully installed regex-2019.8.19 sacremoses-0.0.34 sentencepiece-0.1.83 transformers-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_hP_ivA4e1Q",
        "colab_type": "text"
      },
      "source": [
        "### Aquire corpus from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxLGGUe-tHFI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "00f12ab9-e58a-4944-feb1-5af489d2ca02"
      },
      "source": [
        "!git clone https://github.com/m-stoeckel/TrumpTweetGAN.git\n",
        "!git clone https://github.com/huggingface/transformers\n",
        "!git clone https://github.com/nshepperd/gpt-2.git\n",
        "!pip install -r gpt-2/requirements.txt\n",
        "!python ./gpt-2/download_model.py 117M"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TrumpTweetGAN'...\n",
            "remote: Enumerating objects: 680, done.\u001b[K\n",
            "remote: Counting objects: 100% (680/680), done.\u001b[K\n",
            "remote: Compressing objects: 100% (278/278), done.\u001b[K\n",
            "remote: Total 680 (delta 395), reused 670 (delta 391), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (680/680), 33.19 MiB | 11.46 MiB/s, done.\n",
            "Resolving deltas: 100% (395/395), done.\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 9150 (delta 10), reused 11 (delta 5), pack-reused 9124\u001b[K\n",
            "Receiving objects: 100% (9150/9150), 4.85 MiB | 4.33 MiB/s, done.\n",
            "Resolving deltas: 100% (6621/6621), done.\n",
            "Cloning into 'gpt-2'...\n",
            "remote: Enumerating objects: 310, done.\u001b[K\n",
            "remote: Total 310 (delta 0), reused 0 (delta 0), pack-reused 310\u001b[K\n",
            "Receiving objects: 100% (310/310), 4.41 MiB | 3.33 MiB/s, done.\n",
            "Resolving deltas: 100% (167/167), done.\n",
            "Collecting fire>=0.1.3 (from -r gpt-2/requirements.txt (line 1))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/69/faeaae8687f4de0f5973694d02e9d6c3eb827636a009157352d98de1129e/fire-0.2.1.tar.gz (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 3.2MB/s \n",
            "\u001b[?25hCollecting regex==2017.4.5 (from -r gpt-2/requirements.txt (line 2))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n",
            "\u001b[K     |████████████████████████████████| 604kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests==2.21.0 in /usr/local/lib/python3.6/dist-packages (from -r gpt-2/requirements.txt (line 3)) (2.21.0)\n",
            "Collecting tqdm==4.31.1 (from -r gpt-2/requirements.txt (line 4))\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/4b/c38b5144cf167c4f52288517436ccafefe9dc01b8d1c190e18a6b154cd4a/tqdm-4.31.1-py2.py3-none-any.whl (48kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 19.9MB/s \n",
            "\u001b[?25hCollecting toposort==1.5 (from -r gpt-2/requirements.txt (line 5))\n",
            "  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r gpt-2/requirements.txt (line 1)) (1.12.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from fire>=0.1.3->-r gpt-2/requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r gpt-2/requirements.txt (line 3)) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r gpt-2/requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r gpt-2/requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests==2.21.0->-r gpt-2/requirements.txt (line 3)) (2019.6.16)\n",
            "Building wheels for collected packages: fire, regex\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.2.1-py2.py3-none-any.whl size=103527 sha256=0b3cfdbdc09c7ebf1994dccfa7e66792ca8dd9430352120640f96fb9ebeff6b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/9c/c0/07b6dc7faf1844bb4688f46b569efe6cafaa2179c95db821da\n",
            "  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for regex: filename=regex-2017.4.5-cp36-cp36m-linux_x86_64.whl size=533186 sha256=1c0be16619947cda68fa4b1306eeca18a919dba0804d15eafea620cd0f2f17f0\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n",
            "Successfully built fire regex\n",
            "Installing collected packages: fire, regex, tqdm, toposort\n",
            "  Found existing installation: regex 2019.8.19\n",
            "    Uninstalling regex-2019.8.19:\n",
            "      Successfully uninstalled regex-2019.8.19\n",
            "  Found existing installation: tqdm 4.28.1\n",
            "    Uninstalling tqdm-4.28.1:\n",
            "      Successfully uninstalled tqdm-4.28.1\n",
            "Successfully installed fire-0.2.1 regex-2017.4.5 toposort-1.5 tqdm-4.31.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 574kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 38.7Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 661kit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:13, 37.5Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 3.09Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 30.8Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 36.4Mit/s]                                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O93rOcqctPQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !python ./transformers/examples/run_generation.py \\\n",
        "#     --model_type=xlnet \\\n",
        "#     --length=20 \\\n",
        "#     --model_name_or_path=xlnet-base-cased \\"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wlWKlWe5xwh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a496d39-c763-4956-d0e6-6a2d3fd0b392"
      },
      "source": [
        "!PYTHONPATH=/content/gpt-2/src ./gpt-2/train.py --dataset /content/TrumpTweetGAN/dataset/trump_tweets_2009-2019.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /content/gpt-2/src/model.py:147: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/memory_saving_gradients.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From ./gpt-2/train.py:88: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From ./gpt-2/train.py:91: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "2019-09-27 18:31:46.037602: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2200000000 Hz\n",
            "2019-09-27 18:31:46.039123: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2341480 executing computations on platform Host. Devices:\n",
            "2019-09-27 18:31:46.039157: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-09-27 18:31:46.043432: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
            "2019-09-27 18:31:46.211510: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-27 18:31:46.212463: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x2341640 executing computations on platform CUDA. Devices:\n",
            "2019-09-27 18:31:46.212524: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-09-27 18:31:46.213835: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-27 18:31:46.214568: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "2019-09-27 18:31:46.222287: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-27 18:31:46.419686: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "2019-09-27 18:31:46.515853: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcufft.so.10.0\n",
            "2019-09-27 18:31:46.538974: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcurand.so.10.0\n",
            "2019-09-27 18:31:46.764089: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2019-09-27 18:31:46.888639: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2019-09-27 18:31:47.307237: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
            "2019-09-27 18:31:47.307568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-27 18:31:47.308537: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-27 18:31:47.309265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1763] Adding visible gpu devices: 0\n",
            "2019-09-27 18:31:47.312966: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudart.so.10.0\n",
            "2019-09-27 18:31:47.314686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-09-27 18:31:47.314721: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 \n",
            "2019-09-27 18:31:47.314736: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N \n",
            "2019-09-27 18:31:47.322232: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-27 18:31:47.323045: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:1005] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-09-27 18:31:47.323768: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1326] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10805 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From ./gpt-2/train.py:92: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:65: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:16: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/gpt-2/src/sample.py:70: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "WARNING:tensorflow:From ./gpt-2/train.py:121: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From ./gpt-2/train.py:144: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From ./gpt-2/train.py:147: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "WARNING:tensorflow:From ./gpt-2/train.py:149: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From ./gpt-2/train.py:152: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "Loading checkpoint models/117M/model.ckpt\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Loading dataset...\n",
            "100% 1/1 [00:05<00:00,  5.93s/it]\n",
            "dataset has 975111 tokens\n",
            "Training...\n",
            "2019-09-27 18:32:18.077720: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n",
            "2019-09-27 18:32:18.364984: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcublas.so.10.0\n",
            "[1 | 5.53] loss=3.28 avg=3.28\n",
            "[2 | 6.47] loss=3.35 avg=3.32\n",
            "[3 | 7.39] loss=3.26 avg=3.30\n",
            "[4 | 8.33] loss=3.62 avg=3.38\n",
            "[5 | 9.25] loss=3.33 avg=3.37\n",
            "[6 | 10.19] loss=3.00 avg=3.31\n",
            "[7 | 11.12] loss=3.27 avg=3.30\n",
            "[8 | 12.06] loss=3.64 avg=3.34\n",
            "[9 | 13.00] loss=3.21 avg=3.33\n",
            "[10 | 13.93] loss=3.34 avg=3.33\n",
            "[11 | 14.87] loss=3.62 avg=3.36\n",
            "[12 | 15.81] loss=3.53 avg=3.37\n",
            "[13 | 16.73] loss=2.98 avg=3.34\n",
            "[14 | 17.68] loss=3.11 avg=3.32\n",
            "[15 | 18.61] loss=3.26 avg=3.32\n",
            "[16 | 19.55] loss=3.05 avg=3.30\n",
            "[17 | 20.49] loss=3.33 avg=3.30\n",
            "[18 | 21.42] loss=3.07 avg=3.29\n",
            "[19 | 22.35] loss=3.01 avg=3.27\n",
            "[20 | 23.29] loss=3.28 avg=3.27\n",
            "[21 | 24.22] loss=3.40 avg=3.28\n",
            "[22 | 25.16] loss=3.17 avg=3.27\n",
            "[23 | 26.09] loss=3.32 avg=3.28\n",
            "[24 | 27.02] loss=3.28 avg=3.28\n",
            "[25 | 27.96] loss=3.05 avg=3.27\n",
            "[26 | 28.90] loss=3.21 avg=3.26\n",
            "[27 | 29.83] loss=3.16 avg=3.26\n",
            "[28 | 30.77] loss=3.43 avg=3.27\n",
            "[29 | 31.70] loss=3.12 avg=3.26\n",
            "[30 | 32.64] loss=3.42 avg=3.27\n",
            "[31 | 33.57] loss=2.76 avg=3.25\n",
            "[32 | 34.50] loss=3.51 avg=3.26\n",
            "[33 | 35.44] loss=3.09 avg=3.25\n",
            "[34 | 36.37] loss=2.86 avg=3.24\n",
            "[35 | 37.30] loss=3.14 avg=3.23\n",
            "[36 | 38.23] loss=2.75 avg=3.22\n",
            "[37 | 39.17] loss=2.98 avg=3.21\n",
            "[38 | 40.10] loss=2.90 avg=3.20\n",
            "[39 | 41.03] loss=3.25 avg=3.20\n",
            "[40 | 41.97] loss=3.49 avg=3.21\n",
            "[41 | 42.90] loss=2.79 avg=3.20\n",
            "[42 | 43.84] loss=3.00 avg=3.19\n",
            "[43 | 44.77] loss=3.12 avg=3.19\n",
            "[44 | 45.71] loss=2.95 avg=3.18\n",
            "[45 | 46.64] loss=3.18 avg=3.18\n",
            "[46 | 47.57] loss=3.06 avg=3.18\n",
            "[47 | 48.50] loss=3.05 avg=3.18\n",
            "[48 | 49.44] loss=3.04 avg=3.17\n",
            "[49 | 50.37] loss=3.15 avg=3.17\n",
            "[50 | 51.30] loss=3.10 avg=3.17\n",
            "[51 | 52.24] loss=3.09 avg=3.17\n",
            "[52 | 53.16] loss=2.96 avg=3.16\n",
            "[53 | 54.10] loss=3.10 avg=3.16\n",
            "[54 | 55.03] loss=2.97 avg=3.16\n",
            "[55 | 55.96] loss=3.17 avg=3.16\n",
            "[56 | 56.88] loss=3.13 avg=3.16\n",
            "[57 | 57.81] loss=3.29 avg=3.16\n",
            "[58 | 58.75] loss=2.92 avg=3.16\n",
            "[59 | 59.68] loss=3.21 avg=3.16\n",
            "[60 | 60.62] loss=3.29 avg=3.16\n",
            "[61 | 61.54] loss=3.39 avg=3.16\n",
            "[62 | 62.48] loss=3.25 avg=3.17\n",
            "[63 | 63.41] loss=3.18 avg=3.17\n",
            "[64 | 64.34] loss=3.28 avg=3.17\n",
            "[65 | 65.27] loss=3.10 avg=3.17\n",
            "[66 | 66.21] loss=2.96 avg=3.16\n",
            "[67 | 67.13] loss=3.04 avg=3.16\n",
            "[68 | 68.06] loss=2.97 avg=3.16\n",
            "[69 | 68.99] loss=2.78 avg=3.15\n",
            "[70 | 69.93] loss=2.74 avg=3.14\n",
            "[71 | 70.86] loss=3.29 avg=3.14\n",
            "[72 | 71.79] loss=2.68 avg=3.13\n",
            "[73 | 72.72] loss=3.30 avg=3.14\n",
            "[74 | 73.65] loss=3.09 avg=3.14\n",
            "[75 | 74.57] loss=2.61 avg=3.13\n",
            "[76 | 75.51] loss=3.06 avg=3.13\n",
            "[77 | 76.44] loss=3.12 avg=3.13\n",
            "[78 | 77.37] loss=3.09 avg=3.13\n",
            "[79 | 78.29] loss=2.63 avg=3.12\n",
            "[80 | 79.22] loss=2.88 avg=3.11\n",
            "[81 | 80.15] loss=3.06 avg=3.11\n",
            "[82 | 81.08] loss=3.02 avg=3.11\n",
            "[83 | 82.01] loss=3.13 avg=3.11\n",
            "[84 | 82.94] loss=3.07 avg=3.11\n",
            "[85 | 83.87] loss=2.93 avg=3.11\n",
            "[86 | 84.80] loss=2.98 avg=3.10\n",
            "[87 | 85.73] loss=3.09 avg=3.10\n",
            "[88 | 86.66] loss=2.87 avg=3.10\n",
            "[89 | 87.59] loss=2.88 avg=3.10\n",
            "[90 | 88.52] loss=2.64 avg=3.09\n",
            "[91 | 89.46] loss=3.12 avg=3.09\n",
            "[92 | 90.39] loss=3.06 avg=3.09\n",
            "[93 | 91.33] loss=3.11 avg=3.09\n",
            "[94 | 92.25] loss=2.84 avg=3.08\n",
            "[95 | 93.18] loss=2.86 avg=3.08\n",
            "[96 | 94.12] loss=3.06 avg=3.08\n",
            "[97 | 95.05] loss=3.04 avg=3.08\n",
            "[98 | 95.97] loss=2.92 avg=3.08\n",
            "[99 | 96.90] loss=3.22 avg=3.08\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " the is-a-fraud\n",
            "\n",
            "There is a huge amount of misinformation about this event. They are lying so often that it has created an alluring illusion. They claim that millions of dollars came from the \"Lotto\" and some big-money investors including the Chicago Fire and Miami Dolphins will be pouring millions directly into the Clinton Foundation.\n",
            "\n",
            "If you think it looks like a big deal it isn't. All you have to do is watch the show and watch the show.\n",
            "\n",
            "This website was originally created in September 2013. It is being revamped into a more comprehensive and accurate website.\n",
            "\n",
            "http://www.nydailynews.com/blogs/the-new-american-will-be-the-first-president-to-be-in-a-new-land\n",
            "\n",
            "If you think it looks like a big deal it is totally wrong. The Trump Presidency (which will take about ten years!) will be run through the very next President and will be nothing but a farce.\n",
            "\n",
            "You need to be an authentic Trump voter. Make your purchase today to support Donald Trump.<|endoftext|>Diana Cressey , PhD\n",
            "\n",
            "It does not take a lot of knowledge to get lost in the sea. You never know what your destination is going to be when you come back to it.\n",
            "\n",
            "When you land - your new ' destination . . .\n",
            "\n",
            "This is the place to be to go and where your dreams will be in life .\n",
            "\n",
            "A fabulous website . . .\n",
            "\n",
            "A must-visit website that will bring many people together and give them a glimpse the world over . . .\n",
            "\n",
            "Best Travel Travel blog in town . . .\n",
            "\n",
            "A wonderful source for people of all different religions - Christian, Muslim and Hindu.\n",
            "\n",
            "I love it in this country ! The only reason I am doing this is because I love my country . . .\n",
            "\n",
            "Great site . A must for all lovers of the American Southwest . Great job\n",
            "\n",
            "I love this blog , and am so glad you are here . The real deal , the real deal . -- Jim<|endoftext|>The Chicago Board of Trade has called for federal investment in Chicago's economy and the right to build and manage it , The Chicago Tribune reported yesterday . An official from the board of trade released a statement today telling the Chicago Tribune that the new contract will save over $700 million by 2024 . The new deal will reduce the national deficit by 1 .8 percent .\n",
            "\n",
            "The Chicago Board of Trade\n",
            "\n",
            "The Board of Trade is considering a new contract with Chicago-based Doral International Airport as its next airport .\n",
            "\n",
            "The Trump Organization , with its billions of dollars in assets and its many subsidiaries and interests in Chicago , is a major owner and a beneficiary of the new deal .\n",
            "\n",
            "Doral International Airport\n",
            "\n",
            "As of today it has had a contract with the Chicago Department of Transportation (DOT) that will see the Chicago airport built and operated by the Trump Companies , but this deal had previously been terminated and had already left the city , and will run out by the end of the year in New York .\n",
            "\n",
            "The Trump Organization ( Trump ) will join the Obama-Bush family in purchasing the Chicago Airport for a fee of $15 billion , but the deal is in its early stages .\n",
            "\n",
            "Now the Chicago Bears and the NFL are playing in Chicago . One can only watch them play from the field .\n",
            "\n",
            "A new deal between the Chicago-Washington State ( WSS ) and its major football rivals [ Chicago ] is in the planning stage .\n",
            "\n",
            "On Nov 3 , U.S. District Judge Ronald Kessler approved a lawsuit that names Donald Trump , Eric Trump , Trump Entertainment Group , and Donald and Melania Trump , as defendants . The defendants claim that it was illegal, misleading and unfair . The Chicago Appeals Court upheld the motion .\n",
            "\n",
            "The Trump Organization , the biggest American company that operates the White House , is in the process of building a new office building in Chicago . [ Trump ] will be building the former National Park Service tower where President Trump once lived , which will house and build a massive new and expanded border wall , which is very much coming to fruition , and would allow an extensive new and expanded illegal immigration .\n",
            "\n",
            "President Donald Trump and members of his staff hold a press conference in the Oval Office at the White House in Washington, DC , November 2, 2017 . Trump, who is expected to speak at the event on Oct 11 , speaks highly of Judge Kessler ... [ Trump ] will be holding meetings in the coming weeks in the State of D.C . . [ Trump ] said he will consider ' the ' [ Trump ] [ Kessler ] , he has been a big proponent of building a great border building . [ Trump ] [ Kessler ] has been a great guy and a great negotiator . He is just the right man to lead for the great country that will make him proud ! Thanks :]\n",
            "\n",
            "The Trump Organization \" would like to renew the Trump International Hotel at 2330 Pennsylvania Avenue for an additional\n",
            "\n",
            "[100 | 116.39] loss=3.15 avg=3.08\n",
            "[101 | 117.32] loss=3.00 avg=3.08\n",
            "[102 | 118.26] loss=2.80 avg=3.07\n",
            "[103 | 119.19] loss=3.14 avg=3.08\n",
            "[104 | 120.11] loss=2.90 avg=3.07\n",
            "[105 | 121.05] loss=3.00 avg=3.07\n",
            "[106 | 121.98] loss=2.86 avg=3.07\n",
            "[107 | 122.92] loss=2.96 avg=3.07\n",
            "[108 | 123.86] loss=3.23 avg=3.07\n",
            "[109 | 124.80] loss=3.21 avg=3.07\n",
            "[110 | 125.73] loss=3.03 avg=3.07\n",
            "[111 | 126.67] loss=2.82 avg=3.07\n",
            "[112 | 127.60] loss=3.03 avg=3.07\n",
            "[113 | 128.54] loss=2.99 avg=3.07\n",
            "[114 | 129.48] loss=3.06 avg=3.07\n",
            "[115 | 130.41] loss=2.74 avg=3.06\n",
            "[116 | 131.34] loss=2.91 avg=3.06\n",
            "[117 | 132.27] loss=3.03 avg=3.06\n",
            "[118 | 133.20] loss=2.75 avg=3.05\n",
            "[119 | 134.14] loss=3.03 avg=3.05\n",
            "[120 | 135.07] loss=2.64 avg=3.05\n",
            "[121 | 136.01] loss=2.79 avg=3.04\n",
            "[122 | 136.95] loss=3.01 avg=3.04\n",
            "[123 | 137.88] loss=2.98 avg=3.04\n",
            "[124 | 138.82] loss=3.04 avg=3.04\n",
            "[125 | 139.76] loss=2.90 avg=3.04\n",
            "[126 | 140.70] loss=2.99 avg=3.04\n",
            "[127 | 141.63] loss=3.03 avg=3.04\n",
            "[128 | 142.57] loss=2.82 avg=3.04\n",
            "[129 | 143.50] loss=2.97 avg=3.04\n",
            "[130 | 144.43] loss=2.94 avg=3.03\n",
            "[131 | 145.37] loss=2.68 avg=3.03\n",
            "[132 | 146.30] loss=2.97 avg=3.03\n",
            "[133 | 147.23] loss=2.75 avg=3.02\n",
            "[134 | 148.16] loss=2.94 avg=3.02\n",
            "[135 | 149.10] loss=3.14 avg=3.03\n",
            "[136 | 150.03] loss=2.85 avg=3.02\n",
            "[137 | 150.95] loss=2.93 avg=3.02\n",
            "[138 | 151.89] loss=2.60 avg=3.02\n",
            "[139 | 152.83] loss=3.07 avg=3.02\n",
            "[140 | 153.76] loss=2.80 avg=3.01\n",
            "[141 | 154.70] loss=3.00 avg=3.01\n",
            "[142 | 155.64] loss=3.23 avg=3.02\n",
            "[143 | 156.58] loss=2.85 avg=3.01\n",
            "[144 | 157.52] loss=2.86 avg=3.01\n",
            "[145 | 158.45] loss=2.90 avg=3.01\n",
            "[146 | 159.38] loss=2.85 avg=3.01\n",
            "[147 | 160.32] loss=2.98 avg=3.01\n",
            "[148 | 161.26] loss=2.98 avg=3.01\n",
            "[149 | 162.19] loss=2.98 avg=3.01\n",
            "[150 | 163.12] loss=2.90 avg=3.01\n",
            "[151 | 164.06] loss=2.76 avg=3.00\n",
            "[152 | 164.99] loss=2.93 avg=3.00\n",
            "[153 | 165.92] loss=2.60 avg=3.00\n",
            "[154 | 166.86] loss=2.66 avg=2.99\n",
            "[155 | 167.79] loss=3.05 avg=2.99\n",
            "[156 | 168.73] loss=3.03 avg=2.99\n",
            "[157 | 169.68] loss=2.85 avg=2.99\n",
            "[158 | 170.61] loss=3.07 avg=2.99\n",
            "[159 | 171.54] loss=2.89 avg=2.99\n",
            "[160 | 172.48] loss=2.73 avg=2.99\n",
            "[161 | 173.42] loss=3.13 avg=2.99\n",
            "[162 | 174.36] loss=2.98 avg=2.99\n",
            "[163 | 175.28] loss=3.11 avg=2.99\n",
            "[164 | 176.22] loss=3.16 avg=2.99\n",
            "[165 | 177.15] loss=2.87 avg=2.99\n",
            "[166 | 178.08] loss=2.77 avg=2.99\n",
            "[167 | 179.01] loss=3.32 avg=2.99\n",
            "[168 | 179.95] loss=2.72 avg=2.99\n",
            "[169 | 180.88] loss=3.06 avg=2.99\n",
            "[170 | 181.81] loss=2.55 avg=2.99\n",
            "[171 | 182.75] loss=3.07 avg=2.99\n",
            "[172 | 183.68] loss=2.92 avg=2.99\n",
            "[173 | 184.62] loss=3.02 avg=2.99\n",
            "[174 | 185.55] loss=2.78 avg=2.98\n",
            "[175 | 186.48] loss=2.96 avg=2.98\n",
            "[176 | 187.42] loss=2.88 avg=2.98\n",
            "[177 | 188.35] loss=2.82 avg=2.98\n",
            "[178 | 189.29] loss=2.83 avg=2.98\n",
            "[179 | 190.21] loss=2.99 avg=2.98\n",
            "[180 | 191.15] loss=3.06 avg=2.98\n",
            "[181 | 192.08] loss=2.82 avg=2.98\n",
            "[182 | 193.02] loss=2.81 avg=2.98\n",
            "[183 | 193.95] loss=2.92 avg=2.97\n",
            "[184 | 194.89] loss=2.93 avg=2.97\n",
            "[185 | 195.82] loss=2.92 avg=2.97\n",
            "[186 | 196.75] loss=2.91 avg=2.97\n",
            "[187 | 197.68] loss=2.80 avg=2.97\n",
            "[188 | 198.61] loss=2.63 avg=2.97\n",
            "[189 | 199.55] loss=2.95 avg=2.97\n",
            "[190 | 200.49] loss=2.82 avg=2.97\n",
            "[191 | 201.43] loss=2.77 avg=2.96\n",
            "[192 | 202.36] loss=2.96 avg=2.96\n",
            "[193 | 203.30] loss=2.86 avg=2.96\n",
            "[194 | 204.23] loss=2.89 avg=2.96\n",
            "[195 | 205.17] loss=2.63 avg=2.96\n",
            "[196 | 206.11] loss=3.06 avg=2.96\n",
            "[197 | 207.05] loss=2.90 avg=2.96\n",
            "[198 | 207.98] loss=2.75 avg=2.96\n",
            "[199 | 208.92] loss=2.60 avg=2.95\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " s/e is not good for the American people [HASHTAG] [URL] by [HANDLE] with [HANDLE]\n",
            "The Washington Post article is bad enough that it is being used to discredit [HANDLE] as we are going to attack him so easily -- so easily that nobody really cares about the article and would rather focus on the fact Donald Trump doesn't like to be attacked -- and he doesn't think so (as much as he likes it when Obama attacks him). Trump is doing a disservice to the American people by going on record and suggesting that they have to pay for some phony 'experts' who will say anything that doesn't suit their new plan when they get there -- or maybe try to sell it to them . I don't think Mr. Trump really knows how to do this , but it definitely is not a good sign . [HASHTAG] [URL] [HANDLE] [HANDLE] [HANDLE] \n",
            "The ' media ' has a very short reach and very unprofessional reach . Fake news is very bad news for the country , and we all get hurt . \n",
            " [HANDLE] : Trump : [HANDLE] Trump says a lot of good things about [HANDLE] .\" He even did the ' s first live Celebrity Apprentice . He must be very well prepared . \n",
            " [HANDLE] : [HANDLE] It is now time to send the message to you , [HANDLE] , that ' s true : [HANDLE] \n",
            "If you read The Atlantic , you probably already know that [HANDLE] is a great and respected writer . He writes about so much . [HASHTAG] [HANDLE] [HANDLE] [HANDLE] \n",
            " [HANDLE] : [HANDLE] Your book [HANDLE] should be in our top 25 books of all time . [HANDLE] \n",
            " [HANDLE] : [HANDLE] Great interview . Best interview . Best quote , ' \" [HASHTAG] [URL] by [HANDLE] [HANDLE] [HANDLE] \n",
            " [HANDLE] : He ' s the best show all year . No better for a show , especially on the Sunday after Thanksgiving . [HANDLE] \n",
            " [HANDLE] : [HANDLE] You ' re going to be great . I wish you luck with your golf season .\" Thanks ! \n",
            " [HANDLE] : [HANDLE] You ' re really the best . Be good . \n",
            " [HANDLE] : [HANDLE] Great piece , ' \" , and you owe [HANDLE] a big big \"Thank you ! \n",
            " [HANDLE] : [HANDLE] : \" [HANDLE] your going to be a great president \" : [HASHTAG] \n",
            "I think [HANDLE] really should have called Trump ( [HANDLE] is so far behind on Twitter , he doesn ' t even get to retweet me ! [HASHTAG] \n",
            "A lot depends on the person who does the thing that will make my life a success [HASHTAG] \n",
            " [HANDLE] : [HANDLE] [HANDLE] you should know !\" I love you ! \n",
            "I have been thinking deeply about what I should do with my life -- and what I think to do with it . Think back to how I ' ve spent my money . Now think big ! \n",
            " [HANDLE] : You are a great man [HANDLE]\n",
            " . [HANDLE] is very important to [HANDLE] . I do not ' t feel comfortable with that name . Not one i respect or care much for ! \n",
            "The [HANDLE] interview with [HANDLE] is on \" [HANDLE] . \n",
            " [HANDLE] : [HANDLE] [HANDLE] what do you think about Mr . Trump ' s remarks about his [HANDLE] ?\" \n",
            " [HANDLE] and [HANDLE] are going to be interviewed tonight , 10-9 on [HANDLE] ! You can watch and read [HANDLE] at [HANDLE] at 11:30pm EDT . \n",
            " [HANDLE] : Don ' t let the [HANDLE] fool you with the [HANDLE] [HANDLE] \n",
            " [HANDLE] is the real deal ( as reported by the [HANDLE] ) and if he ' s on the same side of issues then you ' re a fool . You ' re totally under-rated ! \n",
            "Thank you [HANDLE] for your nice response . Not only is he a\n",
            "\n",
            "[200 | 227.67] loss=2.98 avg=2.95\n",
            "[201 | 228.60] loss=2.96 avg=2.95\n",
            "[202 | 229.53] loss=2.96 avg=2.95\n",
            "[203 | 230.47] loss=3.02 avg=2.95\n",
            "[204 | 231.40] loss=2.84 avg=2.95\n",
            "[205 | 232.33] loss=2.82 avg=2.95\n",
            "[206 | 233.25] loss=3.12 avg=2.95\n",
            "[207 | 234.20] loss=2.84 avg=2.95\n",
            "[208 | 235.13] loss=3.09 avg=2.95\n",
            "[209 | 236.06] loss=2.86 avg=2.95\n",
            "[210 | 236.99] loss=3.13 avg=2.95\n",
            "[211 | 237.94] loss=3.12 avg=2.95\n",
            "[212 | 238.87] loss=2.80 avg=2.95\n",
            "[213 | 239.80] loss=2.96 avg=2.95\n",
            "[214 | 240.74] loss=3.03 avg=2.95\n",
            "[215 | 241.67] loss=2.70 avg=2.95\n",
            "[216 | 242.61] loss=3.27 avg=2.95\n",
            "[217 | 243.55] loss=2.62 avg=2.95\n",
            "[218 | 244.48] loss=3.18 avg=2.95\n",
            "[219 | 245.41] loss=2.62 avg=2.95\n",
            "[220 | 246.34] loss=2.74 avg=2.95\n",
            "[221 | 247.27] loss=2.88 avg=2.95\n",
            "[222 | 248.21] loss=2.85 avg=2.95\n",
            "[223 | 249.14] loss=2.79 avg=2.94\n",
            "[224 | 250.07] loss=2.61 avg=2.94\n",
            "[225 | 251.00] loss=2.61 avg=2.94\n",
            "[226 | 251.93] loss=3.07 avg=2.94\n",
            "[227 | 252.86] loss=2.99 avg=2.94\n",
            "[228 | 253.80] loss=2.70 avg=2.94\n",
            "[229 | 254.72] loss=2.85 avg=2.93\n",
            "[230 | 255.65] loss=2.74 avg=2.93\n",
            "[231 | 256.58] loss=3.06 avg=2.93\n",
            "[232 | 257.52] loss=2.83 avg=2.93\n",
            "[233 | 258.44] loss=2.88 avg=2.93\n",
            "[234 | 259.37] loss=2.82 avg=2.93\n",
            "[235 | 260.31] loss=2.58 avg=2.93\n",
            "[236 | 261.24] loss=2.75 avg=2.93\n",
            "[237 | 262.18] loss=2.74 avg=2.92\n",
            "[238 | 263.11] loss=2.77 avg=2.92\n",
            "[239 | 264.05] loss=3.13 avg=2.92\n",
            "[240 | 264.98] loss=2.74 avg=2.92\n",
            "[241 | 265.90] loss=2.65 avg=2.92\n",
            "[242 | 266.83] loss=2.79 avg=2.92\n",
            "[243 | 267.76] loss=2.93 avg=2.92\n",
            "[244 | 268.70] loss=2.74 avg=2.92\n",
            "[245 | 269.64] loss=2.83 avg=2.91\n",
            "[246 | 270.58] loss=2.98 avg=2.92\n",
            "[247 | 271.51] loss=2.88 avg=2.92\n",
            "[248 | 272.44] loss=3.06 avg=2.92\n",
            "[249 | 273.39] loss=2.80 avg=2.92\n",
            "[250 | 274.32] loss=3.03 avg=2.92\n",
            "[251 | 275.26] loss=2.92 avg=2.92\n",
            "[252 | 276.19] loss=2.76 avg=2.91\n",
            "[253 | 277.11] loss=2.91 avg=2.91\n",
            "[254 | 278.04] loss=2.73 avg=2.91\n",
            "[255 | 278.96] loss=2.93 avg=2.91\n",
            "[256 | 279.89] loss=2.99 avg=2.91\n",
            "[257 | 280.83] loss=2.69 avg=2.91\n",
            "[258 | 281.75] loss=3.09 avg=2.91\n",
            "[259 | 282.68] loss=2.96 avg=2.91\n",
            "[260 | 283.61] loss=2.83 avg=2.91\n",
            "[261 | 284.54] loss=2.96 avg=2.91\n",
            "[262 | 285.47] loss=2.94 avg=2.91\n",
            "[263 | 286.40] loss=2.86 avg=2.91\n",
            "[264 | 287.34] loss=3.08 avg=2.91\n",
            "[265 | 288.27] loss=3.15 avg=2.92\n",
            "[266 | 289.20] loss=2.96 avg=2.92\n",
            "[267 | 290.13] loss=2.77 avg=2.92\n",
            "[268 | 291.06] loss=2.98 avg=2.92\n",
            "[269 | 291.99] loss=2.73 avg=2.91\n",
            "[270 | 292.92] loss=3.06 avg=2.92\n",
            "[271 | 293.85] loss=2.91 avg=2.92\n",
            "[272 | 294.78] loss=2.64 avg=2.91\n",
            "[273 | 295.71] loss=2.43 avg=2.91\n",
            "[274 | 296.63] loss=2.85 avg=2.91\n",
            "[275 | 297.56] loss=2.95 avg=2.91\n",
            "[276 | 298.49] loss=2.94 avg=2.91\n",
            "[277 | 299.41] loss=2.89 avg=2.91\n",
            "[278 | 300.35] loss=2.65 avg=2.91\n",
            "[279 | 301.29] loss=3.07 avg=2.91\n",
            "[280 | 302.23] loss=2.85 avg=2.91\n",
            "[281 | 303.16] loss=2.77 avg=2.91\n",
            "[282 | 304.09] loss=2.52 avg=2.90\n",
            "[283 | 305.01] loss=2.80 avg=2.90\n",
            "[284 | 305.95] loss=2.98 avg=2.90\n",
            "[285 | 306.88] loss=2.61 avg=2.90\n",
            "[286 | 307.81] loss=2.77 avg=2.90\n",
            "[287 | 308.75] loss=2.96 avg=2.90\n",
            "[288 | 309.68] loss=3.11 avg=2.90\n",
            "[289 | 310.61] loss=2.64 avg=2.90\n",
            "[290 | 311.54] loss=2.67 avg=2.89\n",
            "[291 | 312.48] loss=3.08 avg=2.90\n",
            "[292 | 313.42] loss=2.77 avg=2.89\n",
            "[293 | 314.35] loss=2.66 avg=2.89\n",
            "[294 | 315.28] loss=2.56 avg=2.89\n",
            "[295 | 316.21] loss=2.85 avg=2.89\n",
            "[296 | 317.15] loss=2.98 avg=2.89\n",
            "[297 | 318.08] loss=2.76 avg=2.89\n",
            "[298 | 319.01] loss=3.22 avg=2.89\n",
            "[299 | 319.95] loss=2.49 avg=2.89\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "!!! ' s a very bad way to do business [URL] \n",
            "Happy Saturday everyone ! Enjoy , a GREAT Saturday in Iowa which will be a great opportunity for the people of Iowa ! \n",
            " [HANDLE] : [HANDLE] [HANDLE] you are the best man alive\n",
            " [HANDLE] [HANDLE] [HANDLE] YOU A GREAT HERO ! He who knows the difference has always done it , and always will ! \n",
            "The only man who will actually be a real politician is a dead duck . He does NOT deserve to run for Congress . \n",
            "Via [HANDLE] [HANDLE] : [HANDLE] [HANDLE] [HANDLE] [HANDLE] [HANDLE] [HANDLE] [HANDLE] [HANDLE] [HANDLE] [HANDLE] [HANDLE] [HANDLE] \n",
            "Just arrived at Camp Pendleton airport . A great honor to see the great people of Ohio ! [URL] \n",
            "I will run for President in 2016 ! If I win , America  \n",
            "There are now 5 MILLION ( plus ) of Fake News stories about Donald Trump and his campaign . They are total crap . They are phony . \n",
            " [HANDLE] : [HANDLE] [HANDLE] the only one who would care about this country if it never got bombed on Iran and Russia\n",
            " [HANDLE] : It's not a terrible thing to be [HANDLE] but the president gets treated like a celebrity . Now it seems like everyone will be doing something about it - who ever thought that President Kim Jong Il would be doing his job . \n",
            " [HANDLE] : ' ' [HANDLE] I have to thank [HANDLE] for the support and for standing up for what they can do !! [HASHTAG] \n",
            " [HANDLE] : [HANDLE] [HANDLE] [HASHTAG] [HASHTAG] [HASHTAG] \n",
            "The president of the United States is on his way to Chicago tonight . Very interesting with [HANDLE] and his family .\" Thank you in advance ! \n",
            "Why is the FBI so slow to get answers from the House Republicans ? Why isnt there any accountability to the entire country on this massive mess before we get the answers ? [HASHTAG] \n",
            "Wow , the crowd is over a million in Ohio . The numbers are just starting to fall off . People will MAKE UP ANYTHING ! [URL] \n",
            "With 4.5 Billion on the Border , there is no choice but to make Mexico pay for the wall . Great jobs for ALL of us ! \n",
            " [HANDLE] : [HANDLE] [HASHTAG] [HANDLE] [HANDLE] [HANDLE] [HANDLE] [HANDLE] [HANDLE] [HANDLE] [HANDLE] [HANDLE] [HANDLE] \n",
            " [HANDLE] : I will be standing with my son-in-law at the National Press Club . I was a great [HANDLE] , and I do like Mr . Trump . \n",
            " [HANDLE] : [HANDLE] Trump is the most powerful man we have ever seen . He has all the guts to speak his mind . It ain't his problem ! \n",
            "We are going to be fighting ISIS , but we will never take them : ). It will all come crashing down . They will take over your country , take it from you , and your children . [URL] \n",
            " [HANDLE] : [HANDLE] Trump is the man ! He is the man that will become president when he leaves (he would become a President !), and he really is the man ! \n",
            " [HANDLE] : [HANDLE] [HANDLE] He has been a wonderful father . [HASHTAG] \n",
            " . [HANDLE] : \" [HANDLE] : [HANDLE] [HANDLE] I totally agree , Trump is a total liar , and a big fraud .\" It sounds like . \n",
            " [HANDLE] : [HANDLE] [HANDLE] Trump . \n",
            " [HANDLE] : [HANDLE] [HANDLE] Trump . \n",
            " . [HANDLE] [HANDLE] [HANDLE] Trump ! \n",
            " [HANDLE] : A few days ago , [HANDLE] said he would be willing to run for president after Obama ' s loss for reelection . Who would he run for ? [URL] \n",
            " [HANDLE] : [HANDLE] [HANDLE] So many people complaining that Obama is weak . Great\n",
            "\n",
            "[300 | 338.21] loss=2.79 avg=2.89\n",
            "[301 | 339.14] loss=3.03 avg=2.89\n",
            "[302 | 340.08] loss=2.86 avg=2.89\n",
            "[303 | 341.01] loss=2.98 avg=2.89\n",
            "[304 | 341.95] loss=3.03 avg=2.89\n",
            "[305 | 342.89] loss=2.95 avg=2.89\n",
            "[306 | 343.82] loss=3.03 avg=2.89\n",
            "[307 | 344.76] loss=2.87 avg=2.89\n",
            "[308 | 345.69] loss=2.34 avg=2.89\n",
            "[309 | 346.63] loss=2.90 avg=2.89\n",
            "[310 | 347.57] loss=2.91 avg=2.89\n",
            "[311 | 348.50] loss=2.72 avg=2.88\n",
            "[312 | 349.44] loss=2.78 avg=2.88\n",
            "[313 | 350.37] loss=3.16 avg=2.89\n",
            "[314 | 351.30] loss=2.86 avg=2.89\n",
            "[315 | 352.24] loss=3.02 avg=2.89\n",
            "[316 | 353.17] loss=3.06 avg=2.89\n",
            "[317 | 354.11] loss=3.03 avg=2.89\n",
            "[318 | 355.05] loss=2.83 avg=2.89\n",
            "[319 | 355.98] loss=2.93 avg=2.89\n",
            "[320 | 356.92] loss=2.95 avg=2.89\n",
            "[321 | 357.86] loss=3.02 avg=2.89\n",
            "[322 | 358.79] loss=2.95 avg=2.89\n",
            "[323 | 359.73] loss=2.67 avg=2.89\n",
            "[324 | 360.66] loss=2.90 avg=2.89\n",
            "[325 | 361.59] loss=2.94 avg=2.89\n",
            "[326 | 362.52] loss=2.83 avg=2.89\n",
            "[327 | 363.46] loss=2.66 avg=2.89\n",
            "[328 | 364.39] loss=2.89 avg=2.89\n",
            "[329 | 365.33] loss=2.84 avg=2.89\n",
            "[330 | 366.26] loss=2.90 avg=2.89\n",
            "[331 | 367.20] loss=3.01 avg=2.89\n",
            "[332 | 368.14] loss=2.76 avg=2.89\n",
            "[333 | 369.07] loss=2.73 avg=2.89\n",
            "[334 | 370.01] loss=2.81 avg=2.89\n",
            "[335 | 370.95] loss=2.80 avg=2.88\n",
            "[336 | 371.89] loss=2.72 avg=2.88\n",
            "[337 | 372.83] loss=3.02 avg=2.88\n",
            "[338 | 373.77] loss=2.75 avg=2.88\n",
            "[339 | 374.71] loss=2.68 avg=2.88\n",
            "[340 | 375.64] loss=3.15 avg=2.88\n",
            "[341 | 376.58] loss=2.92 avg=2.88\n",
            "[342 | 377.51] loss=2.77 avg=2.88\n",
            "[343 | 378.44] loss=2.50 avg=2.88\n",
            "[344 | 379.37] loss=2.84 avg=2.88\n",
            "[345 | 380.30] loss=2.89 avg=2.88\n",
            "[346 | 381.24] loss=2.71 avg=2.88\n",
            "[347 | 382.18] loss=2.63 avg=2.87\n",
            "[348 | 383.11] loss=3.00 avg=2.88\n",
            "[349 | 384.03] loss=2.79 avg=2.87\n",
            "[350 | 384.97] loss=2.99 avg=2.88\n",
            "[351 | 385.90] loss=2.84 avg=2.88\n",
            "[352 | 386.83] loss=2.94 avg=2.88\n",
            "[353 | 387.77] loss=2.97 avg=2.88\n",
            "[354 | 388.71] loss=2.77 avg=2.88\n",
            "[355 | 389.65] loss=2.83 avg=2.88\n",
            "[356 | 390.58] loss=2.90 avg=2.88\n",
            "[357 | 391.52] loss=2.61 avg=2.87\n",
            "[358 | 392.46] loss=2.79 avg=2.87\n",
            "[359 | 393.40] loss=3.04 avg=2.87\n",
            "[360 | 394.34] loss=2.67 avg=2.87\n",
            "[361 | 395.27] loss=2.78 avg=2.87\n",
            "[362 | 396.21] loss=2.73 avg=2.87\n",
            "[363 | 397.15] loss=2.60 avg=2.87\n",
            "[364 | 398.08] loss=3.00 avg=2.87\n",
            "[365 | 399.02] loss=2.80 avg=2.87\n",
            "[366 | 399.95] loss=2.73 avg=2.87\n",
            "[367 | 400.89] loss=2.93 avg=2.87\n",
            "[368 | 401.81] loss=2.81 avg=2.87\n",
            "[369 | 402.75] loss=2.60 avg=2.86\n",
            "[370 | 403.68] loss=2.72 avg=2.86\n",
            "[371 | 404.62] loss=2.52 avg=2.86\n",
            "[372 | 405.56] loss=2.45 avg=2.85\n",
            "[373 | 406.49] loss=2.83 avg=2.85\n",
            "[374 | 407.43] loss=2.75 avg=2.85\n",
            "[375 | 408.37] loss=2.62 avg=2.85\n",
            "[376 | 409.30] loss=2.50 avg=2.85\n",
            "[377 | 410.24] loss=2.68 avg=2.84\n",
            "[378 | 411.18] loss=2.83 avg=2.84\n",
            "[379 | 412.12] loss=2.86 avg=2.85\n",
            "[380 | 413.05] loss=2.52 avg=2.84\n",
            "[381 | 413.99] loss=2.56 avg=2.84\n",
            "[382 | 414.93] loss=2.89 avg=2.84\n",
            "[383 | 415.86] loss=2.68 avg=2.84\n",
            "[384 | 416.80] loss=2.57 avg=2.83\n",
            "[385 | 417.73] loss=2.95 avg=2.84\n",
            "[386 | 418.66] loss=2.66 avg=2.83\n",
            "[387 | 419.59] loss=2.79 avg=2.83\n",
            "[388 | 420.53] loss=2.77 avg=2.83\n",
            "[389 | 421.47] loss=2.72 avg=2.83\n",
            "[390 | 422.40] loss=2.70 avg=2.83\n",
            "[391 | 423.34] loss=2.40 avg=2.83\n",
            "[392 | 424.28] loss=2.53 avg=2.82\n",
            "[393 | 425.21] loss=2.84 avg=2.82\n",
            "[394 | 426.14] loss=2.54 avg=2.82\n",
            "[395 | 427.07] loss=2.91 avg=2.82\n",
            "[396 | 428.00] loss=2.95 avg=2.82\n",
            "[397 | 428.93] loss=3.11 avg=2.83\n",
            "[398 | 429.86] loss=3.17 avg=2.83\n",
            "[399 | 430.79] loss=2.78 avg=2.83\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " Th . V . was very much alive . \n",
            " [HANDLE] : [HANDLE] will be the next [HASHTAG] winner\n",
            "So . Obama is giving the Muslim terrorists so much money to give out on his website because he doesn ' t like it . [URL] He then gives out big , stupid red checker cards to charity . \n",
            "Our immigration system is a failure that will bankrupt the United States . It will cost the economy trillions . \n",
            "Just ran for President . Donald J . Trump is so different from the other candidates ! \n",
            "The very dangerous Donald Trump is doing great so far . We have a great chance to make it past a majority vote by the end of 2017 . \n",
            " [HANDLE] : [HANDLE] Trump may very well end [HASHTAG] [URL] \n",
            " [HANDLE] : Trump : [HANDLE] I ' ve never said sorry for myself but [HANDLE] can ' t wait \". [URL] \n",
            "We have done nothing . [HANDLE] won ' t win ! He is not a good leader or good strategist , he is a bad leader . \n",
            " [HANDLE] : [HANDLE] [HANDLE] so many good things can be done , just need a change !!! \n",
            " [HANDLE] We had [HANDLE] on the show last night . I was so excited ! \n",
            " [HANDLE] : [HANDLE] [HANDLE] Thank you ! \n",
            " . [HANDLE] has been doing very well lately in business so far . The new owners have not yet been given orders ! \n",
            "I am doing well in the polls . I am doing quite well in the polls . My opponent for next year ' s president , my campaign manager , has only a 4 . \n",
            " [HANDLE] : Who has more time for a fight than my great friend [HANDLE] ? He ' s a good guy .\" -- [HANDLE] \n",
            "Via [HANDLE] : [HANDLE] You know what it ' s like when you ' ve done everything to get my nomination . Can ' t believe I don ' t get it ! \n",
            " [HANDLE] : [HANDLE] [HANDLE] Donald Trump [HANDLE] \n",
            "Thank you for supporting our wonderful people . There will be a big change soon for the USA ! [URL] \n",
            "Thank you to all our loyal Americans ! [URL] \n",
            "Via [HANDLE] : The Truth is out on [HANDLE] \" You ' re the greatest . It is so important for me to run against Trump , his massive debt-driven agenda . That ' s right ! \n",
            "So nice to see Mr . [HANDLE] \n",
            " . [HANDLE] You are a champion of the most important values . [HASHTAG] [URL] \n",
            "Thank you Iowa ! [URL] \n",
            " [HANDLE] : [HANDLE] [HANDLE] great event ! Thanks a lot [HASHTAG] .\" Thanks , I will do a lot of it [URL] \n",
            " [HANDLE] : [HANDLE]  Haters out , this is a big deal ! \n",
            " [HASHTAG] by [HANDLE] [HANDLE] - no surprise , no reason . \n",
            " [HANDLE] : I ' m going to be a big president , but I don ' t know ' what to see\n",
            "The biggest failure of the Trump Administration is our economy . We have nothing [URL] \n",
            "Wow , the [HANDLE] interview last night . Not a bad one , except for his wife that seems to have no clue what she ' s talking about . \n",
            " [HANDLE] : Thank you [HASHTAG] ! \n",
            " [HANDLE] : [HANDLE] your new interview for USA Today - watch it today on [HANDLE] on [HANDLE] . You can expect lots of things ! \n",
            " . [HANDLE] , thank you : The Greatest People to Run for President\n",
            "This has become very popular ! - [URL] \n",
            "The most corrupt politician in the world is Donald Trump . Who can stop him - or the U . S . \n",
            "There is something terribly deplorable about Donald Trump who has absolutely no respect for anyone ' s job ratings . It ' s the worst thing we can do . He is so weak , weak . \n",
            "My last speech in the United States would not look so good in a long time . It would look like garbage . [URL] \n",
            "The Clintons are taking advantage of me on immigration , by allowing them to get away with something so ridiculous - and so dangerous and unfair . \n",
            "Obama wants to stop our\n",
            "\n",
            "[400 | 449.01] loss=2.78 avg=2.83\n",
            "[401 | 449.94] loss=2.73 avg=2.83\n",
            "[402 | 450.87] loss=2.85 avg=2.83\n",
            "[403 | 451.81] loss=2.73 avg=2.83\n",
            "[404 | 452.74] loss=2.50 avg=2.82\n",
            "[405 | 453.67] loss=3.12 avg=2.83\n",
            "[406 | 454.61] loss=2.78 avg=2.83\n",
            "[407 | 455.55] loss=2.60 avg=2.82\n",
            "[408 | 456.48] loss=2.48 avg=2.82\n",
            "[409 | 457.43] loss=2.63 avg=2.82\n",
            "[410 | 458.36] loss=2.91 avg=2.82\n",
            "[411 | 459.30] loss=2.48 avg=2.82\n",
            "[412 | 460.22] loss=2.54 avg=2.81\n",
            "[413 | 461.16] loss=2.70 avg=2.81\n",
            "[414 | 462.09] loss=2.60 avg=2.81\n",
            "[415 | 463.03] loss=2.88 avg=2.81\n",
            "[416 | 463.97] loss=2.82 avg=2.81\n",
            "[417 | 464.90] loss=2.74 avg=2.81\n",
            "[418 | 465.84] loss=2.89 avg=2.81\n",
            "[419 | 466.77] loss=2.60 avg=2.81\n",
            "[420 | 467.70] loss=2.83 avg=2.81\n",
            "[421 | 468.64] loss=2.86 avg=2.81\n",
            "[422 | 469.58] loss=2.88 avg=2.81\n",
            "[423 | 470.51] loss=2.54 avg=2.81\n",
            "[424 | 471.43] loss=2.61 avg=2.81\n",
            "[425 | 472.36] loss=2.63 avg=2.80\n",
            "[426 | 473.30] loss=2.84 avg=2.80\n",
            "[427 | 474.24] loss=2.63 avg=2.80\n",
            "[428 | 475.17] loss=3.04 avg=2.80\n",
            "[429 | 476.10] loss=3.05 avg=2.81\n",
            "[430 | 477.04] loss=2.58 avg=2.80\n",
            "[431 | 477.97] loss=2.97 avg=2.81\n",
            "[432 | 478.90] loss=2.39 avg=2.80\n",
            "[433 | 479.83] loss=2.67 avg=2.80\n",
            "[434 | 480.77] loss=2.69 avg=2.80\n",
            "[435 | 481.70] loss=2.77 avg=2.80\n",
            "[436 | 482.64] loss=2.64 avg=2.80\n",
            "[437 | 483.57] loss=2.61 avg=2.80\n",
            "[438 | 484.50] loss=3.06 avg=2.80\n",
            "[439 | 485.43] loss=2.78 avg=2.80\n",
            "[440 | 486.37] loss=2.82 avg=2.80\n",
            "[441 | 487.30] loss=2.75 avg=2.80\n",
            "[442 | 488.24] loss=2.66 avg=2.80\n",
            "[443 | 489.18] loss=2.85 avg=2.80\n",
            "[444 | 490.11] loss=2.55 avg=2.79\n",
            "[445 | 491.04] loss=2.50 avg=2.79\n",
            "[446 | 491.97] loss=2.63 avg=2.79\n",
            "[447 | 492.91] loss=2.51 avg=2.79\n",
            "[448 | 493.84] loss=2.34 avg=2.78\n",
            "[449 | 494.77] loss=2.65 avg=2.78\n",
            "[450 | 495.71] loss=2.53 avg=2.78\n",
            "[451 | 496.65] loss=2.96 avg=2.78\n",
            "[452 | 497.59] loss=2.77 avg=2.78\n",
            "[453 | 498.52] loss=2.78 avg=2.78\n",
            "[454 | 499.46] loss=2.80 avg=2.78\n",
            "[455 | 500.39] loss=2.78 avg=2.78\n",
            "[456 | 501.33] loss=2.46 avg=2.78\n",
            "[457 | 502.27] loss=2.76 avg=2.78\n",
            "[458 | 503.21] loss=2.74 avg=2.78\n",
            "[459 | 504.13] loss=3.00 avg=2.78\n",
            "[460 | 505.07] loss=2.80 avg=2.78\n",
            "[461 | 506.00] loss=2.79 avg=2.78\n",
            "[462 | 506.93] loss=3.00 avg=2.78\n",
            "[463 | 507.87] loss=2.73 avg=2.78\n",
            "[464 | 508.80] loss=3.06 avg=2.78\n",
            "[465 | 509.74] loss=2.68 avg=2.78\n",
            "[466 | 510.67] loss=2.70 avg=2.78\n",
            "[467 | 511.60] loss=2.87 avg=2.78\n",
            "[468 | 512.54] loss=2.73 avg=2.78\n",
            "[469 | 513.48] loss=2.81 avg=2.78\n",
            "[470 | 514.42] loss=2.78 avg=2.78\n",
            "[471 | 515.34] loss=3.05 avg=2.79\n",
            "[472 | 516.28] loss=2.84 avg=2.79\n",
            "[473 | 517.22] loss=2.69 avg=2.78\n",
            "[474 | 518.15] loss=2.85 avg=2.79\n",
            "[475 | 519.09] loss=2.84 avg=2.79\n",
            "[476 | 520.02] loss=2.85 avg=2.79\n",
            "[477 | 520.96] loss=3.01 avg=2.79\n",
            "[478 | 521.88] loss=3.04 avg=2.79\n",
            "[479 | 522.82] loss=2.79 avg=2.79\n",
            "[480 | 523.76] loss=2.92 avg=2.79\n",
            "[481 | 524.70] loss=2.99 avg=2.79\n",
            "[482 | 525.63] loss=2.64 avg=2.79\n",
            "[483 | 526.57] loss=2.79 avg=2.79\n",
            "[484 | 527.50] loss=2.42 avg=2.79\n",
            "[485 | 528.44] loss=2.73 avg=2.79\n",
            "[486 | 529.38] loss=3.00 avg=2.79\n",
            "[487 | 530.31] loss=2.75 avg=2.79\n",
            "[488 | 531.25] loss=2.74 avg=2.79\n",
            "[489 | 532.19] loss=2.75 avg=2.79\n",
            "[490 | 533.12] loss=2.62 avg=2.79\n",
            "[491 | 534.05] loss=2.88 avg=2.79\n",
            "[492 | 534.98] loss=2.87 avg=2.79\n",
            "[493 | 535.92] loss=2.72 avg=2.79\n",
            "[494 | 536.85] loss=2.86 avg=2.79\n",
            "[495 | 537.78] loss=2.69 avg=2.79\n",
            "[496 | 538.72] loss=2.81 avg=2.79\n",
            "[497 | 539.65] loss=2.88 avg=2.79\n",
            "[498 | 540.57] loss=2.94 avg=2.79\n",
            "[499 | 541.51] loss=2.74 avg=2.79\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " . ( t ) ( a ) d ( ( ( c ) ( , ( b ) ) ) \n",
            "The only problem is that he doesn- \n",
            " [HANDLE] : I would love to see you in the White House -- you know the place , I live there -- but I dont live here . \n",
            "Just landed and they were all there discussing the Iraq War . The only way I would be able to live for two days on the Gulf Coast was if all of us would show up and vote for the men and women who are fighting for our Great Country . They are GREAT people . \n",
            "I love doing business as usual - all , I do it so that others dont . \n",
            "Congratulations to Donald Trump and others at NY GOP State of the Nation Address . Our Country stands on the border against a crime bill that will make illegal immigration a Crime Problem ! \n",
            "In a move that will be expected . All over the country , some are saying we are now on our way to the Moon , which is one of these strange places , and this is happening so fast . Our Country is very small . China isn ' t helping us -- just trying to steal our jobs . No wonder the Fake News is laughing about this ! \n",
            "I will be interviewed at the Kennedy Center tonight and tomorrow ! Also tomorrow at 1 : 30pm , at 11pm Eastern ! You will hear a lot ! \n",
            "Congratulations to the great American families of the [URL] \n",
            "The people of Arizona are asking me to stay at [URL] until I stop killing people , the American people ask me to start caring more about the country . The Democrats don ' t believe in me . \n",
            "Remember our great [URL] in the State of Arizona ! [URL] \n",
            "We need to get back the border again with Mexico , a beautiful and easy way for us to send our boys through . The Mexico Wall , as bad as it really is , is also much better than it was yesterday . \n",
            " [HANDLE] : [HANDLE] [HANDLE] [HANDLE] You just beat Hillary , your only chance to be president is by voting NO\n",
            "The world is changing , even people who don ' t know what they are talking about . They are changing their mind , they have no clue they are discussing something that isnt in the cards !\" - [URL] \n",
            "I spent much of yesterday at the [HASHTAG] in the heart of Marlboro , Mississippi , with the new President . The man is great ! \n",
            "Thank you to all for all of the nice words of the last week . I had a great time and enjoyed working with you , all of you - we are all just waiting for the final cut . \n",
            " [HANDLE] : ' Donald Trump Jr ' s Lawsuit ' \" It ' s very important to remember that Mr . Flynn , despite being under investigation by the FBI , was never asked to testify against him by the FBI . He lied , got away with it , got rid of it , then got fired . \n",
            "Thank you to Nancy Pelosi , all of the Dems , who have been so wonderful to me and my family . Our Country is going to be a great place to be in the future . \n",
            "I am happy to report that on Monday , I will host two bipartisan hearings on [URL] [HASHTAG] with President Obama and [HANDLE] [URL] , [URL] ! [HASHTAG] \n",
            "The [HANDLE] interview was great - I will miss her ! Love the energy and optimism she brings to our Country ! \n",
            "A GREAT job by the great [HANDLE] Team on [HANDLE] . She has been one of my heroes for many many years . \n",
            " [HANDLE] : [HANDLE] I just bought a lot of [HANDLE] \n",
            "Just got off a cruise and is enjoying the view from [HANDLE] . \n",
            "I do not want to just leave a positive impression on any of you , or the country ! It is important to understand that everything you do or look at in the media is biased , or based on something that is negative , to make sense of it . The more dishonest you look the more you will like what you see ! \n",
            "Just started a meeting with [HANDLE] at [HANDLE] in New Hampshire . He has all the information his eyes have on the event , has the great ideas and people he will bring into the team [URL] \n",
            " [HANDLE] : [HANDLE] your new [HANDLE] is really cool\n",
            "Congratulations to Senator Mike Lee, an Arizona Democrat , and to [HANDLE] : . [URL] [URL] \n",
            "The Democrats are a laughing stock in this country because they are such a laughing stock because they cant even find a Republican to run . No Democrat . \n",
            " [HANDLE]\n",
            "\n",
            "[500 | 559.68] loss=2.64 avg=2.79\n",
            "[501 | 560.62] loss=2.58 avg=2.79\n",
            "[502 | 561.56] loss=2.40 avg=2.78\n",
            "[503 | 562.49] loss=3.01 avg=2.79\n",
            "[504 | 563.43] loss=2.68 avg=2.78\n",
            "[505 | 564.37] loss=2.67 avg=2.78\n",
            "[506 | 565.30] loss=2.75 avg=2.78\n",
            "[507 | 566.24] loss=2.58 avg=2.78\n",
            "[508 | 567.17] loss=2.82 avg=2.78\n",
            "[509 | 568.11] loss=2.67 avg=2.78\n",
            "[510 | 569.04] loss=2.65 avg=2.78\n",
            "[511 | 569.98] loss=2.80 avg=2.78\n",
            "[512 | 570.91] loss=2.55 avg=2.78\n",
            "[513 | 571.85] loss=2.58 avg=2.78\n",
            "[514 | 572.78] loss=2.58 avg=2.77\n",
            "[515 | 573.71] loss=2.89 avg=2.77\n",
            "[516 | 574.65] loss=2.87 avg=2.78\n",
            "[517 | 575.57] loss=2.82 avg=2.78\n",
            "[518 | 576.50] loss=2.72 avg=2.78\n",
            "[519 | 577.43] loss=2.70 avg=2.77\n",
            "[520 | 578.37] loss=2.76 avg=2.77\n",
            "[521 | 579.31] loss=2.62 avg=2.77\n",
            "[522 | 580.24] loss=2.90 avg=2.77\n",
            "[523 | 581.17] loss=2.70 avg=2.77\n",
            "[524 | 582.11] loss=2.56 avg=2.77\n",
            "[525 | 583.05] loss=2.85 avg=2.77\n",
            "[526 | 583.99] loss=2.93 avg=2.77\n",
            "[527 | 584.92] loss=2.42 avg=2.77\n",
            "[528 | 585.86] loss=2.80 avg=2.77\n",
            "[529 | 586.80] loss=2.53 avg=2.77\n",
            "[530 | 587.73] loss=2.46 avg=2.76\n",
            "[531 | 588.66] loss=2.82 avg=2.77\n",
            "[532 | 589.59] loss=2.35 avg=2.76\n",
            "[533 | 590.52] loss=2.83 avg=2.76\n",
            "[534 | 591.46] loss=2.76 avg=2.76\n",
            "[535 | 592.40] loss=3.01 avg=2.76\n",
            "[536 | 593.34] loss=2.44 avg=2.76\n",
            "[537 | 594.27] loss=3.10 avg=2.76\n",
            "[538 | 595.21] loss=2.57 avg=2.76\n",
            "[539 | 596.15] loss=2.41 avg=2.76\n",
            "[540 | 597.08] loss=2.61 avg=2.76\n",
            "[541 | 598.02] loss=2.90 avg=2.76\n",
            "[542 | 598.95] loss=3.00 avg=2.76\n",
            "[543 | 599.89] loss=2.90 avg=2.76\n",
            "[544 | 600.83] loss=2.94 avg=2.76\n",
            "[545 | 601.76] loss=2.75 avg=2.76\n",
            "[546 | 602.70] loss=2.43 avg=2.76\n",
            "[547 | 603.64] loss=2.66 avg=2.76\n",
            "[548 | 604.56] loss=2.78 avg=2.76\n",
            "[549 | 605.50] loss=2.72 avg=2.76\n",
            "[550 | 606.43] loss=2.97 avg=2.76\n",
            "[551 | 607.37] loss=3.05 avg=2.76\n",
            "[552 | 608.30] loss=2.72 avg=2.76\n",
            "[553 | 609.24] loss=2.92 avg=2.77\n",
            "[554 | 610.18] loss=2.98 avg=2.77\n",
            "[555 | 611.11] loss=2.93 avg=2.77\n",
            "[556 | 612.05] loss=2.82 avg=2.77\n",
            "[557 | 612.99] loss=3.00 avg=2.77\n",
            "[558 | 613.91] loss=2.88 avg=2.77\n",
            "[559 | 614.84] loss=2.55 avg=2.77\n",
            "[560 | 615.78] loss=2.61 avg=2.77\n",
            "[561 | 616.72] loss=2.66 avg=2.77\n",
            "[562 | 617.65] loss=2.66 avg=2.77\n",
            "[563 | 618.58] loss=2.68 avg=2.77\n",
            "[564 | 619.52] loss=2.75 avg=2.77\n",
            "[565 | 620.46] loss=2.77 avg=2.77\n",
            "[566 | 621.40] loss=2.81 avg=2.77\n",
            "[567 | 622.34] loss=2.95 avg=2.77\n",
            "[568 | 623.27] loss=2.73 avg=2.77\n",
            "[569 | 624.20] loss=2.96 avg=2.77\n",
            "[570 | 625.13] loss=2.91 avg=2.77\n",
            "[571 | 626.06] loss=2.55 avg=2.77\n",
            "[572 | 627.00] loss=2.91 avg=2.77\n",
            "[573 | 627.93] loss=2.59 avg=2.77\n",
            "[574 | 628.87] loss=2.85 avg=2.77\n",
            "[575 | 629.79] loss=2.55 avg=2.77\n",
            "[576 | 630.72] loss=2.61 avg=2.77\n",
            "[577 | 631.65] loss=2.71 avg=2.77\n",
            "[578 | 632.59] loss=2.66 avg=2.76\n",
            "[579 | 633.52] loss=2.37 avg=2.76\n",
            "[580 | 634.45] loss=2.74 avg=2.76\n",
            "[581 | 635.39] loss=2.82 avg=2.76\n",
            "[582 | 636.32] loss=2.35 avg=2.76\n",
            "[583 | 637.26] loss=2.96 avg=2.76\n",
            "[584 | 638.19] loss=2.75 avg=2.76\n",
            "[585 | 639.13] loss=2.84 avg=2.76\n",
            "[586 | 640.06] loss=3.05 avg=2.76\n",
            "[587 | 641.00] loss=2.88 avg=2.76\n",
            "[588 | 641.93] loss=2.69 avg=2.76\n",
            "[589 | 642.87] loss=2.96 avg=2.76\n",
            "[590 | 643.80] loss=2.59 avg=2.76\n",
            "[591 | 644.74] loss=2.56 avg=2.76\n",
            "[592 | 645.66] loss=2.69 avg=2.76\n",
            "[593 | 646.59] loss=2.99 avg=2.76\n",
            "[594 | 647.53] loss=2.71 avg=2.76\n",
            "[595 | 648.46] loss=2.50 avg=2.76\n",
            "[596 | 649.40] loss=2.61 avg=2.76\n",
            "[597 | 650.34] loss=2.94 avg=2.76\n",
            "[598 | 651.28] loss=2.43 avg=2.76\n",
            "[599 | 652.22] loss=2.67 avg=2.76\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " whatever in the world I don ' t wish on anybody else but myself .\" - Steve Jobs\n",
            "Remember , I was never a \" fan \" of Trump !\" He is a clown\n",
            "     It ' s a sad statement that Trump has lost touch with reality , and it ' s one I blame\n",
            "Thank you to all . Thank you to our wonderful friends- including everyone from Texas to California ! [HASHTAG] [URL] \n",
            " [HANDLE] : \" You can be a genius as long as you keep making smart choices .\" - [HANDLE] \n",
            " [HANDLE] : [HANDLE] [HANDLE] [HANDLE] you are a genius ( but we do not need someone like Donald Trump , why do you have the luxury of a lifetime ?] [HASHTAG] \" True . \n",
            " [HANDLE] [HASHTAG] [HASHTAG] [HASHTAG] [HANDLE] \n",
            "I would be a good president to be even smarter and bring that to the table . -- Ronald Reagan\n",
            " [HASHTAG] [HASHTAG] [HASHTAG] [URL] \n",
            " [HANDLE] : [HANDLE] [HASHTAG] The Apprentice must be the most anticipated [HASHTAG] show in 2016 !\" \n",
            " . [HANDLE] ' s [HASHTAG] episode , a big success ! \n",
            " [HANDLE] : [HANDLE] [HANDLE] [HASHTAG] [HASHTAG] \" [URL] \n",
            " [HANDLE] : [HANDLE] [HANDLE] I hope your father can run for President\n",
            " [HANDLE] : [HANDLE] You have made it so clear . You are such a smart guy ! You have the instincts ! [HANDLE] \" Thank you . \n",
            "This is going to be a great day for our country . We are going to stop the Ebola virus - get involved ! [URL] VOTEL REGISTER now [URL] \n",
            "Big news conference at 9 p . m . at the [HANDLE] with [HANDLE] - a great group , including [HANDLE] , [HANDLE] , [HANDLE] and [HANDLE] . Thank you . [HASHTAG] [URL] \n",
            " [HANDLE] : [HANDLE] your campaign just landed in New York \" [URL] I love it ! \n",
            "I ' ll be appearing on [HANDLE] tomorrow night at 7 : 45 A . M . to be with my friend [HANDLE] ! Tickets - $10 each ! \n",
            " [HANDLE] : [HANDLE] [HANDLE] [HASHTAG] [HASHTAG] [URL] \n",
            "Via [HANDLE] by Peter Brimelow : \" Donald Trump Is No More Crazy Than Trumpism \" by The Hollywood Reporter :\n",
            "He ' s a little bit of a loser -- a bit of a loser who has never really played on a golf course . His success should be a happy one ! \n",
            " [HANDLE] : . [HANDLE] is great , great , great , great ! [HASHTAG] \" Very nice . \n",
            " [HANDLE] : [HANDLE] I just had a wonderful time with [HANDLE] tonight . He is an outstanding young man . We need a leader , and [HANDLE] has it !! [HASHTAG] [URL] \n",
            "Trump : There Is Still Another Witch Hunt in our Country \" [HASHTAG] [HASHTAG] \n",
            "The Fake News Media will go ahead with stories about how President Obama was forced to have a child , and then begrudgingly use that same name . If you are a Republican , you just ' m a hypocrite . \n",
            " [HANDLE] : I ' m not a big fan Donald , I ' m only voting for Donald ! \n",
            " [HANDLE] : [HANDLE] you will be a tremendous leader as President of the United States ! [HASHTAG] \" Thank you . \n",
            " [HANDLE] : [HANDLE] [HANDLE] I would do anything to be President of the United States . \n",
            " [HANDLE] : [HANDLE] [HANDLE] [HANDLE] what ' s so funny about [HASHTAG] ' s ratings ? \n",
            "No one is perfect . What ' s a perfect person is to be humble . \n",
            "Doing so means you know the right thing to say . In a world where politicians have been silent on most important issues , do it today , tomorrow\n",
            "\n",
            "[600 | 670.50] loss=3.04 avg=2.76\n",
            "[601 | 671.43] loss=3.08 avg=2.76\n",
            "[602 | 672.37] loss=2.66 avg=2.76\n",
            "[603 | 673.30] loss=2.53 avg=2.76\n",
            "[604 | 674.23] loss=2.64 avg=2.76\n",
            "[605 | 675.16] loss=2.62 avg=2.76\n",
            "[606 | 676.09] loss=2.71 avg=2.76\n",
            "[607 | 677.03] loss=2.78 avg=2.76\n",
            "[608 | 677.96] loss=2.67 avg=2.75\n",
            "[609 | 678.90] loss=2.92 avg=2.76\n",
            "[610 | 679.82] loss=2.94 avg=2.76\n",
            "[611 | 680.76] loss=2.66 avg=2.76\n",
            "[612 | 681.69] loss=2.31 avg=2.75\n",
            "[613 | 682.62] loss=2.64 avg=2.75\n",
            "[614 | 683.57] loss=2.79 avg=2.75\n",
            "[615 | 684.50] loss=2.62 avg=2.75\n",
            "[616 | 685.44] loss=2.64 avg=2.75\n",
            "[617 | 686.38] loss=2.75 avg=2.75\n",
            "[618 | 687.31] loss=2.67 avg=2.75\n",
            "[619 | 688.24] loss=2.85 avg=2.75\n",
            "[620 | 689.17] loss=2.95 avg=2.75\n",
            "[621 | 690.10] loss=2.85 avg=2.75\n",
            "[622 | 691.04] loss=2.59 avg=2.75\n",
            "[623 | 691.97] loss=2.90 avg=2.75\n",
            "[624 | 692.90] loss=2.92 avg=2.75\n",
            "[625 | 693.84] loss=3.10 avg=2.76\n",
            "[626 | 694.77] loss=2.61 avg=2.76\n",
            "[627 | 695.70] loss=2.65 avg=2.76\n",
            "[628 | 696.64] loss=2.77 avg=2.76\n",
            "[629 | 697.57] loss=2.57 avg=2.75\n",
            "[630 | 698.50] loss=2.54 avg=2.75\n",
            "[631 | 699.43] loss=2.90 avg=2.75\n",
            "[632 | 700.37] loss=2.59 avg=2.75\n",
            "[633 | 701.31] loss=2.65 avg=2.75\n",
            "[634 | 702.25] loss=2.62 avg=2.75\n",
            "[635 | 703.17] loss=2.89 avg=2.75\n",
            "[636 | 704.10] loss=2.56 avg=2.75\n",
            "[637 | 705.03] loss=2.72 avg=2.75\n",
            "[638 | 705.97] loss=2.77 avg=2.75\n",
            "[639 | 706.90] loss=2.56 avg=2.75\n",
            "[640 | 707.84] loss=2.51 avg=2.74\n",
            "[641 | 708.78] loss=2.80 avg=2.74\n",
            "[642 | 709.72] loss=2.72 avg=2.74\n",
            "[643 | 710.65] loss=2.72 avg=2.74\n",
            "[644 | 711.59] loss=2.73 avg=2.74\n",
            "[645 | 712.52] loss=2.49 avg=2.74\n",
            "[646 | 713.46] loss=2.65 avg=2.74\n",
            "[647 | 714.39] loss=2.57 avg=2.74\n",
            "[648 | 715.32] loss=2.79 avg=2.74\n",
            "[649 | 716.27] loss=2.66 avg=2.74\n",
            "[650 | 717.19] loss=2.92 avg=2.74\n",
            "[651 | 718.13] loss=2.52 avg=2.74\n",
            "[652 | 719.06] loss=2.92 avg=2.74\n",
            "[653 | 720.00] loss=3.04 avg=2.74\n",
            "[654 | 720.94] loss=2.59 avg=2.74\n",
            "[655 | 721.87] loss=2.93 avg=2.74\n",
            "[656 | 722.81] loss=2.77 avg=2.74\n",
            "[657 | 723.75] loss=2.85 avg=2.74\n",
            "[658 | 724.69] loss=2.70 avg=2.74\n",
            "[659 | 725.62] loss=2.87 avg=2.75\n",
            "[660 | 726.55] loss=2.92 avg=2.75\n",
            "[661 | 727.48] loss=2.75 avg=2.75\n",
            "[662 | 728.42] loss=2.94 avg=2.75\n",
            "[663 | 729.35] loss=2.79 avg=2.75\n",
            "[664 | 730.28] loss=2.63 avg=2.75\n",
            "[665 | 731.21] loss=3.25 avg=2.75\n",
            "[666 | 732.15] loss=2.71 avg=2.75\n",
            "[667 | 733.09] loss=2.72 avg=2.75\n",
            "[668 | 734.02] loss=2.93 avg=2.75\n",
            "[669 | 734.95] loss=2.79 avg=2.76\n",
            "[670 | 735.88] loss=2.70 avg=2.75\n",
            "[671 | 736.82] loss=2.60 avg=2.75\n",
            "[672 | 737.76] loss=2.64 avg=2.75\n",
            "[673 | 738.70] loss=2.80 avg=2.75\n",
            "[674 | 739.63] loss=2.77 avg=2.75\n",
            "[675 | 740.57] loss=2.88 avg=2.75\n",
            "[676 | 741.51] loss=2.74 avg=2.75\n",
            "[677 | 742.44] loss=2.36 avg=2.75\n",
            "[678 | 743.37] loss=2.46 avg=2.75\n",
            "[679 | 744.31] loss=2.56 avg=2.74\n",
            "[680 | 745.24] loss=2.63 avg=2.74\n",
            "[681 | 746.18] loss=2.90 avg=2.75\n",
            "[682 | 747.11] loss=2.60 avg=2.74\n",
            "[683 | 748.04] loss=2.87 avg=2.75\n",
            "[684 | 748.98] loss=2.75 avg=2.75\n",
            "[685 | 749.92] loss=2.78 avg=2.75\n",
            "[686 | 750.86] loss=2.94 avg=2.75\n",
            "[687 | 751.79] loss=2.74 avg=2.75\n",
            "[688 | 752.73] loss=2.78 avg=2.75\n",
            "[689 | 753.67] loss=2.86 avg=2.75\n",
            "[690 | 754.61] loss=2.98 avg=2.75\n",
            "[691 | 755.53] loss=2.75 avg=2.75\n",
            "[692 | 756.48] loss=2.58 avg=2.75\n",
            "[693 | 757.40] loss=2.52 avg=2.75\n",
            "[694 | 758.33] loss=2.82 avg=2.75\n",
            "[695 | 759.27] loss=2.92 avg=2.75\n",
            "[696 | 760.20] loss=2.76 avg=2.75\n",
            "[697 | 761.14] loss=3.00 avg=2.75\n",
            "[698 | 762.07] loss=2.54 avg=2.75\n",
            "[699 | 763.00] loss=2.81 avg=2.75\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "HLEE !!!! [URL] \n",
            "Hillary just said something that will make me very angry !!!! [URL] \n",
            "I am happy to see that many members of Congress have stated it is safe for Republicans to filibuster ObamaCare . They could easily pass a bill to do so , which includes funding the VA , just in time for Christmas . ( The Fake News ) \n",
            " [HANDLE] : \" The next [HANDLE] will be a [HANDLE] [HANDLE] \" Great ! \n",
            " [HANDLE] : [HASHTAG] The [HASHTAG] Celebrity Apprentice [HANDLE] [HANDLE] [URL] \n",
            " [HANDLE] : [HANDLE] why don ' t you play golf with the ladies ? I love golf !\" Go to the course . \n",
            "I told you so , we are done . But you want to know the truth . Trump to you , no promises ! \n",
            " . [HANDLE] looks like a lot of fools if they did [HANDLE] instead of focusing on the ridiculous and sad thing happening in Washington . \n",
            " . [HANDLE] is the ultimate compliment . \n",
            "In the next 48 hours they will release the final and last ( cont ) [URL] [URL] \n",
            "Congratulations to John McCain on a fantastic run , a strong presidency , and a beautiful victory . [URL] [URL] \n",
            "Congratulations to [HANDLE] on having the best weekend of his career . [HASHTAG] \n",
            " [HANDLE] : We are looking forward to seeing you in the next big game . Love to see your work and your leadership . Big numbers !! [HASHTAG] \n",
            " [HANDLE] : [HANDLE] just voted for [HANDLE] in Trump 2016 U . S . [URL] \n",
            " [HANDLE] : [HANDLE] Donald Trump for President [HANDLE] Thanks . \n",
            " [HANDLE] : [HANDLE] [HANDLE] [HANDLE] [HANDLE] this is amazing .\" -- but he will be back and happy about it . \n",
            "A major scandal in this country . The media refuses to get the full picture . It is a disaster . The Dems should focus on the big picture ! \n",
            " [HANDLE] : [HANDLE] this is not our time - we want the next president\n",
            "The Donald is going to be president , thanks [HANDLE] . I would love for his team to get him elected president . \n",
            " [HANDLE] : Why hasn ' t [HANDLE] given us any sign that he is not a loser ? [HASHTAG]\n",
            " [HANDLE] : [HANDLE] I will not be voting for Trump - I am an American patriot .\" If you are , you ARE the candidate . \n",
            " [HASHTAG] [HASHTAG] [URL] \n",
            " [HANDLE] : [HANDLE] What ' s with the hatches [HANDLE] is getting a much more beautiful hat ! \n",
            " [HANDLE] : [HANDLE] \" We all have to have a good laugh ! [HASHTAG] \" [HASHTAG] \n",
            " [HANDLE] : [HANDLE] [HANDLE] [HANDLE] He should run ! Run ! \n",
            "So many of you are very proud of your team . Thank you !! The people love you . \n",
            " [HANDLE] : Trump is going to be the best president this country has ever known . He will bring the world some great knowledge\n",
            "I would be very surprised if the Federal government would not be spending up to 6 billion dollars that it will spend on the [HANDLE] , and to a lesser extent , on my candidacy . The very high cost of Medicare , which is being paid by taxpayers , is being used to finance [HANDLE] ! \n",
            "With [HANDLE] , [HANDLE] ... [URL] \n",
            " [HANDLE] : [HANDLE] [HANDLE] is not [HANDLE] but [HANDLE] is . Thanks . \n",
            " [HANDLE] : [HANDLE] I would rather play [HASHTAG] than [HANDLE] but I ' ll support a [HASHTAG]\n",
            " [HANDLE] : [HANDLE] You ' re awesome !!!! [HASHTAG] \" Thanks . \n",
            "Don ' t trust the media in any way , shape or form . No matter how much they want to help , that ' s all it can ever be .\" -- George Washington\n",
            " [HANDLE] : [HANDLE] Thank you [HANDLE] \n",
            "\n",
            "[700 | 781.39] loss=2.83 avg=2.75\n",
            "[701 | 782.32] loss=2.85 avg=2.75\n",
            "[702 | 783.26] loss=2.49 avg=2.75\n",
            "[703 | 784.19] loss=2.75 avg=2.75\n",
            "[704 | 785.12] loss=2.84 avg=2.75\n",
            "[705 | 786.05] loss=2.60 avg=2.75\n",
            "[706 | 786.99] loss=2.93 avg=2.75\n",
            "[707 | 787.93] loss=2.81 avg=2.75\n",
            "[708 | 788.87] loss=2.33 avg=2.75\n",
            "[709 | 789.81] loss=2.78 avg=2.75\n",
            "[710 | 790.74] loss=2.76 avg=2.75\n",
            "[711 | 791.68] loss=2.89 avg=2.75\n",
            "[712 | 792.61] loss=2.69 avg=2.75\n",
            "[713 | 793.55] loss=2.86 avg=2.75\n",
            "[714 | 794.49] loss=2.69 avg=2.75\n",
            "[715 | 795.43] loss=2.60 avg=2.75\n",
            "[716 | 796.36] loss=2.75 avg=2.75\n",
            "[717 | 797.30] loss=2.81 avg=2.75\n",
            "[718 | 798.23] loss=2.71 avg=2.75\n",
            "[719 | 799.17] loss=2.75 avg=2.75\n",
            "[720 | 800.09] loss=2.85 avg=2.75\n",
            "[721 | 801.02] loss=2.78 avg=2.75\n",
            "[722 | 801.96] loss=2.50 avg=2.75\n",
            "[723 | 802.90] loss=2.48 avg=2.74\n",
            "[724 | 803.83] loss=2.90 avg=2.75\n",
            "[725 | 804.77] loss=2.76 avg=2.75\n",
            "[726 | 805.70] loss=2.50 avg=2.74\n",
            "[727 | 806.64] loss=3.03 avg=2.75\n",
            "[728 | 807.58] loss=2.69 avg=2.75\n",
            "[729 | 808.52] loss=2.45 avg=2.74\n",
            "[730 | 809.45] loss=2.85 avg=2.74\n",
            "[731 | 810.38] loss=2.56 avg=2.74\n",
            "[732 | 811.32] loss=2.87 avg=2.74\n",
            "[733 | 812.25] loss=2.56 avg=2.74\n",
            "[734 | 813.19] loss=2.90 avg=2.74\n",
            "[735 | 814.12] loss=2.61 avg=2.74\n",
            "[736 | 815.06] loss=2.52 avg=2.74\n",
            "[737 | 815.99] loss=2.81 avg=2.74\n",
            "[738 | 816.93] loss=2.70 avg=2.74\n",
            "[739 | 817.87] loss=2.85 avg=2.74\n",
            "[740 | 818.80] loss=2.65 avg=2.74\n",
            "[741 | 819.74] loss=2.88 avg=2.74\n",
            "[742 | 820.67] loss=3.01 avg=2.74\n",
            "[743 | 821.60] loss=2.66 avg=2.74\n",
            "[744 | 822.53] loss=2.71 avg=2.74\n",
            "[745 | 823.48] loss=2.62 avg=2.74\n",
            "[746 | 824.41] loss=2.69 avg=2.74\n",
            "[747 | 825.34] loss=2.83 avg=2.74\n",
            "[748 | 826.28] loss=2.89 avg=2.74\n",
            "[749 | 827.21] loss=2.86 avg=2.74\n",
            "[750 | 828.14] loss=2.82 avg=2.75\n",
            "[751 | 829.07] loss=2.75 avg=2.75\n",
            "[752 | 830.01] loss=2.67 avg=2.74\n",
            "[753 | 830.94] loss=2.31 avg=2.74\n",
            "[754 | 831.87] loss=2.82 avg=2.74\n",
            "[755 | 832.81] loss=2.90 avg=2.74\n",
            "[756 | 833.74] loss=2.79 avg=2.74\n",
            "[757 | 834.67] loss=2.87 avg=2.74\n",
            "[758 | 835.60] loss=2.57 avg=2.74\n",
            "[759 | 836.53] loss=2.70 avg=2.74\n",
            "[760 | 837.47] loss=2.24 avg=2.74\n",
            "[761 | 838.41] loss=2.72 avg=2.74\n",
            "[762 | 839.34] loss=2.35 avg=2.73\n",
            "[763 | 840.28] loss=2.89 avg=2.73\n",
            "[764 | 841.21] loss=2.85 avg=2.74\n",
            "[765 | 842.15] loss=2.75 avg=2.74\n",
            "[766 | 843.09] loss=2.74 avg=2.74\n",
            "[767 | 844.02] loss=2.79 avg=2.74\n",
            "[768 | 844.96] loss=2.95 avg=2.74\n",
            "[769 | 845.90] loss=2.37 avg=2.74\n",
            "[770 | 846.84] loss=2.76 avg=2.74\n",
            "[771 | 847.77] loss=2.50 avg=2.73\n",
            "[772 | 848.71] loss=2.46 avg=2.73\n",
            "[773 | 849.64] loss=2.54 avg=2.73\n",
            "[774 | 850.57] loss=2.95 avg=2.73\n",
            "[775 | 851.50] loss=2.51 avg=2.73\n",
            "[776 | 852.44] loss=2.37 avg=2.72\n",
            "[777 | 853.37] loss=2.57 avg=2.72\n",
            "[778 | 854.31] loss=2.49 avg=2.72\n",
            "[779 | 855.25] loss=2.67 avg=2.72\n",
            "[780 | 856.18] loss=2.62 avg=2.72\n",
            "[781 | 857.12] loss=2.61 avg=2.72\n",
            "[782 | 858.06] loss=2.81 avg=2.72\n",
            "[783 | 858.99] loss=2.64 avg=2.72\n",
            "[784 | 859.93] loss=2.64 avg=2.72\n",
            "[785 | 860.87] loss=2.76 avg=2.72\n",
            "[786 | 861.81] loss=2.49 avg=2.72\n",
            "[787 | 862.74] loss=2.52 avg=2.71\n",
            "[788 | 863.67] loss=2.53 avg=2.71\n",
            "[789 | 864.61] loss=2.37 avg=2.71\n",
            "[790 | 865.54] loss=2.61 avg=2.71\n",
            "[791 | 866.49] loss=2.60 avg=2.71\n",
            "[792 | 867.42] loss=2.72 avg=2.71\n",
            "[793 | 868.35] loss=2.66 avg=2.71\n",
            "[794 | 869.28] loss=2.65 avg=2.71\n",
            "[795 | 870.21] loss=2.74 avg=2.71\n",
            "[796 | 871.15] loss=2.63 avg=2.71\n",
            "[797 | 872.08] loss=2.66 avg=2.70\n",
            "[798 | 873.02] loss=2.91 avg=2.71\n",
            "[799 | 873.94] loss=2.70 avg=2.71\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " [Bass] [HANDLE] [HANDLE] [HANDLE] [HANDLE] I do know that u need to give the [HANDLE] [HANDLE] in the first place . \n",
            " [HASHTAG] [URL] \n",
            " [HANDLE] : [HANDLE] [HANDLE] [HANDLE] \n",
            "Great idea ! Thank you to [HANDLE] for a great interview ! [URL] \n",
            "Looking forward to speaking to you today [HANDLE] via [HANDLE] in North Carolina . Get to know [HANDLE] then go to [HASHTAG] [HASHTAG] \n",
            "Remember , as in politics , a ' vote ' is for real ! [URL] \n",
            " [HANDLE] : [HANDLE] [HANDLE] He is the only person who thinks like you !\" Thanks . \n",
            "Great interview in my first time in Las Vegas ! I am sure that nobody has seen this video in almost 30 years . Thank you to Michael Moore , for the incredible and wonderful support from all of our great Senators . [HASHTAG] \n",
            " . [HANDLE] ran for Governor of Virginia . She has the energy and passion of a great governor -- just ask [HANDLE] ! \n",
            " [HANDLE] : [HANDLE] will be on [HANDLE] tonight ' s show ! Will be a great interview and will be on [HANDLE] at 10pm . \n",
            "I will be on CNN at 9 p . m . ET on [HANDLE] from [HANDLE] with Chris Matthews . Tune in and don ' t miss it ! \n",
            "The President ' s Speech on DACA , the Sanctuary City , and the National Guard has been a beautiful part of my life . It was so true to me and so true to my Country that I ' ve never forgotten it . [URL] \n",
            " [HANDLE] Would love to go there and enjoy the beautiful weather [HANDLE] is the best . I hope he ' s with us .\" Great conversation with Trump . \n",
            "Trump Tries To Build Big Wall By Accruing A Crime [URL] \n",
            " [HANDLE] : [HANDLE] [HANDLE] [HANDLE] [HASHTAG] \n",
            "Trump : You must be thinking how I would have won the ' s in a real way . We will do well in the polls - and you have a great manager ! \n",
            "We must create a new immigration system that will meet current needs of our country -- not an amnesty . [HASHTAG] [HASHTAG] \n",
            " [HANDLE] : [HANDLE] [HANDLE] [HANDLE] [URL] [HASHTAG] \n",
            " [HANDLE] : [HANDLE] [HANDLE] you are going to be the best at it right now !!! \n",
            "Dude , DonaldTrump just signed the contract signed by all of us ! [HANDLE] [HANDLE] [URL] \n",
            " [HANDLE] : [HANDLE] [HANDLE] [HANDLE] Just bought my house , built many new villas .. [HANDLE] \n",
            "The new president of our great country will be named President- [URL] \n",
            "Donald Trump . \" [HANDLE] ' s interview from Las Vegas \" [URL] \n",
            "Thank you [HANDLE] and [HANDLE] for your amazing service to the Country , and thank you for your continued commitment to our Great Nation . Also , [HANDLE] is a great person . \n",
            " [HANDLE] : [HANDLE] : The Trump International Hotel , Las Vegas .... [URL] [HANDLE] \n",
            "Wow ! Donald Trump ' s win-win poll numbers are now up to 13 . He gets 50 , 44 , 33 and 44 , 43 .. and just 42 , 40 . That gives him a very strong lead over Hillary . Polls are now up to 14 which means he is on his way to becoming the President ! [URL] \n",
            " [HANDLE] : [HANDLE] [HANDLE] [HANDLE] [HANDLE] Great for [HANDLE] [HASHTAG] \n",
            "It was wonderful to be with you last night at Trump National Doral . \n",
            " [HANDLE] : [HANDLE] [HANDLE] [HANDLE] [HASHTAG] will have a great run . \n",
            "Entrepreneurs : Always stay focused and always be creative . Think Big , not just think small . Know how to be innovative in all facets of your business . \n",
            " [HANDLE] :\n",
            "\n",
            "[800 | 892.16] loss=2.67 avg=2.71\n",
            "[801 | 893.08] loss=2.47 avg=2.70\n",
            "[802 | 894.02] loss=2.65 avg=2.70\n",
            "[803 | 894.96] loss=2.93 avg=2.71\n",
            "[804 | 895.89] loss=2.62 avg=2.70\n",
            "[805 | 896.83] loss=2.44 avg=2.70\n",
            "[806 | 897.76] loss=2.77 avg=2.70\n",
            "[807 | 898.69] loss=2.59 avg=2.70\n",
            "[808 | 899.63] loss=2.92 avg=2.70\n",
            "[809 | 900.56] loss=2.63 avg=2.70\n",
            "[810 | 901.49] loss=2.80 avg=2.70\n",
            "[811 | 902.43] loss=3.15 avg=2.71\n",
            "[812 | 903.37] loss=2.53 avg=2.71\n",
            "[813 | 904.30] loss=2.91 avg=2.71\n",
            "[814 | 905.24] loss=2.57 avg=2.71\n",
            "[815 | 906.17] loss=2.68 avg=2.71\n",
            "[816 | 907.12] loss=2.70 avg=2.71\n",
            "[817 | 908.06] loss=2.48 avg=2.70\n",
            "[818 | 908.99] loss=2.83 avg=2.71\n",
            "[819 | 909.93] loss=2.72 avg=2.71\n",
            "[820 | 910.87] loss=2.99 avg=2.71\n",
            "[821 | 911.81] loss=2.61 avg=2.71\n",
            "[822 | 912.74] loss=2.70 avg=2.71\n",
            "[823 | 913.68] loss=2.92 avg=2.71\n",
            "[824 | 914.62] loss=2.67 avg=2.71\n",
            "[825 | 915.55] loss=2.83 avg=2.71\n",
            "[826 | 916.48] loss=2.93 avg=2.71\n",
            "[827 | 917.41] loss=2.64 avg=2.71\n",
            "[828 | 918.34] loss=2.44 avg=2.71\n",
            "[829 | 919.28] loss=2.79 avg=2.71\n",
            "[830 | 920.20] loss=2.84 avg=2.71\n",
            "[831 | 921.14] loss=2.89 avg=2.71\n",
            "[832 | 922.07] loss=2.56 avg=2.71\n",
            "[833 | 923.00] loss=2.80 avg=2.71\n",
            "[834 | 923.94] loss=2.72 avg=2.71\n",
            "[835 | 924.87] loss=2.46 avg=2.71\n",
            "[836 | 925.81] loss=2.98 avg=2.71\n",
            "[837 | 926.74] loss=2.67 avg=2.71\n",
            "[838 | 927.68] loss=2.75 avg=2.71\n",
            "[839 | 928.62] loss=2.86 avg=2.71\n",
            "[840 | 929.57] loss=2.84 avg=2.72\n",
            "[841 | 930.50] loss=2.81 avg=2.72\n",
            "[842 | 931.44] loss=2.73 avg=2.72\n",
            "[843 | 932.37] loss=2.84 avg=2.72\n",
            "[844 | 933.31] loss=2.98 avg=2.72\n",
            "[845 | 934.24] loss=2.86 avg=2.72\n",
            "[846 | 935.17] loss=2.57 avg=2.72\n",
            "[847 | 936.11] loss=2.78 avg=2.72\n",
            "[848 | 937.04] loss=2.60 avg=2.72\n",
            "[849 | 937.97] loss=2.45 avg=2.72\n",
            "[850 | 938.90] loss=2.43 avg=2.71\n",
            "[851 | 939.84] loss=2.73 avg=2.71\n",
            "[852 | 940.77] loss=2.59 avg=2.71\n",
            "[853 | 941.71] loss=2.48 avg=2.71\n",
            "[854 | 942.65] loss=2.54 avg=2.71\n",
            "[855 | 943.59] loss=2.54 avg=2.71\n",
            "[856 | 944.52] loss=2.22 avg=2.70\n",
            "[857 | 945.45] loss=2.57 avg=2.70\n",
            "[858 | 946.39] loss=2.74 avg=2.70\n",
            "[859 | 947.32] loss=2.74 avg=2.70\n",
            "[860 | 948.26] loss=2.87 avg=2.70\n",
            "[861 | 949.18] loss=2.93 avg=2.71\n",
            "[862 | 950.12] loss=2.47 avg=2.70\n",
            "[863 | 951.06] loss=2.52 avg=2.70\n",
            "[864 | 952.00] loss=2.72 avg=2.70\n",
            "[865 | 952.93] loss=2.60 avg=2.70\n",
            "[866 | 953.87] loss=2.54 avg=2.70\n",
            "[867 | 954.80] loss=2.77 avg=2.70\n",
            "[868 | 955.75] loss=2.56 avg=2.70\n",
            "[869 | 956.68] loss=3.14 avg=2.70\n",
            "[870 | 957.62] loss=2.42 avg=2.70\n",
            "[871 | 958.55] loss=2.60 avg=2.70\n",
            "[872 | 959.48] loss=2.58 avg=2.70\n",
            "[873 | 960.43] loss=2.80 avg=2.70\n",
            "[874 | 961.37] loss=2.73 avg=2.70\n",
            "[875 | 962.31] loss=2.85 avg=2.70\n",
            "[876 | 963.25] loss=2.82 avg=2.70\n",
            "[877 | 964.19] loss=2.53 avg=2.70\n",
            "[878 | 965.13] loss=2.81 avg=2.70\n",
            "[879 | 966.06] loss=2.73 avg=2.70\n",
            "[880 | 967.00] loss=2.76 avg=2.70\n",
            "[881 | 967.93] loss=2.64 avg=2.70\n",
            "[882 | 968.87] loss=2.96 avg=2.70\n",
            "[883 | 969.79] loss=2.96 avg=2.71\n",
            "[884 | 970.72] loss=2.72 avg=2.71\n",
            "[885 | 971.65] loss=2.85 avg=2.71\n",
            "[886 | 972.59] loss=2.76 avg=2.71\n",
            "[887 | 973.51] loss=2.93 avg=2.71\n",
            "[888 | 974.44] loss=2.71 avg=2.71\n",
            "[889 | 975.37] loss=2.52 avg=2.71\n",
            "[890 | 976.31] loss=2.55 avg=2.71\n",
            "[891 | 977.25] loss=2.81 avg=2.71\n",
            "[892 | 978.18] loss=2.67 avg=2.71\n",
            "[893 | 979.12] loss=2.69 avg=2.71\n",
            "[894 | 980.06] loss=2.63 avg=2.71\n",
            "[895 | 980.99] loss=2.55 avg=2.71\n",
            "[896 | 981.93] loss=2.76 avg=2.71\n",
            "[897 | 982.86] loss=2.69 avg=2.71\n",
            "[898 | 983.79] loss=2.97 avg=2.71\n",
            "[899 | 984.72] loss=2.65 avg=2.71\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " .\n",
            " [HANDLE] : [HANDLE] [HANDLE] the Democrats are doing it again ! So proud of the Democrats . The Democrats are running them down and making them look BAD . So sad \"]\n",
            " [HANDLE] : . [HANDLE] The reason I am a candidate for president is ' I can get to the core of the problems our country is experiencing without the war and debt .\" Thank you ! [HASHTAG] \n",
            " [HANDLE] : [HANDLE] The guy [HANDLE] should be in office , if he doesnt we will go crazy . We need a leader so badly ! \n",
            "The world hates us . We are losers and a joke . -- [HANDLE] \n",
            "I love the people . I love these people - but we have to MAKE AMERICA SAFE ! Thats why I put in a law that makes our country the best place for young entrepreneurs . - [URL] \n",
            " [HANDLE] : I would never be able to find an Obama bumper sticker . We could have a big rally and it would not look like a Trump rally or bumper picture\n",
            "It ' s time for Democrats to take back the House . We need them to put the GOP backs on the table if they wish to succeed . - [HANDLE] \n",
            "It appears that [HANDLE] was the only candidate who could get the economy back on track . He didn ' t get it ! We need someone with the guts to take the party back to it ' s roots ! \n",
            "Thank you Iowa ! We are preparing for a special day . \n",
            " [HANDLE] : [HANDLE] [HANDLE] [HANDLE] It was interesting how well [HANDLE] and some of the others were talking about this topic at a dinner . Not enough people are interested , and the Republicans are taking great offense ! \n",
            " [HANDLE] : [HANDLE] I would vote for him if he ran for President . \n",
            "Thank you , Arizona ! [HASHTAG] [HASHTAG] [URL] \n",
            "We are not only destroying Afghanistan , but we are destroying the whole world . The United States is a total loser ! \n",
            " [URL] \n",
            "Happy to announce that we are working through a serious and deadly Ebola virus which has already killed up to 6 , 000 . I look forward to seeing you all soon ! \n",
            " . [HANDLE] has just been awarded the Presidential Medal of Freedom for his work on the U . S . Border . Thank you ! \n",
            "We would all be well served if the Republicans on our Supreme Court did not vote to attack a major Supreme Court nominee . He is the one who saved our country ! \n",
            "The U . S . is in desperate need of leadership . Donald J . Trump will be a leader .\" [URL] \n",
            " [HANDLE] : [HANDLE] [HANDLE] I just got back from Hawaii with my friends- I want to know how much time has passed for you . \n",
            "I ' m a strong advocate for the U . S . SECRETARY OF DEFENSE , V . S . [HASHTAG] . And thanks ! \n",
            "The Democrats won the Midterms , and they should be ashamed if [HANDLE] was the President .\" [URL] \n",
            "My [HANDLE] interviews on [HANDLE] , [HANDLE] , [HANDLE] and [HANDLE] ' s [HANDLE] [URL] \n",
            "Why is [HANDLE] running so poorly as opposed to him doing badly ? \n",
            "I ' ll be interviewed on [HANDLE] tomorrow at 6pm on [HANDLE] with [HANDLE] at 9 : 00 . We will have a full hour . \n",
            "My [HANDLE] interview discussing [HANDLE] ' s recent trip to Kenya , [HASHTAG] . Enjoy ! \n",
            "Thank you for making my campaign so much fun ! [URL] \n",
            " [HANDLE] : [HANDLE [HANDLE]] you ' re one of the best people I have seen on television in my life ! Congratulations ! \n",
            " . [HANDLE] has always been my idol . As a writer and as a businessman , I think his work is a reflection of a country that is changing like a lightbulb on every morning . \n",
            " [HANDLE] : [HANDLE] the dumbest man in the land . I can see how he was born with a big . \n",
            " [HANDLE] : [HANDLE] \" [URL] [URL] \n",
            "I will be on Meet the Press tomorrow at 8PM . The first episode will be fantastic ! \n",
            "The Democrats are now working out a plan that goes exactly as stated . They can save\n",
            "\n",
            "[900 | 1002.91] loss=2.72 avg=2.71\n",
            "[901 | 1003.84] loss=2.63 avg=2.71\n",
            "[902 | 1004.77] loss=2.67 avg=2.71\n",
            "[903 | 1005.71] loss=2.55 avg=2.71\n",
            "[904 | 1006.65] loss=2.70 avg=2.71\n",
            "[905 | 1007.59] loss=2.55 avg=2.70\n",
            "[906 | 1008.52] loss=2.99 avg=2.71\n",
            "[907 | 1009.46] loss=2.68 avg=2.71\n",
            "[908 | 1010.40] loss=2.63 avg=2.71\n",
            "[909 | 1011.34] loss=2.83 avg=2.71\n",
            "[910 | 1012.27] loss=2.78 avg=2.71\n",
            "[911 | 1013.20] loss=2.68 avg=2.71\n",
            "[912 | 1014.15] loss=2.73 avg=2.71\n",
            "[913 | 1015.09] loss=2.48 avg=2.71\n",
            "[914 | 1016.03] loss=2.51 avg=2.70\n",
            "[915 | 1016.96] loss=2.58 avg=2.70\n",
            "[916 | 1017.90] loss=2.54 avg=2.70\n",
            "[917 | 1018.84] loss=2.63 avg=2.70\n",
            "[918 | 1019.77] loss=2.61 avg=2.70\n",
            "[919 | 1020.71] loss=2.82 avg=2.70\n",
            "[920 | 1021.63] loss=2.86 avg=2.70\n",
            "[921 | 1022.56] loss=2.57 avg=2.70\n",
            "[922 | 1023.50] loss=2.38 avg=2.70\n",
            "[923 | 1024.43] loss=2.78 avg=2.70\n",
            "[924 | 1025.37] loss=2.77 avg=2.70\n",
            "[925 | 1026.30] loss=2.96 avg=2.70\n",
            "[926 | 1027.24] loss=2.36 avg=2.70\n",
            "[927 | 1028.18] loss=2.72 avg=2.70\n",
            "[928 | 1029.12] loss=2.41 avg=2.70\n",
            "[929 | 1030.05] loss=2.81 avg=2.70\n",
            "[930 | 1030.99] loss=2.31 avg=2.69\n",
            "[931 | 1031.92] loss=2.37 avg=2.69\n",
            "[932 | 1032.86] loss=2.78 avg=2.69\n",
            "[933 | 1033.78] loss=2.55 avg=2.69\n",
            "[934 | 1034.72] loss=2.79 avg=2.69\n",
            "[935 | 1035.66] loss=2.48 avg=2.69\n",
            "[936 | 1036.58] loss=2.40 avg=2.68\n",
            "[937 | 1037.51] loss=2.57 avg=2.68\n",
            "[938 | 1038.44] loss=2.47 avg=2.68\n",
            "[939 | 1039.38] loss=2.70 avg=2.68\n",
            "[940 | 1040.30] loss=2.62 avg=2.68\n",
            "[941 | 1041.24] loss=2.58 avg=2.68\n",
            "[942 | 1042.18] loss=2.53 avg=2.68\n",
            "[943 | 1043.11] loss=2.61 avg=2.68\n",
            "[944 | 1044.04] loss=2.49 avg=2.68\n",
            "[945 | 1044.97] loss=2.68 avg=2.68\n",
            "[946 | 1045.92] loss=2.61 avg=2.68\n",
            "[947 | 1046.85] loss=2.69 avg=2.68\n",
            "[948 | 1047.79] loss=2.72 avg=2.68\n",
            "[949 | 1048.73] loss=2.55 avg=2.67\n",
            "[950 | 1049.67] loss=2.61 avg=2.67\n",
            "[951 | 1050.60] loss=2.71 avg=2.67\n",
            "[952 | 1051.53] loss=2.58 avg=2.67\n",
            "[953 | 1052.46] loss=2.69 avg=2.67\n",
            "[954 | 1053.40] loss=2.73 avg=2.67\n",
            "[955 | 1054.33] loss=2.68 avg=2.67\n",
            "[956 | 1055.27] loss=2.28 avg=2.67\n",
            "[957 | 1056.19] loss=2.78 avg=2.67\n",
            "[958 | 1057.12] loss=2.79 avg=2.67\n",
            "[959 | 1058.06] loss=2.70 avg=2.67\n",
            "[960 | 1058.99] loss=2.65 avg=2.67\n",
            "[961 | 1059.94] loss=2.97 avg=2.68\n",
            "[962 | 1060.87] loss=2.79 avg=2.68\n",
            "[963 | 1061.80] loss=2.90 avg=2.68\n",
            "[964 | 1062.73] loss=2.69 avg=2.68\n",
            "[965 | 1063.67] loss=2.75 avg=2.68\n",
            "[966 | 1064.61] loss=2.60 avg=2.68\n",
            "[967 | 1065.53] loss=2.78 avg=2.68\n",
            "[968 | 1066.46] loss=2.63 avg=2.68\n",
            "[969 | 1067.39] loss=2.54 avg=2.68\n",
            "[970 | 1068.33] loss=2.89 avg=2.68\n",
            "[971 | 1069.26] loss=2.79 avg=2.68\n",
            "[972 | 1070.19] loss=2.54 avg=2.68\n",
            "[973 | 1071.13] loss=2.63 avg=2.68\n",
            "[974 | 1072.06] loss=2.66 avg=2.68\n",
            "[975 | 1073.00] loss=2.75 avg=2.68\n",
            "[976 | 1073.93] loss=2.73 avg=2.68\n",
            "[977 | 1074.87] loss=2.29 avg=2.68\n",
            "[978 | 1075.80] loss=2.53 avg=2.67\n",
            "[979 | 1076.73] loss=2.69 avg=2.68\n",
            "[980 | 1077.67] loss=3.04 avg=2.68\n",
            "[981 | 1078.60] loss=3.05 avg=2.68\n",
            "[982 | 1079.53] loss=2.62 avg=2.68\n",
            "[983 | 1080.48] loss=2.89 avg=2.68\n",
            "[984 | 1081.41] loss=2.86 avg=2.69\n",
            "[985 | 1082.35] loss=2.69 avg=2.69\n",
            "[986 | 1083.28] loss=2.56 avg=2.68\n",
            "[987 | 1084.21] loss=2.43 avg=2.68\n",
            "[988 | 1085.15] loss=3.21 avg=2.69\n",
            "[989 | 1086.09] loss=2.57 avg=2.69\n",
            "[990 | 1087.02] loss=2.74 avg=2.69\n",
            "[991 | 1087.96] loss=2.55 avg=2.69\n",
            "[992 | 1088.90] loss=2.38 avg=2.68\n",
            "[993 | 1089.83] loss=2.16 avg=2.68\n",
            "[994 | 1090.76] loss=2.77 avg=2.68\n",
            "[995 | 1091.70] loss=2.81 avg=2.68\n",
            "[996 | 1092.64] loss=2.68 avg=2.68\n",
            "[997 | 1093.56] loss=2.49 avg=2.68\n",
            "[998 | 1094.50] loss=2.84 avg=2.68\n",
            "[999 | 1095.43] loss=2.69 avg=2.68\n",
            "Saving checkpoint/run1/model-1000\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "\n",
            "Derek Jeter will be the greatest pitcher in baseball if he is healthy . He is one of one of the best . -- The Art of the Deal\n",
            "\n",
            "Obama is trying to blame everyone for his \" poor health \" . They need to listen to each other and understand they can do more than the individual can ever do with their health care [URL] \n",
            "\n",
            "Democrats were right when they said Obama was weak , weak . He is weak on Crime ! [URL] \n",
            "\n",
            "Great job in Texas . Thank you to Texas ! \n",
            "\n",
            "New York Daily News : Fake News Media [URL] [URL] \n",
            "\n",
            "Obama ' s [HANDLE] ' s in his corner . [HASHTAG] [HASHTAG] [URL] \n",
            "\n",
            "Thank you , the wonderful people of Chicago ! [URL] \n",
            "\n",
            "President Trump is a great friend of the Nation ! [URL] \n",
            "\n",
            "Just landed in Japan to discuss [HANDLE] ' s [HANDLE] Summit . The people of West [HANDLE] are united in our determination [URL] \n",
            "\n",
            " [HANDLE] : [HANDLE] [HANDLE] I hope you can get me out of this country ! \n",
            "\n",
            "Thank you . The [HANDLE] Poll , just completed , gives us 52 ... The real poll is on Thursday at 10 : 00 P . M . \n",
            "\n",
            "Thank you [HANDLE] . [URL] \n",
            "\n",
            " [HANDLE] : [HANDLE] [HANDLE] [HANDLE] [HANDLE] [HANDLE] [HANDLE] What ' s the [HANDLE Logo] on his jacket??? \n",
            "\n",
            "I have a very great relationship with Prime Minister Abe and I have great respect for him . ( video ) [URL] \n",
            "\n",
            "Thank you , Pennsylvania ! We are bringing together many talented people for the first time since WWII ! [HASHTAG] [URL] [URL] \n",
            "\n",
            "I will be interviewed on [HANDLE] tonight at 10 : 00 A . M . on [HANDLE] . [HASHTAG] [HANDLE] \n",
            "\n",
            "If we are going to win , we must defeat the Republicans and win more votes than we win by the dishonest Democrats. [URL] \n",
            "\n",
            "Thank you Pennsylvania ! [HASHTAG] [HASHTAG] [URL] \n",
            "\n",
            "Congratulations to [HANDLE] on proving that he had the stamina to be president ! [URL] \n",
            "\n",
            "The Republican Party will not give us another election until they give us the money to give back . They know how to pay for the Dems . \n",
            "\n",
            "Congratulations to [HANDLE] for a GREAT honor . [HASHTAG] [URL] \n",
            "\n",
            "With all of our wonderful thoughts and prayers , we will remain steadfast ... Thank you . [URL] [URL] \n",
            "\n",
            " [HANDLE] : [HANDLE] [HANDLE] [HANDLE] I hope he can see his vision . He ' s going to get it !!\" Thank you . \n",
            "\n",
            "When Obama has more money , his economy . If he doesn ' t , we ' llnt be so lucky ! \n",
            "\n",
            "I ' m with Obama on Syria . He should be fighting the terrorists who want to get into our country . Not Obama .\" - [HANDLE] \n",
            "\n",
            "The Democrats are going to be tough on terrorists , especially those , such as myself that we don ' t believe are there , who are responsible for the atrocities committed by ISIS in Syria ... [URL] \n",
            "\n",
            "Great crowd at Trump Tower ( just announced ) . Lots of support and enthusiasm coming back for our great convention ! \n",
            "\n",
            " [HANDLE] : [HANDLE] Trump is going to give this country a win ! [HASHTAG] \n",
            "\n",
            " . [HANDLE] is one step ahead of the game [HANDLE] has great ideas and I ' m excited for his vision . He will create jobs , build the wall , help our veterans , and so much more ! \n",
            "\n",
            "I like my work and I ' m excited to be on The Apprentice . Very few people do this very well and youve got to be a smart and dedicated person to have success ! \n",
            "\n",
            " [HANDLE] : My [HANDLE] fav movie is [HASHTAG] , it ' s one of best [HANDLE] !!! [HASHTAG] \" Thanks . \n",
            "\n",
            "Trump Tower , New York . This is the highest high-rise that stands outside the U . S . It features an all-pervents look [URL] A perfect place for a presidential wedding . [URL] \n",
            "\n",
            "The [HASHTAG] Poll is out and will be\n",
            "\n",
            "[1000 | 1117.65] loss=2.38 avg=2.68\n",
            "[1001 | 1118.59] loss=2.32 avg=2.67\n",
            "[1002 | 1119.53] loss=2.76 avg=2.67\n",
            "[1003 | 1120.47] loss=2.88 avg=2.68\n",
            "[1004 | 1121.40] loss=2.85 avg=2.68\n",
            "[1005 | 1122.33] loss=2.99 avg=2.68\n",
            "[1006 | 1123.27] loss=2.67 avg=2.68\n",
            "[1007 | 1124.20] loss=2.44 avg=2.68\n",
            "[1008 | 1125.14] loss=2.46 avg=2.68\n",
            "[1009 | 1126.08] loss=2.33 avg=2.67\n",
            "[1010 | 1127.01] loss=2.55 avg=2.67\n",
            "[1011 | 1127.94] loss=2.45 avg=2.67\n",
            "[1012 | 1128.88] loss=3.04 avg=2.67\n",
            "[1013 | 1129.82] loss=2.74 avg=2.67\n",
            "[1014 | 1130.74] loss=2.84 avg=2.67\n",
            "[1015 | 1131.67] loss=2.89 avg=2.68\n",
            "[1016 | 1132.61] loss=3.02 avg=2.68\n",
            "[1017 | 1133.55] loss=2.64 avg=2.68\n",
            "[1018 | 1134.48] loss=2.75 avg=2.68\n",
            "[1019 | 1135.41] loss=2.56 avg=2.68\n",
            "[1020 | 1136.35] loss=2.67 avg=2.68\n",
            "[1021 | 1137.28] loss=2.51 avg=2.68\n",
            "[1022 | 1138.22] loss=2.73 avg=2.68\n",
            "[1023 | 1139.16] loss=2.70 avg=2.68\n",
            "[1024 | 1140.09] loss=2.44 avg=2.68\n",
            "[1025 | 1141.02] loss=2.66 avg=2.68\n",
            "[1026 | 1141.96] loss=2.57 avg=2.67\n",
            "[1027 | 1142.89] loss=2.59 avg=2.67\n",
            "[1028 | 1143.82] loss=2.71 avg=2.67\n",
            "[1029 | 1144.75] loss=2.35 avg=2.67\n",
            "[1030 | 1145.69] loss=2.63 avg=2.67\n",
            "[1031 | 1146.63] loss=2.36 avg=2.67\n",
            "[1032 | 1147.56] loss=2.76 avg=2.67\n",
            "[1033 | 1148.50] loss=2.36 avg=2.67\n",
            "[1034 | 1149.43] loss=2.39 avg=2.66\n",
            "[1035 | 1150.36] loss=2.73 avg=2.66\n",
            "[1036 | 1151.29] loss=2.84 avg=2.67\n",
            "[1037 | 1152.23] loss=2.74 avg=2.67\n",
            "[1038 | 1153.16] loss=2.54 avg=2.66\n",
            "[1039 | 1154.10] loss=2.27 avg=2.66\n",
            "[1040 | 1155.04] loss=2.74 avg=2.66\n",
            "[1041 | 1155.97] loss=2.83 avg=2.66\n",
            "[1042 | 1156.90] loss=2.68 avg=2.66\n",
            "[1043 | 1157.83] loss=2.35 avg=2.66\n",
            "[1044 | 1158.76] loss=2.67 avg=2.66\n",
            "[1045 | 1159.70] loss=2.68 avg=2.66\n",
            "[1046 | 1160.64] loss=2.48 avg=2.66\n",
            "[1047 | 1161.57] loss=2.89 avg=2.66\n",
            "[1048 | 1162.51] loss=2.64 avg=2.66\n",
            "[1049 | 1163.44] loss=2.80 avg=2.66\n",
            "[1050 | 1164.38] loss=2.51 avg=2.66\n",
            "[1051 | 1165.32] loss=3.00 avg=2.66\n",
            "[1052 | 1166.25] loss=2.63 avg=2.66\n",
            "[1053 | 1167.18] loss=2.61 avg=2.66\n",
            "[1054 | 1168.11] loss=2.55 avg=2.66\n",
            "[1055 | 1169.05] loss=2.74 avg=2.66\n",
            "[1056 | 1169.98] loss=2.61 avg=2.66\n",
            "[1057 | 1170.92] loss=2.56 avg=2.66\n",
            "[1058 | 1171.85] loss=2.62 avg=2.66\n",
            "[1059 | 1172.78] loss=2.57 avg=2.66\n",
            "[1060 | 1173.73] loss=2.61 avg=2.66\n",
            "[1061 | 1174.67] loss=2.71 avg=2.66\n",
            "[1062 | 1175.61] loss=2.82 avg=2.66\n",
            "[1063 | 1176.54] loss=2.83 avg=2.66\n",
            "[1064 | 1177.47] loss=2.74 avg=2.66\n",
            "[1065 | 1178.40] loss=2.59 avg=2.66\n",
            "[1066 | 1179.33] loss=2.61 avg=2.66\n",
            "[1067 | 1180.26] loss=2.77 avg=2.66\n",
            "[1068 | 1181.20] loss=2.82 avg=2.67\n",
            "[1069 | 1182.13] loss=2.89 avg=2.67\n",
            "[1070 | 1183.07] loss=2.63 avg=2.67\n",
            "[1071 | 1184.01] loss=2.45 avg=2.67\n",
            "[1072 | 1184.95] loss=2.57 avg=2.66\n",
            "[1073 | 1185.88] loss=2.68 avg=2.66\n",
            "[1074 | 1186.81] loss=2.39 avg=2.66\n",
            "[1075 | 1187.75] loss=2.65 avg=2.66\n",
            "[1076 | 1188.69] loss=2.44 avg=2.66\n",
            "[1077 | 1189.62] loss=2.49 avg=2.66\n",
            "[1078 | 1190.55] loss=2.53 avg=2.66\n",
            "[1079 | 1191.48] loss=2.49 avg=2.65\n",
            "[1080 | 1192.42] loss=2.71 avg=2.66\n",
            "[1081 | 1193.35] loss=2.59 avg=2.65\n",
            "[1082 | 1194.28] loss=2.79 avg=2.66\n",
            "[1083 | 1195.22] loss=2.62 avg=2.66\n",
            "[1084 | 1196.15] loss=2.34 avg=2.65\n",
            "[1085 | 1197.09] loss=2.86 avg=2.65\n",
            "[1086 | 1198.02] loss=2.46 avg=2.65\n",
            "[1087 | 1198.96] loss=2.77 avg=2.65\n",
            "[1088 | 1199.89] loss=2.50 avg=2.65\n",
            "[1089 | 1200.83] loss=2.54 avg=2.65\n",
            "[1090 | 1201.77] loss=2.44 avg=2.65\n",
            "[1091 | 1202.71] loss=2.74 avg=2.65\n",
            "[1092 | 1203.65] loss=2.66 avg=2.65\n",
            "[1093 | 1204.58] loss=2.66 avg=2.65\n",
            "[1094 | 1205.52] loss=2.47 avg=2.65\n",
            "[1095 | 1206.45] loss=2.62 avg=2.65\n",
            "[1096 | 1207.39] loss=2.54 avg=2.65\n",
            "[1097 | 1208.31] loss=2.85 avg=2.65\n",
            "[1098 | 1209.25] loss=2.85 avg=2.65\n",
            "[1099 | 1210.19] loss=2.51 avg=2.65\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " : [HANDLE] ? [HANDLE] ? [HANDLE] \" Great- Thank you ! \n",
            " [HANDLE] : [HANDLE] [HASHTAG] [URL] [HASHTAG] [HANDLE] \" The real deal ! \n",
            "The truth is that I am a smart person and as much as I enjoy [HANDLE] ( he gets our votes ), I am not a smart person in politics . \n",
            " [HANDLE] : [HANDLE] [HANDLE] [HANDLE] \n",
            " [HANDLE] : [HASHTAG] [HASHTAG] [HASHTAG] [HASHTAG] [URL] \n",
            " [HANDLE] : [HANDLE] You are so smart . You know it . Keep reading , please ! \n",
            "I don ' t think Obama has met this great man before ! That is why I said the election is not yet decided . \n",
            "The failing New York Times ran a phony story on illegal immigration during the 2016 presidential campaign . They then reported that my plan was to bring back 1M migrants to our country . Wrong\n",
            "Happy Birthday to two great patriots ! \n",
            "We must stop the Obama Administration ' s unconstitutional and dangerous agenda . \n",
            "If [HANDLE] wants me to run we need to put our economy back on Track . [HANDLE] knows this . He is fighting for our Country . If elected , he will work very hard to Make America Great Again ! \n",
            " [HANDLE] I just love [HANDLE] . It has my full endorsement and I will do everything in my power to help her win . My vote will be decisive ! \n",
            " [HANDLE] : [HANDLE] is going nowhere by calling me a dog and then [HANDLE] makes us even richer . He thinks my life is like the Apprentice . Wrong ! \n",
            "Entrepreneur : \" Successful losers often take all the credit .\" - Steve Jobs [URL] \n",
            "Entrepreneurs : It is your responsibility to be successful you ' ve got to make a name for yourself .\" [HANDLE] \n",
            "Trump is a great American who is a hardworking guy who will make America great again ! \n",
            "I love how the Democrats failed to get a majority because they have gone so far back they can never get a majority in the Senate : [HANDLE] . \n",
            "Wow , [HANDLE] just announced that I can ' t run . The media is telling it the opposite : she is just waiting on me . She knows ! \n",
            " [HANDLE] : A great evening [HASHTAG] with [HANDLE] at Trump Tower [URL] \n",
            "I want you to know that I look forward to meeting you on the campaign trail tomorrow ! You ' ll be the next president of the United States . Keep up the great work ! \n",
            "Wow , the Democrats have zero chance of winning the next election for President . The Dems ' business is all about the economy . Vote for me , we deserve it . I am with you ! \n",
            " [HANDLE] : [HANDLE] is right . There is nothing more boring than Trump . [HASHTAG] \n",
            " [HANDLE] : [HANDLE] you are the only one who would stop the Obama agenda ! \n",
            "I want to thank our great Military for their amazing service to our country in Afghanistan . \n",
            "Thank you to the wonderful National Guard for your outstanding actions in the fight against ISIS . [HASHTAG] [URL] \n",
            "I will be discussing this subject with the Vice Presidents of the various countries that are fighting ISIS [HANDLE] , and with our Prime Minister [HANDLE] : [URL] \n",
            " [HANDLE] : [HANDLE] the best place in the world to spend Christmas is on [HASHTAG] ! [HASHTAG] \n",
            " [HANDLE] : [HANDLE] [HANDLE] You are my inspiration . Please do so in a strong and fair way ! \n",
            " [HANDLE] [HANDLE] [HANDLE] [HANDLE] [URL] \n",
            "I want to thank all of our great Patriots and the people of New Hampshire from all across America . We love you both dearly . [URL] \n",
            " . [HANDLE] has lost the debate and will have an even better debate after the first one ! [HASHTAG] [HASHTAG] [URL] \n",
            " [HANDLE] : [HANDLE] [HANDLE] [HANDLE] [HANDLE] [URL] \n",
            " [HANDLE] : [HANDLE] I ' m not giving you a chance . He has no real idea how to\n",
            "\n",
            "[1100 | 1228.40] loss=2.77 avg=2.65\n",
            "[1101 | 1229.33] loss=2.41 avg=2.65\n",
            "[1102 | 1230.27] loss=2.73 avg=2.65\n",
            "[1103 | 1231.20] loss=2.76 avg=2.65\n",
            "[1104 | 1232.13] loss=2.44 avg=2.65\n",
            "[1105 | 1233.06] loss=2.82 avg=2.65\n",
            "[1106 | 1234.00] loss=2.65 avg=2.65\n",
            "[1107 | 1234.93] loss=2.48 avg=2.65\n",
            "[1108 | 1235.87] loss=2.90 avg=2.65\n",
            "[1109 | 1236.81] loss=2.81 avg=2.65\n",
            "[1110 | 1237.75] loss=2.76 avg=2.65\n",
            "[1111 | 1238.68] loss=2.86 avg=2.66\n",
            "[1112 | 1239.62] loss=2.22 avg=2.65\n",
            "[1113 | 1240.55] loss=2.24 avg=2.65\n",
            "[1114 | 1241.49] loss=2.84 avg=2.65\n",
            "[1115 | 1242.42] loss=2.78 avg=2.65\n",
            "[1116 | 1243.36] loss=2.87 avg=2.65\n",
            "[1117 | 1244.29] loss=2.63 avg=2.65\n",
            "[1118 | 1245.22] loss=2.39 avg=2.65\n",
            "[1119 | 1246.15] loss=2.69 avg=2.65\n",
            "[1120 | 1247.08] loss=2.47 avg=2.65\n",
            "[1121 | 1248.02] loss=2.68 avg=2.65\n",
            "[1122 | 1248.96] loss=2.76 avg=2.65\n",
            "[1123 | 1249.89] loss=2.45 avg=2.65\n",
            "[1124 | 1250.83] loss=2.69 avg=2.65\n",
            "[1125 | 1251.77] loss=2.28 avg=2.64\n",
            "[1126 | 1252.70] loss=2.64 avg=2.64\n",
            "[1127 | 1253.64] loss=2.68 avg=2.64\n",
            "[1128 | 1254.58] loss=2.86 avg=2.65\n",
            "[1129 | 1255.52] loss=2.57 avg=2.65\n",
            "[1130 | 1256.45] loss=2.70 avg=2.65\n",
            "[1131 | 1257.38] loss=2.61 avg=2.65\n",
            "[1132 | 1258.32] loss=2.33 avg=2.64\n",
            "[1133 | 1259.25] loss=2.73 avg=2.64\n",
            "[1134 | 1260.18] loss=2.76 avg=2.65\n",
            "[1135 | 1261.11] loss=2.65 avg=2.65\n",
            "[1136 | 1262.05] loss=2.40 avg=2.64\n",
            "[1137 | 1262.98] loss=2.50 avg=2.64\n",
            "[1138 | 1263.91] loss=2.85 avg=2.64\n",
            "[1139 | 1264.85] loss=2.39 avg=2.64\n",
            "[1140 | 1265.78] loss=2.83 avg=2.64\n",
            "[1141 | 1266.71] loss=2.75 avg=2.64\n",
            "[1142 | 1267.64] loss=2.65 avg=2.64\n",
            "[1143 | 1268.57] loss=2.80 avg=2.65\n",
            "[1144 | 1269.51] loss=2.55 avg=2.64\n",
            "[1145 | 1270.44] loss=2.77 avg=2.65\n",
            "[1146 | 1271.37] loss=2.79 avg=2.65\n",
            "[1147 | 1272.31] loss=2.52 avg=2.65\n",
            "[1148 | 1273.24] loss=2.72 avg=2.65\n",
            "[1149 | 1274.17] loss=2.74 avg=2.65\n",
            "[1150 | 1275.11] loss=2.48 avg=2.65\n",
            "[1151 | 1276.03] loss=2.60 avg=2.65\n",
            "[1152 | 1276.96] loss=2.65 avg=2.65\n",
            "[1153 | 1277.89] loss=2.50 avg=2.64\n",
            "[1154 | 1278.82] loss=2.37 avg=2.64\n",
            "[1155 | 1279.74] loss=2.42 avg=2.64\n",
            "[1156 | 1280.67] loss=2.92 avg=2.64\n",
            "[1157 | 1281.61] loss=2.60 avg=2.64\n",
            "[1158 | 1282.54] loss=2.49 avg=2.64\n",
            "[1159 | 1283.46] loss=2.76 avg=2.64\n",
            "[1160 | 1284.39] loss=2.88 avg=2.64\n",
            "[1161 | 1285.33] loss=3.05 avg=2.65\n",
            "[1162 | 1286.25] loss=2.71 avg=2.65\n",
            "[1163 | 1287.18] loss=2.48 avg=2.65\n",
            "[1164 | 1288.11] loss=2.71 avg=2.65\n",
            "[1165 | 1289.05] loss=2.48 avg=2.65\n",
            "[1166 | 1289.98] loss=2.13 avg=2.64\n",
            "[1167 | 1290.92] loss=2.43 avg=2.64\n",
            "[1168 | 1291.85] loss=2.48 avg=2.64\n",
            "[1169 | 1292.78] loss=2.60 avg=2.64\n",
            "[1170 | 1293.71] loss=2.69 avg=2.64\n",
            "[1171 | 1294.64] loss=2.62 avg=2.64\n",
            "[1172 | 1295.57] loss=2.55 avg=2.64\n",
            "[1173 | 1296.50] loss=2.60 avg=2.64\n",
            "[1174 | 1297.42] loss=2.67 avg=2.64\n",
            "[1175 | 1298.35] loss=2.59 avg=2.64\n",
            "[1176 | 1299.28] loss=2.49 avg=2.63\n",
            "[1177 | 1300.22] loss=2.66 avg=2.63\n",
            "[1178 | 1301.15] loss=2.79 avg=2.64\n",
            "[1179 | 1302.08] loss=2.77 avg=2.64\n",
            "[1180 | 1303.02] loss=2.58 avg=2.64\n",
            "[1181 | 1303.95] loss=2.70 avg=2.64\n",
            "[1182 | 1304.88] loss=2.62 avg=2.64\n",
            "[1183 | 1305.81] loss=2.50 avg=2.64\n",
            "[1184 | 1306.75] loss=2.96 avg=2.64\n",
            "[1185 | 1307.67] loss=2.65 avg=2.64\n",
            "[1186 | 1308.61] loss=2.89 avg=2.64\n",
            "[1187 | 1309.53] loss=2.68 avg=2.64\n",
            "[1188 | 1310.47] loss=2.32 avg=2.64\n",
            "[1189 | 1311.39] loss=2.74 avg=2.64\n",
            "[1190 | 1312.32] loss=2.53 avg=2.64\n",
            "[1191 | 1313.25] loss=2.88 avg=2.64\n",
            "[1192 | 1314.19] loss=2.37 avg=2.64\n",
            "[1193 | 1315.11] loss=2.78 avg=2.64\n",
            "[1194 | 1316.04] loss=2.64 avg=2.64\n",
            "[1195 | 1316.97] loss=2.24 avg=2.64\n",
            "[1196 | 1317.91] loss=2.53 avg=2.63\n",
            "[1197 | 1318.83] loss=2.73 avg=2.64\n",
            "[1198 | 1319.76] loss=2.57 avg=2.63\n",
            "[1199 | 1320.69] loss=2.91 avg=2.64\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "HANDLE] : I ' m at the [HANDLE] show and having a great time on [HANDLE] . Great crowd . \n",
            " [HANDLE] : [HANDLE] I don ' t think you ' re going thru the same problem in a long time with your record keeping .\" [HANDLE] [HASHTAG]\n",
            " [HANDLE] : [HANDLE] You must be one of the greatest coaches in the history of our country . No matter who it is , there are far more people in [HASHTAG] than at one time ! \n",
            "Via [HANDLE] by [HANDLE] : A Guide to the World ' s Gimmick , by Charles P Jones [URL] \n",
            "I will be answering your call -- if you want me to , you will have to take it ! \n",
            "With so many stories about ObamaCare , and with the Republican Party stuck in Washington , what do we do ? \n",
            " . [HANDLE] has a record of the most ' ratings loss ' of any TV show to date .\" [HANDLE] \n",
            " [HANDLE] : If there was ever an opportunity to be successful , you were there at Fox as head of [HANDLE] . \n",
            "The Trump ' s Foundation is the first of its kind in the history of [HASHTAG] .\" -- [HANDLE] [HANDLE] \n",
            "On the Record (TODAY), [HANDLE] gives my [HANDLE] a very [HASHTAG] on Donald trump !\" Thank you . \n",
            "My [HANDLE] interview where I talked about the tremendous success of our country . Enjoy ! \n",
            " [HANDLE] : [HASHTAG] \" [HANDLE] : [HANDLE] [HANDLE] [HANDLE] He is our president\n",
            "I have [HANDLE] as your guest on tonight ' ' THE PUNK . Stay tuned ! \n",
            "I have a great relationship with the U . S . The fact that [HANDLE] can ' t stop doing it shows we ' ve got our country back on the right track ! \n",
            "I am in New Hampshire , just outside of Dayton , Ohio , to speak tonight at 7 p . m . on [HANDLE] with [HANDLE] . \n",
            " [HANDLE] Trump and Melania can ' t get away with this . [URL] \n",
            " . [HANDLE] . [HANDLE] I will tell you something . Donald Trump and Hillary Clinton are both running for President - they have nothing to fear from Hillary . There is a lot of blame on both of the two in 2016\n",
            "The [HANDLE] will be having a great time tonight by setting fire to the [HANDLE] and all of the hotels . We love you , and the [HANDLE] will be doing well ! \n",
            " [HANDLE] : Donald , you are a true hero . I will stand for it ! Donald Trump in 2020 [URL] \n",
            "Congratulations to [HANDLE] on his great win in Iowa . He ' s a fantastic man we don ' t need another incompetent [HANDLE] ! \n",
            "The Great State of New Mexico is at a historic crossroads -- [HANDLE] [HASHTAG] ! [HASHTAG] [URL] \n",
            " . [HANDLE] just went down for a 2 on 2 ! No doubt ! \n",
            " [HANDLE] : People think the media will report my thoughts if I don ' t make them up\n",
            " [HANDLE] : [HANDLE] The only person with a better knowledge of the world is Donald Trump ! I will do anything but that .\" A great person ! \n",
            "Via Politico : [HANDLE] Trump Talks Nuclear Deal With China [URL] \n",
            " [HANDLE] : [HANDLE] You ' re a great guy ! \n",
            " [HANDLE] : [HANDLE] I think it made for a great [HASHTAG] : [HASHTAG] [URL] \n",
            "So-called \" Tax Cuts \" ( , that is , the Tax Cut Bill ) are bad for small businesses and will have no impact on the economy in the long run ! \n",
            " [HANDLE] : Donald Trump : \" There Is No God ... There is No God Who Can Stand Up for Us \" [URL] [URL] \n",
            "I am a very focused person , and yet I have so much to learn . The only thing that I feel I have mastered is the knowledge of the numbers . \n",
            "We cannot afford to waste money on frivolous military adventures . They are costly and boring . \n",
            " [HANDLE] : [HANDLE] please run for office ! \n",
            " [H\n",
            "\n",
            "[1200 | 1338.75] loss=2.83 avg=2.64\n",
            "[1201 | 1339.68] loss=2.59 avg=2.64\n",
            "[1202 | 1340.61] loss=2.47 avg=2.64\n",
            "[1203 | 1341.54] loss=2.55 avg=2.64\n",
            "[1204 | 1342.47] loss=2.47 avg=2.63\n",
            "[1205 | 1343.40] loss=2.40 avg=2.63\n",
            "[1206 | 1344.33] loss=2.62 avg=2.63\n",
            "[1207 | 1345.26] loss=2.49 avg=2.63\n",
            "[1208 | 1346.19] loss=2.47 avg=2.63\n",
            "[1209 | 1347.11] loss=2.59 avg=2.63\n",
            "[1210 | 1348.04] loss=2.77 avg=2.63\n",
            "[1211 | 1348.97] loss=2.79 avg=2.63\n",
            "[1212 | 1349.90] loss=2.78 avg=2.63\n",
            "[1213 | 1350.83] loss=2.93 avg=2.64\n",
            "[1214 | 1351.77] loss=2.38 avg=2.63\n",
            "[1215 | 1352.69] loss=2.57 avg=2.63\n",
            "[1216 | 1353.62] loss=2.49 avg=2.63\n",
            "[1217 | 1354.55] loss=2.40 avg=2.63\n",
            "[1218 | 1355.49] loss=2.68 avg=2.63\n",
            "[1219 | 1356.41] loss=2.91 avg=2.63\n",
            "[1220 | 1357.34] loss=2.61 avg=2.63\n",
            "[1221 | 1358.28] loss=2.61 avg=2.63\n",
            "[1222 | 1359.21] loss=2.29 avg=2.63\n",
            "[1223 | 1360.14] loss=2.90 avg=2.63\n",
            "[1224 | 1361.06] loss=2.21 avg=2.63\n",
            "[1225 | 1362.00] loss=2.76 avg=2.63\n",
            "[1226 | 1362.92] loss=2.70 avg=2.63\n",
            "[1227 | 1363.86] loss=2.70 avg=2.63\n",
            "[1228 | 1364.79] loss=2.71 avg=2.63\n",
            "[1229 | 1365.72] loss=2.82 avg=2.63\n",
            "[1230 | 1366.65] loss=2.52 avg=2.63\n",
            "[1231 | 1367.57] loss=2.68 avg=2.63\n",
            "[1232 | 1368.50] loss=2.69 avg=2.63\n",
            "[1233 | 1369.42] loss=2.46 avg=2.63\n",
            "[1234 | 1370.36] loss=2.79 avg=2.63\n",
            "[1235 | 1371.29] loss=2.91 avg=2.64\n",
            "[1236 | 1372.22] loss=2.61 avg=2.64\n",
            "[1237 | 1373.15] loss=2.66 avg=2.64\n",
            "[1238 | 1374.08] loss=2.33 avg=2.63\n",
            "[1239 | 1375.02] loss=2.62 avg=2.63\n",
            "[1240 | 1375.95] loss=2.48 avg=2.63\n",
            "[1241 | 1376.88] loss=2.72 avg=2.63\n",
            "[1242 | 1377.81] loss=2.52 avg=2.63\n",
            "[1243 | 1378.74] loss=2.43 avg=2.63\n",
            "[1244 | 1379.67] loss=2.58 avg=2.63\n",
            "[1245 | 1380.60] loss=2.45 avg=2.63\n",
            "[1246 | 1381.53] loss=2.73 avg=2.63\n",
            "[1247 | 1382.45] loss=2.99 avg=2.63\n",
            "[1248 | 1383.38] loss=2.32 avg=2.63\n",
            "[1249 | 1384.31] loss=2.65 avg=2.63\n",
            "[1250 | 1385.24] loss=2.76 avg=2.63\n",
            "[1251 | 1386.17] loss=2.86 avg=2.63\n",
            "[1252 | 1387.10] loss=2.74 avg=2.63\n",
            "[1253 | 1388.03] loss=2.56 avg=2.63\n",
            "[1254 | 1388.96] loss=2.52 avg=2.63\n",
            "[1255 | 1389.89] loss=2.46 avg=2.63\n",
            "[1256 | 1390.82] loss=2.60 avg=2.63\n",
            "[1257 | 1391.75] loss=2.54 avg=2.63\n",
            "[1258 | 1392.67] loss=2.62 avg=2.63\n",
            "[1259 | 1393.61] loss=2.59 avg=2.63\n",
            "[1260 | 1394.54] loss=2.61 avg=2.63\n",
            "[1261 | 1395.47] loss=2.71 avg=2.63\n",
            "[1262 | 1396.40] loss=2.41 avg=2.63\n",
            "[1263 | 1397.33] loss=2.54 avg=2.63\n",
            "[1264 | 1398.26] loss=2.82 avg=2.63\n",
            "[1265 | 1399.19] loss=2.17 avg=2.62\n",
            "[1266 | 1400.11] loss=2.63 avg=2.62\n",
            "[1267 | 1401.04] loss=2.42 avg=2.62\n",
            "[1268 | 1401.98] loss=2.81 avg=2.62\n",
            "[1269 | 1402.91] loss=2.60 avg=2.62\n",
            "[1270 | 1403.84] loss=2.65 avg=2.62\n",
            "[1271 | 1404.77] loss=2.67 avg=2.62\n",
            "[1272 | 1405.69] loss=2.83 avg=2.63\n",
            "[1273 | 1406.62] loss=2.93 avg=2.63\n",
            "[1274 | 1407.55] loss=2.73 avg=2.63\n",
            "[1275 | 1408.48] loss=2.72 avg=2.63\n",
            "[1276 | 1409.41] loss=2.64 avg=2.63\n",
            "[1277 | 1410.34] loss=2.88 avg=2.63\n",
            "[1278 | 1411.26] loss=2.46 avg=2.63\n",
            "[1279 | 1412.19] loss=2.67 avg=2.63\n",
            "[1280 | 1413.12] loss=2.36 avg=2.63\n",
            "[1281 | 1414.05] loss=2.46 avg=2.63\n",
            "[1282 | 1414.98] loss=2.72 avg=2.63\n",
            "[1283 | 1415.91] loss=2.95 avg=2.63\n",
            "[1284 | 1416.84] loss=2.67 avg=2.63\n",
            "[1285 | 1417.77] loss=2.83 avg=2.63\n",
            "[1286 | 1418.69] loss=2.66 avg=2.63\n",
            "[1287 | 1419.62] loss=2.67 avg=2.63\n",
            "[1288 | 1420.54] loss=2.42 avg=2.63\n",
            "[1289 | 1421.47] loss=2.36 avg=2.63\n",
            "[1290 | 1422.41] loss=2.40 avg=2.63\n",
            "[1291 | 1423.33] loss=2.61 avg=2.63\n",
            "[1292 | 1424.26] loss=2.79 avg=2.63\n",
            "[1293 | 1425.19] loss=2.74 avg=2.63\n",
            "[1294 | 1426.12] loss=2.58 avg=2.63\n",
            "[1295 | 1427.04] loss=2.61 avg=2.63\n",
            "[1296 | 1427.97] loss=2.51 avg=2.63\n",
            "[1297 | 1428.90] loss=2.37 avg=2.63\n",
            "[1298 | 1429.83] loss=2.96 avg=2.63\n",
            "[1299 | 1430.75] loss=2.43 avg=2.63\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            " for two minutes , which is a bit boring ). \n",
            "When the Republicans had a [HANDLE] debate , and now want to repeal and replace [HANDLE] , how come they didnt show it last time on tv in person . [HANDLE] is a total loser . No wonder they don ' t vote for me ! \n",
            "Why don ' t they do [HANDLE] and [HANDLE] together ? They ' re a really bad rivalry ! \n",
            " [HANDLE] : . [HASHTAG] : [HANDLE] ' s latest [HASHTAG] interview [URL] \n",
            " [HANDLE] : [HANDLE] if i ' s elected president , what do you think I will be doing ? [HASHTAG] \" [HASHTAG]\n",
            "Now that the Democrats and Republicans are getting ready to get ready to get their asses kicked by the FBI and the Special Counsel Robert Mueller , it is time to put the Witch Hunt back on track to end with a big Victory for America ! \n",
            " [HANDLE] : [HANDLE] [HANDLE] [HANDLE] I think he ' s right , Obama is being driven into a corner all the time . \n",
            " [HANDLE] : [HANDLE] [HANDLE] [HANDLE] I ' ll support you ... You ' re my idol ! \n",
            "My daughter Ivanka is the smartest girl there is ! \n",
            " [HANDLE] : [HANDLE] this interview with [HANDLE] is the best I ever watched ! [HASHTAG] [URL] \n",
            "Remember when there were no new tax hikes ( but unemployment was 1 . 7 ) during the Obama Administration ? \n",
            " [HANDLE] : [HANDLE] [HANDLE] Trump will be a great president ! [HASHTAG] \n",
            "Today it was my honor to present my newest book , [HANDLE] ( now Trump World ). [HASHTAG] [URL] \n",
            "As the nation ' s most visited tourist attraction , [HANDLE] is a landmark in the city . [URL] \n",
            "My [HANDLE] interview with [HANDLE] discussing what he would do as president , his goals for our country ' s future , and how [HANDLE] ' s show is a success [URL] \n",
            "Wow , what a terrific start Mr . Trump has been ! If you are excited , keep going !! [URL] \n",
            "I agree with you - we need to stop Hillary Clinton and her supporters from taking our country back . \n",
            " [HANDLE] : [HANDLE] [HANDLE] [HANDLE] [HANDLE] [HANDLE] [HANDLE] You have my vote \" Thanks ! \n",
            "Great interview : Obama to attend golf course in Scotland ( cont ) [URL] \n",
            " [HANDLE] : [HANDLE] [HANDLE] I can ' t wait to read what people are saying about your book-it makes you look like an expert , and a genius\n",
            "Why did Obama ask me for help at the Benghazi consulate ? Who knows ? \n",
            " [HANDLE] : [HANDLE] you are right ! You are really smart .\" Thank you . \n",
            " [HANDLE] : \" [HANDLE] - Run Donald . You are the best , don ' t give up\n",
            " [HANDLE] : [HANDLE] I ' d vote for my Dad , but I want a real life \" Just say it , we ' ve made this country great . \n",
            " [HANDLE] : [HANDLE] just finished his book , \" The Art of the Deal \" \" [HANDLE] I ' d vote for you ! \n",
            " [HASHTAG] Trump National Doral on June 24thth in Miami Beach [URL] \n",
            " [HANDLE] : [HANDLE] TRUMP FOR PRESIDENT ! We need you \" Thank you ! \n",
            "Just arrived on the coast of Florida . [HASHTAG] . Will be back soon . [URL] \n",
            "Our country is running so late , with so little progress on our National Security , that the next President has a much greater chance of getting out than any previous President has ever had ! \n",
            "The only one [HANDLE] needs are [HASHTAG] [URL] Thanks ! \n",
            "Just spoke with the White House Chief of Staff . He is a great guy and a true champion of American values . You are one of the reasons he is the best . \n",
            " [HANDLE] : [HANDLE] [HANDLE] If he becomes President he should run against me for president . \n",
            " [HASHTAG] is now up over 6800 , and\n",
            "\n",
            "[1300 | 1448.78] loss=2.71 avg=2.63\n",
            "[1301 | 1449.71] loss=2.53 avg=2.63\n",
            "[1302 | 1450.64] loss=2.77 avg=2.63\n",
            "[1303 | 1451.57] loss=2.54 avg=2.63\n",
            "[1304 | 1452.49] loss=2.65 avg=2.63\n",
            "[1305 | 1453.43] loss=2.56 avg=2.63\n",
            "[1306 | 1454.36] loss=2.82 avg=2.63\n",
            "[1307 | 1455.29] loss=2.56 avg=2.63\n",
            "[1308 | 1456.22] loss=2.77 avg=2.63\n",
            "[1309 | 1457.16] loss=2.79 avg=2.63\n",
            "[1310 | 1458.09] loss=2.76 avg=2.63\n",
            "[1311 | 1459.02] loss=2.72 avg=2.63\n",
            "[1312 | 1459.96] loss=2.50 avg=2.63\n",
            "[1313 | 1460.89] loss=2.55 avg=2.63\n",
            "[1314 | 1461.83] loss=2.85 avg=2.63\n",
            "[1315 | 1462.76] loss=2.60 avg=2.63\n",
            "[1316 | 1463.70] loss=2.63 avg=2.63\n",
            "[1317 | 1464.61] loss=2.31 avg=2.63\n",
            "[1318 | 1465.54] loss=2.67 avg=2.63\n",
            "[1319 | 1466.47] loss=2.69 avg=2.63\n",
            "[1320 | 1467.40] loss=2.53 avg=2.63\n",
            "[1321 | 1468.32] loss=2.40 avg=2.63\n",
            "[1322 | 1469.26] loss=2.52 avg=2.63\n",
            "[1323 | 1470.19] loss=2.50 avg=2.62\n",
            "[1324 | 1471.12] loss=2.72 avg=2.63\n",
            "[1325 | 1472.05] loss=2.52 avg=2.62\n",
            "[1326 | 1472.98] loss=2.62 avg=2.62\n",
            "[1327 | 1473.92] loss=2.59 avg=2.62\n",
            "[1328 | 1474.85] loss=2.57 avg=2.62\n",
            "[1329 | 1475.77] loss=2.46 avg=2.62\n",
            "[1330 | 1476.70] loss=2.68 avg=2.62\n",
            "[1331 | 1477.63] loss=2.53 avg=2.62\n",
            "[1332 | 1478.56] loss=2.62 avg=2.62\n",
            "[1333 | 1479.49] loss=2.62 avg=2.62\n",
            "[1334 | 1480.42] loss=2.61 avg=2.62\n",
            "[1335 | 1481.35] loss=2.46 avg=2.62\n",
            "[1336 | 1482.28] loss=2.41 avg=2.62\n",
            "[1337 | 1483.20] loss=2.52 avg=2.62\n",
            "[1338 | 1484.13] loss=2.44 avg=2.62\n",
            "[1339 | 1485.06] loss=2.58 avg=2.61\n",
            "[1340 | 1486.00] loss=2.61 avg=2.61\n",
            "[1341 | 1486.92] loss=2.64 avg=2.61\n",
            "[1342 | 1487.85] loss=2.63 avg=2.62\n",
            "[1343 | 1488.78] loss=2.68 avg=2.62\n",
            "[1344 | 1489.72] loss=2.59 avg=2.62\n",
            "[1345 | 1490.64] loss=2.77 avg=2.62\n",
            "[1346 | 1491.57] loss=2.81 avg=2.62\n",
            "[1347 | 1492.51] loss=2.53 avg=2.62\n",
            "[1348 | 1493.43] loss=2.50 avg=2.62\n",
            "[1349 | 1494.36] loss=2.57 avg=2.62\n",
            "[1350 | 1495.29] loss=2.53 avg=2.62\n",
            "[1351 | 1496.22] loss=2.81 avg=2.62\n",
            "[1352 | 1497.15] loss=2.40 avg=2.62\n",
            "[1353 | 1498.09] loss=2.54 avg=2.61\n",
            "[1354 | 1499.01] loss=2.65 avg=2.62\n",
            "[1355 | 1499.94] loss=2.50 avg=2.61\n",
            "[1356 | 1500.87] loss=2.50 avg=2.61\n",
            "[1357 | 1501.81] loss=2.54 avg=2.61\n",
            "[1358 | 1502.74] loss=2.71 avg=2.61\n",
            "[1359 | 1503.67] loss=2.56 avg=2.61\n",
            "[1360 | 1504.60] loss=2.64 avg=2.61\n",
            "[1361 | 1505.53] loss=2.50 avg=2.61\n",
            "[1362 | 1506.46] loss=2.47 avg=2.61\n",
            "[1363 | 1507.40] loss=2.82 avg=2.61\n",
            "[1364 | 1508.32] loss=2.53 avg=2.61\n",
            "[1365 | 1509.25] loss=2.64 avg=2.61\n",
            "[1366 | 1510.18] loss=2.90 avg=2.61\n",
            "[1367 | 1511.11] loss=3.02 avg=2.62\n",
            "[1368 | 1512.04] loss=3.03 avg=2.62\n",
            "[1369 | 1512.96] loss=2.68 avg=2.62\n",
            "[1370 | 1513.90] loss=2.43 avg=2.62\n",
            "[1371 | 1514.84] loss=2.89 avg=2.62\n",
            "[1372 | 1515.76] loss=2.64 avg=2.62\n",
            "[1373 | 1516.69] loss=2.66 avg=2.62\n",
            "[1374 | 1517.62] loss=2.51 avg=2.62\n",
            "[1375 | 1518.54] loss=2.24 avg=2.62\n",
            "[1376 | 1519.47] loss=2.89 avg=2.62\n",
            "[1377 | 1520.38] loss=2.81 avg=2.62\n",
            "[1378 | 1521.31] loss=2.60 avg=2.62\n",
            "[1379 | 1522.24] loss=2.68 avg=2.62\n",
            "[1380 | 1523.18] loss=2.46 avg=2.62\n",
            "[1381 | 1524.09] loss=2.52 avg=2.62\n",
            "[1382 | 1525.03] loss=2.58 avg=2.62\n",
            "[1383 | 1525.96] loss=2.61 avg=2.62\n",
            "[1384 | 1526.89] loss=2.59 avg=2.62\n",
            "[1385 | 1527.81] loss=2.63 avg=2.62\n",
            "[1386 | 1528.74] loss=2.59 avg=2.62\n",
            "[1387 | 1529.67] loss=2.78 avg=2.62\n",
            "[1388 | 1530.61] loss=2.79 avg=2.62\n",
            "[1389 | 1531.53] loss=2.41 avg=2.62\n",
            "[1390 | 1532.47] loss=2.89 avg=2.62\n",
            "[1391 | 1533.40] loss=2.60 avg=2.62\n",
            "[1392 | 1534.33] loss=2.53 avg=2.62\n",
            "[1393 | 1535.25] loss=2.50 avg=2.62\n",
            "[1394 | 1536.18] loss=2.40 avg=2.62\n",
            "[1395 | 1537.10] loss=2.74 avg=2.62\n",
            "[1396 | 1538.03] loss=2.92 avg=2.62\n",
            "[1397 | 1538.95] loss=2.54 avg=2.62\n",
            "[1398 | 1539.88] loss=2.74 avg=2.62\n",
            "[1399 | 1540.81] loss=2.93 avg=2.63\n",
            "Generating samples...\n",
            "======== SAMPLE 1 ========\n",
            "in .\")\n",
            " [HANDLE] : [HANDLE] [HANDLE] [HANDLE] [HANDLE] Trump is the only person who can bring back our military and our military system ! [HASHTAG] [HASHTAG] \n",
            " [HANDLE] : [HANDLE] I ' ve watched all of your shows and love how you show off \" Thanks\n",
            "The truth is that the truth is that the Dems are totally incompetent . They have no courage . You ' re on TV ! [URL] \n",
            "The biggest disaster in the history of the world is not the U . S . , it ' s the WORLD . [URL] \n",
            "The [HANDLE] report has me thinking about why the unemployment rate is higher than people think ( I thought he was right ). Great job ! [HANDLE] [HANDLE] [URL] \n",
            "My thoughts on the upcoming trip of my Vice Presidential running mate , [HANDLE] , [HANDLE] to South Korea ( and many others ) . \n",
            "Thanks to everybody who called me for this week in Tampa , Florida . Thanks for all your support , we will soon return to our wonderful city . [URL] \n",
            " [HANDLE] : \" Donald Trump is the most accomplished president I ' ve ever had in my life .\" The answer ! \n",
            "Why is [HANDLE] taking advantage of the fact that [HANDLE] now has more money than her and [HANDLE] !\" I can ' t believe [HANDLE] ! \n",
            " [HANDLE] : [HANDLE] What do yon say for yourself ? I don ' t think yon ' t give a hoot\n",
            "The [HANDLE] is now airing all my ratings on [HANDLE] . This isnt fair or any of my ratings will go up ! \n",
            " [HANDLE] : [HANDLE] if you cant beat [HANDLE] and the [HANDLE] in primaries i dont care what you have to do !!!!!! [HASHTAG] \n",
            "My [HANDLE] interview will be out in an hour , which is a record for a first hour show ! [HASHTAG] \n",
            " [HANDLE] : ' [HANDLE] is totally wrong that the [HASHTAG] should be cancelled when I hear [HANDLE] would love a second season . [URL] \n",
            "Thank you [HANDLE] for coming down here today . I will take care of all you have to do . [URL] \n",
            " [HANDLE] : \" [HANDLE] : The truth !!! The real news of the day is that my [HASHTAG] was the biggest poll for me yet !!! RT [HANDLE] [HANDLE] : \" [HANDLE] : [HANDLE] this morning is good news for the economy ( he made a big mistake !)\" Yes , it is\n",
            " [HANDLE] : [HANDLE] DonaldTrump for president\n",
            "If only we had been given the money we will start to get out of this system like never before . ( Not because of you .) \n",
            "Thank you [HANDLE] . It was a great honor to be with President [HANDLE] ! [URL] \n",
            "The U . S . has more military spending on Afghanistan than Afghanistan spends on gas -- and a lot of that is for the Afghan war . \n",
            " [HANDLE] : [HANDLE] If you want to run for President you would better know what it takes , we need someone with guts\n",
            "Just watched our [HANDLE] interview -- they all look to be very good ! \n",
            " [HANDLE] : [HANDLE] [HANDLE] you must be soooo cool for this one [HASHTAG] \n",
            "A great thing about the [HANDLE] ? [HANDLE] is that he ' s a very nice man with no ego\n",
            " [HANDLE] : It is becoming the hottest day in history , not just for [HANDLE] but every day for the next 500 years . [HASHTAG] \n",
            " [HANDLE] What are you working on in your office today ? [HANDLE] [HANDLE]\n",
            " [HANDLE] : [HANDLE] we all love Donald Trump for being the President . The country is with you . We need you . \n",
            "If only we were given the money we will start to get out of this system like never before . \n",
            "We are all together . We have all been involved with many things and have all raised money and been with us many times . \n",
            " [HANDLE] : [HANDLE] [HANDLE] [H\n",
            "\n",
            "[1400 | 1558.89] loss=2.71 avg=2.63\n",
            "[1401 | 1559.83] loss=2.45 avg=2.63\n",
            "[1402 | 1560.75] loss=2.44 avg=2.62\n",
            "[1403 | 1561.68] loss=2.56 avg=2.62\n",
            "[1404 | 1562.60] loss=2.40 avg=2.62\n",
            "[1405 | 1563.54] loss=2.79 avg=2.62\n",
            "[1406 | 1564.47] loss=2.65 avg=2.62\n",
            "[1407 | 1565.40] loss=2.53 avg=2.62\n",
            "[1408 | 1566.33] loss=2.62 avg=2.62\n",
            "[1409 | 1567.26] loss=2.59 avg=2.62\n",
            "[1410 | 1568.19] loss=2.65 avg=2.62\n",
            "[1411 | 1569.12] loss=2.35 avg=2.62\n",
            "[1412 | 1570.05] loss=2.67 avg=2.62\n",
            "[1413 | 1570.98] loss=2.48 avg=2.62\n",
            "[1414 | 1571.91] loss=2.42 avg=2.62\n",
            "[1415 | 1572.84] loss=2.68 avg=2.62\n",
            "[1416 | 1573.76] loss=2.62 avg=2.62\n",
            "[1417 | 1574.69] loss=2.90 avg=2.62\n",
            "[1418 | 1575.62] loss=2.59 avg=2.62\n",
            "[1419 | 1576.54] loss=2.58 avg=2.62\n",
            "[1420 | 1577.47] loss=2.55 avg=2.62\n",
            "[1421 | 1578.40] loss=3.01 avg=2.62\n",
            "[1422 | 1579.32] loss=2.44 avg=2.62\n",
            "[1423 | 1580.25] loss=2.85 avg=2.62\n",
            "[1424 | 1581.18] loss=2.51 avg=2.62\n",
            "[1425 | 1582.10] loss=2.46 avg=2.62\n",
            "[1426 | 1583.03] loss=2.97 avg=2.62\n",
            "[1427 | 1583.96] loss=2.74 avg=2.63\n",
            "[1428 | 1584.89] loss=2.40 avg=2.62\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}